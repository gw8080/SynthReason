Philosophy (from Greek: φιλοσοφία, philosophia, 'love of wisdom')[1][2][3] is the study of general and fundamental questions about existence, knowledge, values, reason, mind, and language.[4][5] Such questions are often posed as problems[6][7] to be studied or resolved. The term was probably coined by Pythagoras (c. 570 – 495 BCE). Philosophical methods include questioning, critical discussion, rational argument, and systematic presentation.[8][9][i]

Classic philosophical questions include: 'is it possible to know anything and to prove it?'[10][11][12] and 'what is most real?' Philosophers also pose more practical and concrete questions such as: 'is there a best way to live?', 'is it better to be just or unjust (if one can get away with it)?',[13] 'do humans have free will?'[14]

Historically, philosophy encompassed all bodies of knowledge.[15] From the time of Ancient Greek philosopher Aristotle to the 19th century, "natural philosophy" encompassed astronomy, medicine, and physics.[16] For example, Newton's 1687 Mathematical Principles of Natural Philosophy later became classified as a book of physics.

In the 19th century, the growth of modern research universities led academic philosophy and other disciplines to professionalize and specialize.[17][18] In the modern era, some investigations that were traditionally part of philosophy became separate academic disciplines, including psychology, sociology, linguistics, and economics. Other investigations closely related to art, science, politics, or other pursuits remained part of philosophy. For example, is beauty objective or subjective?[19][20] Are there many scientific methods or just one?[21] Is political utopia a hopeful dream or hopeless fantasy?[22][23][24]

Major sub-fields of academic philosophy include: metaphysics, which is "concerned with the fundamental nature of reality and being;"[25] and epistemology, which is about "nature and grounds of knowledge [and]…its limits and validity;"[26] as well as ethics, aesthetics, political philosophy, logic, and philosophy of science.


Initially, the term 'philosophy' referred to any body of knowledge.[15] In this sense, philosophy is closely related to religion, mathematics, natural science, education, and politics. Though as of the 2000s it has been classified as a book of physics, Newton's Mathematical Principles of Natural Philosophy (1687) uses the term natural philosophy as it was understood at the time to encompass disciplines, such as astronomy, medicine and physics, that later became associated with sciences.[16]

In the first part of his Academica 1, Cicero introduced the division of philosophy into logic, physics, and ethics, emulating Epicurus' division of his doctrine into canon, physics, and ethics.

In section thirteen of his Lives and Opinions of the Eminent Philosophers 1, Diogenes Laërtius (3rd century), the first historian of philosophy, established the traditional division of philosophical inquiry into three parts:[27]

Natural philosophy (i.e. physics, from Greek: ta physika, lit. 'things having to do with physis [nature]') was the study of the constitution and processes of transformation in the physical world;
Moral philosophy (i.e. ethics, from êthika, 'having to do with character, disposition, manners') was the study of goodness, right and wrong, justice and virtue; and
Metaphysical philosophy (i.e. logic, from logikós, 'of or pertaining to reason or speech') was the study of existence, causation, God, logic, forms, and other abstract objects (meta ta physika, 'after the Physics').
This division is not obsolete but has changed: natural philosophy has split into the various natural sciences, especially physics, astronomy, chemistry, biology, and cosmology; moral philosophy has birthed the social sciences, while still including value theory (e.g. ethics, aesthetics, political philosophy, etc.); and metaphysical philosophy has given way to formal sciences such as logic, mathematics and philosophy of science, while still including epistemology, cosmology, etc.

Many philosophical debates that began in ancient times are still debated today. McGinn (1993) and others claim that no philosophical progress has occurred during that interval.[28] Chalmers (2013) and others, by contrast, see progress in philosophy similar to that in science,[29] while Brewer (2011) argued that "progress" is the wrong standard by which to judge philosophical activity.[30]

In one general sense, philosophy is associated with wisdom, intellectual culture, and a search for knowledge. In this sense, all cultures and literate societies ask philosophical questions, such as "how are we to live" and "what is the nature of reality." A broad and impartial conception of philosophy, then, finds a reasoned inquiry into such matters as reality, morality, and life in all world civilizations.[31]

Statue of Aristotle in the Aristotlepark of Stagira
Western philosophy is the philosophical tradition of the Western world, dating back to pre-Socratic thinkers who were active in 6th-century Greece (BCE), such as Thales (c. 624 – 546 BCE) and Pythagoras (c. 570 – 495 BCE) who practiced a 'love of wisdom' (Latin: philosophia)[32] and were also termed 'students of nature' (physiologoi).

Western philosophy can be divided into three eras:

While our knowledge of the ancient era begins with Thales in the 6th century BCE, comparatively little is known about the philosophers who came before Socrates (commonly known as the pre-Socratics). The ancient era was dominated by Greek philosophical schools, which were significantly influenced by Socrates' teachings. Most notable among these were Plato, who founded the Platonic Academy, and his student Aristotle,[33] who founded the Peripatetic school. Other ancient philosophical traditions included Cynicism, Stoicism, Skepticism and Epicureanism. Important topics covered by the Greeks included metaphysics (with competing theories such as atomism and monism), cosmology, the nature of the well-lived life (eudaimonia), the possibility of knowledge and the nature of reason (logos). With the rise of the Roman empire, Greek philosophy was also increasingly discussed in Latin by Romans such as Cicero and Seneca (see Roman philosophy).


Medieval philosophy (5th–16th centuries) is the period following the fall of the Western Roman Empire and was dominated by the rise of Christianity and hence reflects Judeo-Christian theological concerns as well as retaining a continuity with Greco-Roman thought. Problems such as the existence and nature of God, the nature of faith and reason, metaphysics, the problem of evil were discussed in this period. Some key Medieval thinkers include St. Augustine, Thomas Aquinas, Boethius, Anselm and Roger Bacon. Philosophy for these thinkers was viewed as an aid to Theology (ancilla theologiae) and hence they sought to align their philosophy with their interpretation of sacred scripture. This period saw the development of Scholasticism, a text critical method developed in medieval universities based on close reading and disputation on key texts. The Renaissance period saw increasing focus on classic Greco-Roman thought and on a robust Humanism.


Early modern philosophy in the Western world begins with thinkers such as Thomas Hobbes and René Descartes (1596–1650).[34] Following the rise of natural science, modern philosophy was concerned with developing a secular and rational foundation for knowledge and moved away from traditional structures of authority such as religion, scholastic thought and the Church. Major modern philosophers include Spinoza, Leibniz, Locke, Berkeley, Hume, and Kant.[ii][iii][iv]

19th-century philosophy (sometimes called late modern philosophy) was influenced by the wider 18th-century movement termed "the Enlightenment", and includes figures such as Hegel a key figure in German idealism, Kierkegaard who developed the foundations for existentialism, Nietzsche a famed anti-Christian, John Stuart Mill who promoted utilitarianism, Karl Marx who developed the foundations for communism and the American William James. The 20th century saw the split between analytic philosophy and continental philosophy, as well as philosophical trends such as phenomenology, existentialism, logical positivism, pragmatism and the linguistic turn (see Contemporary philosophy).


An Iranian portrait of Avicenna on a Silver Vase. He was one of the most influential philosophers of the Islamic Golden Age.
See also: Islamic philosophy and Middle Eastern philosophy
The regions of the fertile Crescent, Iran and Arabia are home to the earliest known philosophical Wisdom literature and is today mostly dominated by Islamic culture. Early wisdom literature from the fertile crescent was a genre which sought to instruct people on ethical action, practical living and virtue through stories and proverbs. In Ancient Egypt, these texts were known as sebayt ('teachings') and they are central to our understandings of Ancient Egyptian philosophy. Babylonian astronomy also included much philosophical speculations about cosmology which may have influenced the Ancient Greeks. Jewish philosophy and Christian philosophy are religio-philosophical traditions that developed both in the Middle East and in Europe, which both share certain early Judaic texts (mainly the Tanakh) and monotheistic beliefs. Jewish thinkers such as the Geonim of the Talmudic Academies in Babylonia and Maimonides engaged with Greek and Islamic philosophy. Later Jewish philosophy came under strong Western intellectual influences and includes the works of Moses Mendelssohn who ushered in the Haskalah (the Jewish Enlightenment), Jewish existentialism, and Reform Judaism.

Pre-Islamic Iranian philosophy begins with the work of Zoroaster, one of the first promoters of monotheism and of the dualism between good and evil. This dualistic cosmogony influenced later Iranian developments such as Manichaeism, Mazdakism, and Zurvanism.

After the Muslim conquests, Early Islamic philosophy developed the Greek philosophical traditions in new innovative directions. This Islamic Golden Age influenced European intellectual developments. The two main currents of early Islamic thought are Kalam which focuses on Islamic theology and Falsafa which was based on Aristotelianism and Neoplatonism. The work of Aristotle was very influential among the falsafa such as al-Kindi (9th century), Avicenna (980 – June 1037) and Averroes (12th century). Others such as Al-Ghazali were highly critical of the methods of the Aristotelian falsafa. Islamic thinkers also developed a scientific method, experimental medicine, a theory of optics and a legal philosophy. Ibn Khaldun was an influential thinker in philosophy of history.

In Iran, several schools of Islamic philosophy continued to flourish after the Golden Age and include currents such as Illuminationist philosophy, Sufi philosophy, and Transcendent theosophy. The 19th- and 20th-century Arab world saw the Nahda ('awakening'; aka the 'Arab Renaissance') movement which influenced contemporary Islamic philosophy.

Indian philosophy (Sanskrit: darśana, lit. 'point of view', 'perspective')[35] refers to the diverse philosophical traditions that emerged since the ancient times on the Indian subcontinent. Jainism and Buddhism originated at the end of the Vedic period, while Hinduism emerged after the period as a fusion of diverse traditions.

Hindus generally classify these traditions as either orthodox (āstika) or heterodox (nāstika) depending on whether they accept the authority of the Vedas and the theories of brahman ('eternal', 'conscious', 'irreducible')[36] and ātman ('soul', 'self', 'breathe')[37] therein.[38][39] The orthodox schools include the Hindu traditions of thought, while the heterodox schools include the Buddhist and the Jain traditions.[v] Other schools include the Ajñana, Ājīvika, and Cārvāka which became extinct over their history.[40][41]

Important Indian philosophical concepts shared by the Indian philosophies and virtues include:[42][43]

Akalanka, an 8th century Jain monk and philosopher who wrote influential works on Indian Logic
Main article: Jain philosophy
Jain philosophy accepts the concept of a permanent soul (jiva) as one of the five astikayas (eternal, infinite categories that make up the substance of existence). The other four being dhárma, adharma, ākāśa ('space'), and pudgala ('matter').

The Jain thought separates matter from the soul completely,[46] with two major subtraditions: Digambara ('sky dressed', 'naked') and Śvētāmbara ('white dressed'), along with several more minor traditions such as Terapanthi.[47]

Asceticism is a major monastic virtue in Jainism.[48] Jain texts such as the Tattvartha Sutra state that right faith, right knowledge and right conduct is the path to liberation.[49] The Jain thought holds that all existence is cyclic, eternal and uncreated.[50][51] The Tattvartha Sutra is the earliest known, most comprehensive and authoritative compilation of Jain philosophy.[52][53]

Monks debating at Sera monastery, Tibet, 2013. According to Jan Westerhoff, "public debates constituted the most important and most visible forms of philosophical exchange" in ancient Indian intellectual life.[54]
Buddhist philosophy begins with the thought of Gautama Buddha (fl. between 6th and 4th century BCE) and is preserved in the early Buddhist texts. It originated in India and later spread to East Asia, Tibet, Central Asia, and Southeast Asia, developing various traditions in these regions. Mahayana forms are the dominant Buddhist philosophical traditions in East Asian regions such as China, Korea and Japan. The Theravada forms are dominant in Southeast Asian countries, such as Sri Lanka, Burma and Thailand.

Because ignorance to the true nature of things is considered one of the roots of suffering (dukkha), Buddhist philosophy is concerned with epistemology, metaphysics, ethics and psychology. Buddhist philosophical texts must also be understood within the context of meditative practices which are supposed to bring about certain cognitive shifts.[55]:8 Key innovative concepts include the four noble truths as an analysis of dukkha, anicca (impermanence), and anatta (non-self).[vi][56]

After the death of the Buddha, various groups began to systematize his main teachings, eventually developing comprehensive philosophical systems termed Abhidharma.[55]:37 Following the Abhidharma schools, Mahayana philosophers such as Nagarjuna and Vasubandhu developed the theories of śūnyatā ('emptiness of all phenomena') and vijñapti-matra ('appearance only'), a form of phenomenology or transcendental idealism. The Dignāga school of pramāṇa ('means of knowledge') promoted a sophisticated form of Buddhist logico-epistemology.

There were numerous schools, sub-schools and traditions of Buddhist philosophy in India. According to Oxford professor of Buddhist philosophy Jan Westerhoff, the major Indian schools from 300 BCE to 1000 CE were:[55]:xxiv

After the disappearance of Buddhism from India, some of these philosophical traditions continued to develop in the Tibetan Buddhist, East Asian Buddhist and Theravada Buddhist traditions.[citation needed]


Adi Shankara is one of the much studied Hindu philosophers.[57][58]
Main article: Hindu philosophy
The Vedas-based orthodox schools are a part of the Hindu traditions and they are traditionally classified into six darśanas: Nyaya, Vaisheshika, Samkhya, Yoga, Mīmāṃsā, and Vedanta.[vii][59] The Vedas as a knowledge source were interpreted differently by these six schools of Hindu philosophy, with varying degrees of overlap. They represent a "collection of philosophical views that share a textual connection," according to Chadha (2015).[60] They also reflect a tolerance for a diversity of philosophical interpretations within Hinduism while sharing the same foundation.[viii]

Some of the earliest surviving Hindu mystical and philosophical texts are the Upanishads of the later Vedic period (1000–500 BCE). Hindu philosophers of the six schools developed systems of epistemology (pramana) and investigated topics such as metaphysics, ethics, psychology (guṇa), hermeneutics, and soteriology within the framework of the Vedic knowledge, while presenting a diverse collection of interpretations.[61][62][63][64] These schools of philosophy accepted the Vedas and the Vedic concept of Ātman and Brahman,[vii] differed from the following Indian religions that rejected the authority of the Vedas:[41]

Cārvāka, a materialism school that accepted the existence of free will.[65][66]
Ājīvika, a materialism school that denied the existence of free will.[67][68]
Buddhism, a philosophy that denies the existence of ātman ('unchanging soul', 'Self')[ix][x] and is based on the teachings and enlightenment of Gautama Buddha.[xi][69]
Jainism, a philosophy that accepts the existence of the ātman, but is based on the teachings of twenty-four ascetic teachers known as tirthankaras, with Rishabha as the first and Mahavira as the twenty-fourth.[70]
The commonly named six orthodox schools over time led to what has been called the "Hindu synthesis" as exemplified by its scripture the Bhagavad Gita.[71][72][73]

Kitarō Nishida, professor of philosophy at Kyoto University and founder of the Kyoto School.
Main articles: Chinese philosophy, Korean philosophy, Japanese philosophy, Vietnamese philosophy, and Eastern philosophy
East Asian philosophical thought began in Ancient China, and Chinese philosophy begins during the Western Zhou Dynasty and the following periods after its fall when the "Hundred Schools of Thought" flourished (6th century to 221 BCE).[74][75] This period was characterized by significant intellectual and cultural developments and saw the rise of the major philosophical schools of China, Confucianism, Legalism, and Daoism as well as numerous other less influential schools. These philosophical traditions developed metaphysical, political and ethical theories such Tao, Yin and yang, Ren and Li which, along with Chinese Buddhism, directly influenced Korean philosophy, Vietnamese philosophy and Japanese philosophy (which also includes the native Shinto tradition). Buddhism began arriving in China during the Han Dynasty (206 BCE – 220 CE), through a gradual Silk road transmission and through native influences developed distinct Chinese forms (such as Chan/Zen) which spread throughout the East Asian cultural sphere. During later Chinese dynasties like the Ming Dynasty (1368–1644) as well as in the Korean Joseon dynasty (1392–1897) a resurgent Neo-Confucianism led by thinkers such as Wang Yangming (1472–1529) became the dominant school of thought, and was promoted by the imperial state.

In the Modern era, Chinese thinkers incorporated ideas from Western philosophy. Chinese Marxist philosophy developed under the influence of Mao Zedong, while a Chinese pragmatism under Hu Shih and New Confucianism's rise was influenced by Xiong Shili. Modern Japanese thought meanwhile developed under strong Western influences such as the study of Western Sciences (Rangaku) and the modernist Meirokusha intellectual society which drew from European enlightenment thought. The 20th century saw the rise of State Shinto and also Japanese nationalism. The Kyoto School, an influential and unique Japanese philosophical school developed from Western phenomenology and Medieval Japanese Buddhist philosophy such as that of Dogen.

African philosophy is philosophy produced by African people, philosophy that presents African worldviews, ideas and themes, or philosophy that uses distinct African philosophical methods. Modern African thought has been occupied with Ethnophilosophy, with defining the very meaning of African philosophy and its unique characteristics and what it means to be African.[76] During the 17th century, Ethiopian philosophy developed a robust literary tradition as exemplified by Zera Yacob. Another early African philosopher was Anton Wilhelm Amo (c. 1703–1759) who became a respected philosopher in Germany. Distinct African philosophical ideas include Ujamaa, the Bantu idea of 'Force', Négritude, Pan-Africanism and Ubuntu. Contemporary African thought has also seen the development of Professional philosophy and of Africana philosophy, the philosophical literature of the African diaspora which includes currents such as black existentialism by African-Americans. Some modern African thinkers have been influenced by Marxism, African-American literature, Critical theory, Critical race theory, Postcolonialism and Feminism.

A Tlamatini (Aztec philosopher) observing the stars, from the Codex Mendoza.
Indigenous-American philosophical thought consists of a wide variety of beliefs and traditions among different American cultures. Among some of U.S. Native American communities, there is a belief in a metaphysical principle called the 'Great Spirit' (Siouan: wakȟáŋ tȟáŋka; Algonquian: gitche manitou). Another widely shared concept was that of orenda ('spiritual power'). According to Whiteley (1998), for the Native Americans, "mind is critically informed by transcendental experience (dreams, visions and so on) as well as by reason."[77] The practices to access these transcendental experiences are termed shamanism. Another feature of the indigenous American worldviews was their extension of ethics to non-human animals and plants.[77][78]

In Mesoamerica, Aztec philosophy was an intellectual tradition developed by individuals called Tlamatini ('those who know something')[79] and its ideas are preserved in various Aztec codices. The Aztec worldview posited the concept of an ultimate universal energy or force called Ōmeteōtl ('Dual Cosmic Energy') which sought a way to live in balance with a constantly changing, "slippery" world.

The theory of Teotl can be seen as a form of Pantheism.[80] Aztec philosophers developed theories of metaphysics, epistemology, values, and aesthetics. Aztec ethics was focused on seeking tlamatiliztli ('knowledge', 'wisdom') which was based on moderation and balance in all actions as in the Nahua proverb "the middle good is necessary."[80]

The Inca civilization also had an elite class of philosopher-scholars termed the Amawtakuna who were important in the Inca education system as teachers of religion, tradition, history and ethics. Key concepts of Andean thought are Yanantin and Masintin which involve a theory of “complementary opposites” that sees polarities (such as male/female, dark/light) as interdependent parts of a harmonious whole.[81]

Although men have generally dominated philosophical discourse, women philosophers have engaged in the discipline throughout history. Ancient examples include Hipparchia of Maroneia (active c. 325 BCE) and Arete of Cyrene (active 5th–4th centuries BCE). Some women philosophers were accepted during the medieval and modern eras, but none became part of the Western canon until the 20th and 21st century, when many suggest that G.E.M. Anscombe, Hannah Arendt, Simone de Beauvoir, and Susanne Langer entered the canon.[82][83][84]

In the early 1800s, some colleges and universities in the UK and US began admitting women, producing more female academics. Nevertheless, U.S. Department of Education reports from the 1990s indicate that few women ended up in philosophy, and that philosophy is one of the least gender-proportionate fields in the humanities, with women making up somewhere between 17% and 30% of philosophy faculty according to some studies.[85]

Philosophical questions can be grouped into various branches. These groupings allow philosophers to focus on a set of similar topics and interact with other thinkers who are interested in the same questions. The groupings also make philosophy easier for students to approach. Students can learn the basic principles involved in one aspect of the field without being overwhelmed with the entire set of philosophical theories.

Various sources present different categorical schemes. The categories adopted in this article aim for breadth and simplicity.

These five major branches can be separated into sub-branches and each sub-branch contains many specific fields of study:[86][87]

These divisions are neither exhaustive, nor mutually exclusive. (A philosopher might specialize in Kantian epistemology, or Platonic aesthetics, or modern political philosophy). Furthermore, these philosophical inquiries sometimes overlap with each other and with other inquiries such as science, religion or mathematics.[88]

Dignaga founded a school of Buddhist epistemology and logic.
Epistemology is the branch of philosophy that studies knowledge.[89] Epistemologists examine putative sources of knowledge, including perceptual experience, reason, memory, and testimony. They also investigate questions about the nature of truth, belief, justification, and rationality.[90]

One of the most notable epistemological debates in the early modern period was between empiricism and rationalism. Empiricism places emphasis on observational evidence via sensory experience as the source of knowledge. Empiricism is associated with a posteriori knowledge, which is obtained through experience (such as scientific knowledge). Rationalism places emphasis on reason as a source of knowledge. Rationalism is associated with a priori knowledge, which is independent of experience (such as logic and mathematics).

Philosophical skepticism, which raises doubts some or all claims to knowledge, has been a topic of interest throughout the history of philosophy. Philosophical skepticism dates back thousands of years to ancient philosophers like Pyrrho, and features prominently in the works of modern philosophers René Descartes and David Hume. Skepticism has remained a central topic in contemporary epistemological debates.[90]

One central debate in contemporary epistemology is about the conditions required for a belief to constitute knowledge, which might include truth and justification. This debate was largely the result of attempts to solve the Gettier problem.[90] Another common subject of contemporary debates is the regress problem, which occurs when trying to offer proof or justification for any belief, statement, or proposition. The problem is that whatever the source of justification may be, that source must either be without justification (in which case it must be treated as an arbitrary foundation for belief), or it must have some further justification (in which case justification must either be the result of circular reasoning, as in coherentism, or the result of an infinite regress, as in infinitism).[90]

Metaphysics is the study of the most general features of reality, such as existence, time, objects and their properties, wholes and their parts, events, processes and causation and the relationship between mind and body. Metaphysics includes cosmology, the study of the world in its entirety and ontology, the study of being.

A major point of debate is between realism, which holds that there are entities that exist independently of their mental perception and idealism, which holds that reality is mentally constructed or otherwise immaterial. Metaphysics deals with the topic of identity. Essence is the set of attributes that make an object what it fundamentally is and without which it loses its identity while accident is a property that the object has, without which the object can still retain its identity. Particulars are objects that are said to exist in space and time, as opposed to abstract objects, such as numbers, and universals, which are properties held by multiple particulars, such as redness or a gender. The type of existence, if any, of universals and abstract objects is an issue of debate.

Several subfields of philosophy are closely related to epistemology and metaphysics, most notably philosophy of mind and philosophy of language. All of these are sometimes grouped together as "core" fields in philosophy, although this terminology is now considered outdated.[91] Philosophy of language explores the nature, origins, and use of language. Philosophy of mind explores the nature of the mind and its relationship to the body, as typified by disputes between materialism and dualism. In recent years, this branch has become related to cognitive science.

Value theory (or axiology) is the major branch of philosophy that addresses topics such as goodness, beauty and justice. Value theory includes ethics, aesthetics, political philosophy, feminist philosophy, philosophy of law and more.[citation needed]


The Beijing imperial college was an intellectual center for Confucian ethics and classics during the Yuan, Ming and Qing dynasties.
Ethics, also known as moral philosophy, studies what constitutes good and bad conduct, right and wrong values, and good and evil. Its primary investigations include how to live a good life and identifying standards of morality. It also includes investigating whether or not there is a best way to live or a universal moral standard, and if so, how we come to learn about it. The main branches of ethics are normative ethics, meta-ethics and applied ethics.[92]

The three main views in ethics about what constitute moral actions are:[92]

Consequentialism, which judges actions based on their consequences. One such view is utilitarianism, which judges actions based on the net happiness (or pleasure) and/or lack of suffering (or pain) that they produce.
Deontology, which judges actions based on whether or not they are in accordance with one's moral duty. In the standard form defended by Immanuel Kant, deontology is concerned with whether or not a choice respects the moral agency of other people, regardless of its consequences.
Virtue ethics, which judges actions based on the moral character of the agent who performs them and whether they conform to what an ideally virtuous agent would do.
Aesthetics
Main article: Aesthetics
Aesthetics is the "critical reflection on art, culture and nature."[93][94] It addresses the nature of art, beauty and taste, enjoyment, emotional values, perception and with the creation and appreciation of beauty.[95] It is more precisely defined as the study of sensory or sensori-emotional values, sometimes called judgments of sentiment and taste.[96] Its major divisions are art theory, literary theory, film theory and music theory. An example from art theory is to discern the set of principles underlying the work of a particular artist or artistic movement such as the Cubist aesthetic.[97] The philosophy of film analyzes films and filmmakers for their philosophical content and explores film (images, cinema, etc.) as a medium for philosophical reflection and expression.[citation needed]

Thomas Hobbes, best known for his Leviathan, which expounded an influential formulation of social contract theory.
Political philosophy is the study of government and the relationship of individuals (or families and clans) to communities including the state.[citation needed] It includes questions about justice, law, property and the rights and obligations of the citizen. Politics and ethics are traditionally linked subjects, as both discuss the question of how people should live together.[citation needed]

Philosophy of law (aka jurisprudence): explores the varying theories explaining the nature and interpretation of laws.
Philosophy of education: analyzes the definition and content of education, as well as the goals and challenges of educators.
Feminist philosophy: explores questions surrounding gender, sexuality and the body including the nature of feminism itself as a social and philosophical movement.
Logic, science, and mathematics

This article may not properly summarize its corresponding main article. Please help improve it by rewriting it in an encyclopedic style. (June 2020) (Learn how and when to remove this template message)
The topics of philosophy of science are numbers, symbols and the formal methods of reasoning as employed in the social sciences and natural sciences.[citation needed]

Logic is the study of reasoning and argument. An argument is "a connected series of statements intended to establish a proposition."[citation needed] The connected series of statements are "premises" and the proposition is the conclusion. For example:

All humans are mortal. (premise)
Socrates is a human. (premise)
Therefore, Socrates is mortal. (conclusion)
Deductive reasoning is when, given certain premises, conclusions are unavoidably implied. Rules of inference are used to infer conclusions such as, modus ponens, where given “A” and “If A then B”, then “B” must be concluded.

Because sound reasoning is an essential element of all sciences,[98] social sciences and humanities disciplines, logic became a formal science. Sub-fields include mathematical logic, philosophical logic, Modal logic, computational logic and non-classical logics. A major question in the philosophy of mathematics is whether mathematical entities are objective and discovered, called mathematical realism, or invented, called mathematical antirealism.


This branch explores the foundations, methods, history, implications and purpose of science. Many of its sub-divisions correspond to a specific branch of science. For example, philosophy of biology deals specifically with the metaphysical, epistemological and ethical issues in the biomedical and life sciences. The philosophy of mathematics studies the philosophical assumptions, foundations and implications of mathematics.[citation needed]

Some contemporary philosophers specialize in studying one or more historical periods. The history of philosophy (study of a specific period, individual or school) should not be confused with the philosophy of history, a minor subfield most commonly associated with historicism as first defended in Hegel's Lectures on the Philosophy of History.[citation needed]

Philosophy of religion deals with questions that involve religion and religious ideas from a philosophically neutral perspective (as opposed to theology which begins from religious convictions).[99] Traditionally, religious questions were not seen as a separate field from philosophy proper, the idea of a separate field only arose in the 19th century.[xii]

Issues include the existence of God, the relationship between reason and faith, questions of religious epistemology, the relationship between religion and science, how to interpret religious experiences, questions about the possibility of an afterlife, the problem of religious language and the existence of souls and responses to religious pluralism and diversity.


Metaphilosophy explores the aims of philosophy, its boundaries and its methods.


A variety of other academic and non-academic approaches have been explored. The ideas conceived by a society have profound repercussions on what actions the society performs. Weaver argued that ideas have consequences.

Philosophy yields applications such as those in ethics—applied ethics in particular—and political philosophy. The political and economic philosophies of Confucius, Sun Tzu, Chanakya, Ibn Khaldun, Ibn Rushd, Ibn Taymiyyah, Machiavelli, Leibniz, Hobbes, Locke, Rousseau, Adam Smith, John Stuart Mill, Marx, Tolstoy, Gandhi and Martin Luther King Jr. have been used to shape and justify governments and their actions. Progressive education as championed by Dewey had a profound impact on 20th-century US educational practices. Descendants of this movement include efforts in philosophy for children, which are part of philosophy education. Clausewitz's political philosophy of war has had a profound effect on statecraft, international politics and military strategy in the 20th century, especially around World War II. Logic is important in mathematics, linguistics, psychology, computer science and computer engineering.

Other important applications can be found in epistemology, which aid in understanding the requisites for knowledge, sound evidence and justified belief (important in law, economics, decision theory and a number of other disciplines). The philosophy of science discusses the underpinnings of the scientific method and has affected the nature of scientific investigation and argumentation. Philosophy thus has fundamental implications for science as a whole. For example, the strictly empirical approach of B.F. Skinner's behaviorism affected for decades the approach of the American psychological establishment. Deep ecology and animal rights examine the moral situation of humans as occupants of a world that has non-human occupants to consider also. Aesthetics can help to interpret discussions of music, literature, the plastic arts and the whole artistic dimension of life. In general, the various philosophies strive to provide practical activities with a deeper understanding of the theoretical or conceptual underpinnings of their fields.

The relationship between "X" and the "philosophy of X" is often intensely debated. Richard Feynman argued that the philosophy of a topic is irrelevant to its primary study, saying that "philosophy of science is as useful to scientists as ornithology is to birds."[citation needed] Curtis White (2014), by contrast, argued that philosophical tools are essential to humanities, sciences and social sciences.[100]


Main article: Contemporary philosophy § Outside the profession
Many inquiries outside of academia are philosophical in the broad sense. Novelists, playwrights, filmmakers, and musicians, as well as scientists and others engage in recognizably philosophical activity.

Some of those who study philosophy become professional philosophers, typically by working as professors who teach, research and write in academic institutions.[101] However, most students of academic philosophy later contribute to law, journalism, religion, sciences, politics, business, or various arts.[102][103] For example, public figures who have degrees in philosophy include comedians Steve Martin and Ricky Gervais, filmmaker Terrence Malick, Pope John Paul II, Wikipedia co-founder Larry Sanger, technology entrepreneur Peter Thiel, Supreme Court Justice Stephen Bryer and vice presidential candidate Carly Fiorina.[104][105]

Recent efforts to avail the general public to the work and relevance of philosophers include the million-dollar Berggruen Prize, first awarded to Charles Taylor in 2016.[106]

Germany was the first country to professionalize philosophy. The doctorate of philosophy (PhD) developed in Germany as the terminal Teacher's credential in the mid 17th century.[107] At the end of 1817, Georg Wilhelm Friedrich Hegel was the first philosopher to be appointed Professor by the State, namely by the Prussian Minister of Education, as an effect of Napoleonic reform in Prussia. In the United States, the professionalization grew out of reforms to the American higher-education system largely based on the German model.


Within the last century, philosophy has increasingly become a professional discipline practiced within universities, like other academic disciplines. Accordingly, it has become less general and more specialized. In the view of one prominent recent historian: "Philosophy has become a highly organized discipline, done by specialists primarily for other specialists. The number of philosophers has exploded, the volume of publication has swelled, and the subfields of serious philosophical investigation have multiplied. Not only is the broad field of philosophy today far too vast to be embraced by one mind, something similar is true even of many highly specialized subfields."[108] Some philosophers argue that this professionalization has negatively affected the discipline.[109]

The end result of professionalization for philosophy has meant that work being done in the field is now almost exclusively done by university professors holding a doctorate in the field publishing in highly technical, peer-reviewed journals. While it remains common among the population at large for a person to have a set of religious, political or philosophical views that they consider their "philosophy", these views are rarely informed by or connected to the work being done in professional philosophy today. Furthermore, unlike many of the sciences for which there has come to be a healthy industry of books, magazines, and television shows meant to popularize science and communicate the technical results of a scientific field to the general populace, works by professional philosophers directed at an audience outside the profession remain rare. Philosopher Michael Sandel's book Justice: What's the Right Thing to Do? and Harry Frankfurt's On Bullshit are examples of works that hold the uncommon distinction of having been written by professional philosophers but directed at and ultimately popular among a broader audience of non-philosophers. Both works became New York Times best sellers.


This article is about the usage of premise in discourse and logic. For other uses, see Premise (disambiguation).
A premise or premiss[a] is a statement that an argument claims will induce or justify a conclusion.[1] It is an assumption that something is true.


In logic, an argument requires a set of (at least) two declarative sentences (or "propositions") known as the "premises" (or "premisses"), along with another declarative sentence (or "proposition"), known as the conclusion. This structure of two premises and one conclusion forms the basic argumentative structure. More complex arguments can use a sequence of rules to connect several premises to one conclusion, or to derive a number of conclusions from the original premises which then act as premises for additional conclusions. An example of this is the use of the rules of inference found within symbolic logic.

Aristotle held that any logical argument could be reduced to two premises and a conclusion.[2] Premises are sometimes left unstated, in which case, they are called missing premises, for example:

Socrates is mortal because all men are mortal.

It is evident that a tacitly understood claim is that Socrates is a man. The fully expressed reasoning is thus:

Because all men are mortal and Socrates is a man, Socrates is mortal.

In this example, the independent clauses preceding the comma (namely, "all men are mortal" and "Socrates is a man") are the premises, while "Socrates is mortal" is the conclusion.

The proof of a conclusion depends on both the truth of the premises and the validity of the argument. Also, additional information is required over and above the meaning of the premise to determine if the full meaning of the conclusion coincides with what is.[3]

For Euclid, premises constitute two of the three propositions in a syllogism, with the other being the conclusion.[4] These categorical propositions contain three terms: subject and predicate of the conclusion, and the middle term. The subject of the conclusion is called the minor term while the predicate is the major term. The premise that contains the middle term and major term is called the major premise while the premise that contains the middle term and minor term is called the minor premise.[5]

A premise can also be an indicator word if statements have been combined into a logical argument and such word functions to mark the role of one or more of the statements.[6] It indicates that the statement it is attached to is a premise.[6]

 In general usage, the spelling "premise" is most common; however, in the field of logic, the spelling "premiss" is often used, especially among British writers.
References
 Audi, Robert, ed. (1999). The Cambridge Dictionary of Philosophy (2nd ed.). Cambridge: Cambridge University Press. p. 43. ISBN 0-521-63136-X. Argument: a sequence of statements such that some of them (the premises) purport to give reasons to accept another of them, the conclusion
 Gullberg, Jan (1997). Mathematics : From the Birth of Numbers. New York: W. W. Norton & Company. p. 216. ISBN 0-393-04002-X.
 Byrne, Patrick Hugh (1997). Analysis and Science in Aristotle. New York: State University of New York Press. p. 43. ISBN 0791433218.
 Ryan, John (2018). Studies in Philosophy and the History of Philosophy, Volume 1. Washington, D.C.: CUA Press. p. 178. ISBN 9780813231129.
 Potts, Robert (1864). Euclid's Elements of Geometry, Book 1. London: Longman, Green, Longman, Roberts, & Green. p. 50.
 Luckhardt, C. Grant; Bechtel, William (1994). How to Do Things with Logic. Hillsdale, NJ: Lawrence Erlbaum Associates, Publishers. p. 13. ISBN 0805800751.

The Hubble eXtreme Deep Field (XDF) was completed in September 2012 and shows the farthest galaxies ever photographed. Except for the few stars in the foreground (which are bright and easily recognizable because only they have diffraction spikes), every speck of light in the photo is an individual galaxy, some of them as old as 13.2 billion years; the observable universe is estimated to contain more than 2 trillion galaxies.[1]
Cosmology (from the Greek κόσμος, kosmos "world" and -λογία, -logia "study of") is a branch of astronomy concerned with the studies of the origin and evolution of the universe, from the Big Bang to today and on into the future. It is the scientific study of the origin, evolution, and eventual fate of the universe. Physical cosmology is the scientific study of the universe's origin, its large-scale structures and dynamics, and its ultimate fate, as well as the laws of science that govern these areas.[2]

The term cosmology was first used in English in 1656 in Thomas Blount's Glossographia,[3] and in 1731 taken up in Latin by German philosopher Christian Wolff, in Cosmologia Generalis.[4]

Religious or mythological cosmology is a body of beliefs based on mythological, religious, and esoteric literature and traditions of creation myths and eschatology.

Physical cosmology is studied by scientists, such as astronomers and physicists, as well as philosophers, such as metaphysicians, philosophers of physics, and philosophers of space and time. Because of this shared scope with philosophy, theories in physical cosmology may include both scientific and non-scientific propositions, and may depend upon assumptions that cannot be tested. Cosmology differs from astronomy in that the former is concerned with the Universe as a whole while the latter deals with individual celestial objects. Modern physical cosmology is dominated by the Big Bang theory, which attempts to bring together observational astronomy and particle physics;[5][6] more specifically, a standard parameterization of the Big Bang with dark matter and dark energy, known as the Lambda-CDM model.

Theoretical astrophysicist David N. Spergel has described cosmology as a "historical science" because "when we look out in space, we look back in time" due to the finite nature of the speed of light.[7]


The image above contains clickable links(See also: Human timeline, and Life timeline.)
Physics and astrophysics have played a central role in shaping the understanding of the universe through scientific observation and experiment. Physical cosmology was shaped through both mathematics and observation in an analysis of the whole universe. The universe is generally understood to have begun with the Big Bang, followed almost instantaneously by cosmic inflation; an expansion of space from which the universe is thought to have emerged 13.799 ± 0.021 billion years ago.[8] Cosmogony studies the origin of the Universe, and cosmography maps the features of the Universe.

In Diderot's Encyclopédie, cosmology is broken down into uranology (the science of the heavens), aerology (the science of the air), geology (the science of the continents), and hydrology (the science of waters).[9]

Metaphysical cosmology has also been described as the placing of humans in the universe in relationship to all other entities. This is exemplified by Marcus Aurelius's observation that a man's place in that relationship: "He who does not know what the world is does not know where he is, and he who does not know for what purpose the world exists, does not know who he is, nor what the world is."[10]

Physical cosmology is the branch of physics and astrophysics that deals with the study of the physical origins and evolution of the Universe. It also includes the study of the nature of the Universe on a large scale. In its earliest form, it was what is now known as "celestial mechanics", the study of the heavens. Greek philosophers Aristarchus of Samos, Aristotle, and Ptolemy proposed different cosmological theories. The geocentric Ptolemaic system was the prevailing theory until the 16th century when Nicolaus Copernicus, and subsequently Johannes Kepler and Galileo Galilei, proposed a heliocentric system. This is one of the most famous examples of epistemological rupture in physical cosmology.


Evidence of gravitational waves in the infant universe may have been uncovered by the microscopic examination of the focal plane of the BICEP2 radio telescope.[11][12][13]
Isaac Newton's Principia Mathematica, published in 1687, was the first description of the law of universal gravitation. It provided a physical mechanism for Kepler's laws and also allowed the anomalies in previous systems, caused by gravitational interaction between the planets, to be resolved. A fundamental difference between Newton's cosmology and those preceding it was the Copernican principle—that the bodies on earth obey the same physical laws as all the celestial bodies. This was a crucial philosophical advance in physical cosmology.

Modern scientific cosmology is usually considered to have begun in 1917 with Albert Einstein's publication of his final modification of general relativity in the paper "Cosmological Considerations of the General Theory of Relativity" (although this paper was not widely available outside of Germany until the end of World War I). General relativity prompted cosmogonists such as Willem de Sitter, Karl Schwarzschild, and Arthur Eddington to explore its astronomical ramifications, which enhanced the ability of astronomers to study very distant objects. Physicists began changing the assumption that the Universe was static and unchanging. In 1922 Alexander Friedmann introduced the idea of an expanding universe that contained moving matter. Around the same time (1917 to 1922) the Great Debate took place, with early cosmologists such as Heber Curtis and Ernst Öpik determining that some nebulae seen in telescopes were separate galaxies far distant from our own.

In parallel to this dynamic approach to cosmology, one long-standing debate about the structure of the cosmos was coming to a climax. Mount Wilson astronomer Harlow Shapley championed the model of a cosmos made up of the Milky Way star system only; while Heber D. Curtis argued for the idea that spiral nebulae were star systems in their own right as island universes. This difference of ideas came to a climax with the organization of the Great Debate on 26 April 1920 at the meeting of the U.S. National Academy of Sciences in Washington, D.C. The debate was resolved when Edwin Hubble detected Cepheid Variables in the Andromeda Galaxy in 1923 and 1924. Their distance established spiral nebulae well beyond the edge of the Milky Way.

Subsequent modelling of the universe explored the possibility that the cosmological constant, introduced by Einstein in his 1917 paper, may result in an expanding universe, depending on its value. Thus the Big Bang model was proposed by the Belgian priest Georges Lemaître in 1927 which was subsequently corroborated by Edwin Hubble's discovery of the redshift in 1929 and later by the discovery of the cosmic microwave background radiation by Arno Penzias and Robert Woodrow Wilson in 1964. These findings were a first step to rule out some of many alternative cosmologies.

Since around 1990, several dramatic advances in observational cosmology have transformed cosmology from a largely speculative science into a predictive science with precise agreement between theory and observation. These advances include observations of the microwave background from the COBE, WMAP and Planck satellites, large new galaxy redshift surveys including 2dfGRS and SDSS, and observations of distant supernovae and gravitational lensing. These observations matched the predictions of the cosmic inflation theory, a modified Big Bang theory, and the specific version known as the Lambda-CDM model. This has led many to refer to modern times as the "golden age of cosmology".[14]

On 17 March 2014, astronomers at the Harvard-Smithsonian Center for Astrophysics announced the detection of gravitational waves, providing strong evidence for inflation and the Big Bang.[11][12][13] However, on 19 June 2014, lowered confidence in confirming the cosmic inflation findings was reported.[15][16][17]

On 1 December 2014, at the Planck 2014 meeting in Ferrara, Italy, astronomers reported that the universe is 13.8 billion years old and is composed of 4.9% atomic matter, 26.6% dark matter and 68.5% dark energy.[18]


Religious or mythological cosmology is a body of beliefs based on mythological, religious, and esoteric literature and traditions of creation and eschatology.

Cosmology deals with the world as the totality of space, time and all phenomena. Historically, it has had quite a broad scope, and in many cases was founded in religion.[19] In modern use metaphysical cosmology addresses questions about the Universe which are beyond the scope of science. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods like dialectics. Modern metaphysical cosmology tries to address questions such as:[11][20]

What is the origin of the Universe? What is its first cause? Is its existence necessary? (see monism, pantheism, emanationism and creationism)
What are the ultimate material components of the Universe? (see mechanism, dynamism, hylomorphism, atomism)
What is the ultimate reason for the existence of the Universe? Does the cosmos have a purpose? (see teleology)
Does the existence of consciousness have a purpose? How do we know what we know about the totality of the cosmos? Does cosmological reasoning reveal metaphysical truths? (see epistemology)
Historical cosmologies
Further information: Timeline of cosmological theories and Nicolaus Copernicus § Copernican system

Kinematic expansion without space expansion	Rejects general relativity and the expanding space paradigm. Gravity not included as initial assumption. Obeys cosmological principle and special relativity; consists of a finite spherical cloud of particles (or galaxies) that expands within an infinite and otherwise empty flat space. It has a center and a cosmic edge (surface of the particle cloud) that expands at light speed. Explanation of gravity was elaborate and unconvincing.
Friedmann–Lemaître–Robertson–Walker class of models	Howard Robertson, Arthur Walker, 1935	Uniformly expanding	Class of universes that are homogeneous and isotropic. Spacetime separates into uniformly curved space and cosmic time common to all co-moving observers. The formulation system is now known as the FLRW or Robertson–Walker metrics of cosmic time and curved space.
Steady-state	Hermann Bondi, Thomas Gold, 1948	Expanding, steady state, infinite	Matter creation rate maintains constant density. Continuous creation out of nothing from nowhere. Exponential expansion. Deceleration term q = −1.
Steady-state	Fred Hoyle 1948	Expanding, steady state; but unstable	Matter creation rate maintains constant density. But since matter creation rate must be exactly balanced with the space expansion rate the system is unstable.
Ambiplasma	Hannes Alfvén 1965 Oskar Klein	Cellular universe, expanding by means of matter–antimatter annihilation	Based on the concept of plasma cosmology. The universe is viewed as "meta-galaxies" divided by double layers and thus a bubble-like nature. Other universes are formed from other bubbles. Ongoing cosmic matter-antimatter annihilations keep the bubbles separated and moving apart preventing them from interacting.
Brans–Dicke theory	Carl H. Brans, Robert H. Dicke	Expanding	Based on Mach's principle. G varies with time as universe expands. "But nobody is quite sure what Mach's principle actually means."[citation needed]
Cosmic inflation	Alan Guth 1980	Big Bang modified to solve horizon and flatness problems	Based on the concept of hot inflation. The universe is viewed as a multiple quantum flux – hence its bubble-like nature. Other universes are formed from other bubbles. Ongoing cosmic expansion kept the bubbles separated and moving apart.
Eternal inflation (a multiple universe model)	Andreï Linde, 1983	Big Bang with cosmic inflation	Multiverse based on the concept of cold inflation, in which inflationary events occur at random each with independent initial conditions; some expand into bubble universes supposedly like our entire cosmos. Bubbles nucleate in a spacetime foam.
Cyclic model	Paul Steinhardt; Neil Turok 2002	Expanding and contracting in cycles; M-theory.	Two parallel orbifold planes or M-branes collide periodically in a higher-dimensional space. With quintessence or dark energy.
Cyclic model	Lauris Baum; Paul Frampton 2007	Solution of Tolman's entropy problem	Phantom dark energy fragments universe into large number of disconnected patches. Our patch contracts containing only dark energy with zero entropy.
Table notes: the term "static" simply means not expanding and not contracting. Symbol G represents Newton's gravitational constant; Λ (Lambda) is the cosmological constant.

In metaphysics, a universal is what particular things have in common, namely characteristics or qualities. In other words, universals are repeatable or recurrent entities that can be instantiated or exemplified by many particular things.[1] For example, suppose there are two chairs in a room, each of which is green. These two chairs both share the quality of "chairness", as well as greenness or the quality of being green; in other words, they share a "universal". There are three major kinds of qualities or characteristics: types or kinds (e.g. mammal), properties (e.g. short, strong), and relations (e.g. father of, next to). These are all different types of universals.[2]

Paradigmatically, universals are abstract (e.g. humanity), whereas particulars are concrete (e.g. the personhood of Socrates). However, universals are not necessarily abstract and particulars are not necessarily concrete.[3] For example, one might hold that numbers are particular yet abstract objects. Likewise, some philosophers, such as D. M. Armstrong, consider universals to be concrete.

Most do not consider classes to be universals, although some prominent philosophers do, as John Bigelow.


The history of any creation went through a process of qualifications meeting dependancies of that type thing, including all parts put together to make it an accepted thing of its particular type. A chair must first exist upon a surface with the force of gravity upon it. The chair must be upon something solid and it must provide a platform for something to sit upon. Any other universals for “chairness” must qualify the particular dependencies set forth by authority. The first chair qualified itself as a chair from it’s propriety. Universals exist in every created thing, but only in the individual subparts themselves, not in the whole thing itself. Universals can be thought of as an evolution of a creation’s life constantly on a journey towards perfection.


The problem of universals is an ancient problem in metaphysics about whether universals exist. The problem arises from attempts to account for the phenomenon of similarity or attribute agreement among things.[4] For example, grass and Granny Smith apples are similar or agree in attribute, namely in having the attribute of greenness. The issue is how to account for this sort of agreement in attribute among things.

There are many philosophical positions regarding universals. Taking "beauty" as an example, three positions are:

Idealism or conceptualism: beauty is a property constructed in the mind, so it exists only in descriptions of things.
Platonic realism: beauty is a property that exists in an ideal form independently of any mind or thing.
Aristotelian realism: beauty is a property that only exists when beautiful things exist.
Taking a broader view, the main positions are generally considered classifiable as: realism, nominalism, and idealism (sometimes simply named "anti-realism" with regard to universals).[5] Realists posit the existence of independent, abstract universals to account for attribute agreement. Nominalists deny that universals exist, claiming that they are not necessary to explain attribute agreement. Conceptualists posit that universals exist only in the mind, or when conceptualized, denying the independent existence of universals. Complications which arise include the implications of language use and the complexity of relating language to ontology.

A universal may have instances, known as its particulars. For example, the type dog (or doghood) is a universal, as are the property red (or redness) and the relation betweenness (or being between). Any particular dog, red thing, or object that is between other things is not a universal, however, but is an instance of a universal. That is, a universal type (doghood), property (redness), or relation (betweenness) inheres in a particular object (a specific dog, red thing, or object between other things).


Platonic realism holds universals to be the referents of general terms, such as the abstract, nonphysical, non-mental entities to which words such as "sameness", "circularity", and "beauty" refer. Particulars are the referents of proper names, such as "Phaedo," or of definite descriptions that identify single objects, such as the phrase, "that bed over there". Other metaphysical theories may use the terminology of universals to describe physical entities.

Plato's examples of what we might today call universals included mathematical and geometrical ideas such as a circle and natural numbers as universals. Plato's views on universals did, however, vary across several different discussions. In some cases, Plato spoke as if the perfect circle functioned as the form or blueprint for all copies and for the word definition of circle. In other discussions, Plato describes particulars as "participating" in the associated universal.

Contemporary realists agree with the thesis that universals are multiply-exemplifiable entities. Examples include by D. M. Armstrong, Nicholas Wolterstorff, Reinhardt Grossmann, Michael Loux.

Nominalists hold that universals are not real mind-independent entities but either merely concepts (sometimes called "conceptualism") or merely names. Nominalists typically argue that properties are abstract particulars (like tropes) rather than universals. JP Moreland distinguishes between "extreme" and "moderate" nominalism.[6] Examples of nominalists include the medieval philosophers Roscelin of Compiègne and William of Ockham and contemporary philosophers W. V. O. Quine, Wilfred Sellars, D. C. Williams, and Keith Campbell.


The ness-ity-hood principle is used mainly by English-speaking philosophers to generate convenient, concise names for universals or properties.[7] According to the Ness-Ity-Hood Principle, a name for any universal may be formed that is distinctive, "of left-handers" may be formed by taking the predicate "left-handed" and adding "ness", which yields the name "left-handedness". The principle is most helpful in cases where there is not an established or standard name of the universal in ordinary English usage: What is the name of the universal distinctive of chairs? "Chair" in English is used not only as a subject (as in "The chair is broken"), but also as a predicate (as in "That is a chair"). So to generate a name for the universal distinctive of chairs, take the predicate "chair" and add "ness", which yields "chairness".

Memory is the faculty of the brain by which data or information is encoded, stored, and retrieved when needed. It is the retention of information over time for the purpose of influencing future action.[1] If past events could not be remembered, it would be impossible for language, relationships, or personal identity to develop.[2] Memory loss is usually described as forgetfulness or amnesia.[3][4][5][6][7][8]

Memory is often understood as an informational processing system with explicit and implicit functioning that is made up of a sensory processor, short-term (or working) memory, and long-term memory.[9] This can be related to the neuron. The sensory processor allows information from the outside world to be sensed in the form of chemical and physical stimuli and attended to various levels of focus and intent. Working memory serves as an encoding and retrieval processor. Information in the form of stimuli is encoded in accordance with explicit or implicit functions by the working memory processor. The working memory also retrieves information from previously stored material. Finally, the function of long-term memory is to store data through various categorical models or systems.[9]

Declarative, or explicit, memory is the conscious storage and recollection of data.[10] Under declarative memory resides semantic and episodic memory. Semantic memory refers to memory that is encoded with specific meaning,[2] while episodic memory refers to information that is encoded along a spatial and temporal plane.[11][12][13] Declarative memory is usually the primary process thought of when referencing memory.[2] Non-declarative, or implicit, memory is the unconscious storage and recollection of information.[14] An example of a non-declarative process would be the unconscious learning or retrieval of information by way of procedural memory, or a priming phenomenon.[2][14][15] Priming is the process of subliminally arousing specific responses from memory and shows that not all memory is consciously activated,[15] whereas procedural memory is the slow and gradual learning of skills that often occurs without conscious attention to learning.[2][14]

Memory is not a perfect processor, and is affected by many factors. The ways by which information is encoded, stored, and retrieved can all be corrupted. The amount of attention given new stimuli can diminish the amount of information that becomes encoded for storage.[2] Also, the storage process can become corrupted by physical damage to areas of the brain that are associated with memory storage, such as the hippocampus.[16][17] Finally, the retrieval of information from long-term memory can be disrupted because of decay within long-term memory.[2] Normal functioning, decay over time, and brain damage all affect the accuracy and capacity of the memory.[18][19]



Sensory memory holds information, derived from the senses, less than one second after an item is perceived. The ability to look at an item and remember what it looked like with just a split second of observation, or memorization, is the example of sensory memory. It is out of cognitive control and is an automatic response. With very short presentations, participants often report that they seem to "see" more than they can actually report. The first experiments exploring this form of sensory memory were precisely conducted by George Sperling (1963)[20] using the "partial report paradigm." Subjects were presented with a grid of 12 letters, arranged into three rows of four. After a brief presentation, subjects were then played either a high, medium or low tone, cuing them which of the rows to report. Based on these partial report experiments, Sperling was able to show that the capacity of sensory memory was approximately 12 items, but that it degraded very quickly (within a few hundred milliseconds). Because this form of memory degrades so quickly, participants would see the display but be unable to report all of the items (12 in the "whole report" procedure) before they decayed. This type of memory cannot be prolonged via rehearsal.

Three types of sensory memories exist. Iconic memory is a fast decaying store of visual information, a type of sensory memory that briefly stores an image that has been perceived for a small duration. Echoic memory is a fast decaying store of auditory information, also a sensory memory that briefly stores sounds that have been perceived for short durations.[21] Haptic memory is a type of sensory memory that represents a database for touch stimuli.


Short-term memory is also known as working memory. Short-term memory allows recall for a period of several seconds to a minute without rehearsal. Its capacity, however, is very limited. In 1956, George A. Miller (1920-2012), when working at Bell Laboratories, conducted experiments showing that the store of short-term memory was 7±2 items. (Hence, the title of his famous paper, "The Magical Number 7±2.") Modern estimates of the capacity of short-term memory are lower, typically of the order of 4–5 items;[22] however, memory capacity can be increased through a process called chunking.[23] For example, in recalling a ten-digit telephone number, a person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456), and, last, a four-digit chunk (7890). This method of remembering telephone numbers is far more effective than attempting to remember a string of 10 digits; this is because we are able to chunk the information into meaningful groups of numbers. This is reflected in some countries' tendencies to display telephone numbers as several chunks of two to four numbers.

Short-term memory is believed to rely mostly on an acoustic code for storing information, and to a lesser extent on a visual code. Conrad (1964)[24] found that test subjects had more difficulty recalling collections of letters that were acoustically similar, e.g., E, P, D. Confusion with recalling acoustically similar letters rather than visually similar letters implies that the letters were encoded acoustically. Conrad's (1964) study, however, deals with the encoding of written text; thus, while memory of written language may rely on acoustic components, generalizations to all forms of memory cannot be made.



Olin Levi Warner, Memory (1896). Library of Congress Thomas Jefferson Building, Washington, D.C.
The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration, which means that information is not retained indefinitely. By contrast, long-term memory can store much larger quantities of information for potentially unlimited duration (sometimes a whole life span). Its capacity is immeasurable. For example, given a random seven-digit number, one may remember it for only a few seconds before forgetting, suggesting it was stored in short-term memory. On the other hand, one can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory.

While short-term memory encodes information acoustically, long-term memory encodes it semantically: Baddeley (1966)[25] discovered that, after 20 minutes, test subjects had the most difficulty recalling a collection of words that had similar meanings (e.g. big, large, great, huge) long-term. Another part of long-term memory is episodic memory, "which attempts to capture information such as 'what', 'when' and 'where'".[26] With episodic memory, individuals are able to recall specific events such as birthday parties and weddings.

Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. It was thought that without the hippocampus new memories were unable to be stored into long-term memory and that there would be a very short attention span, as first gleaned from patient Henry Molaison[27] after what was thought to be the full removal of both his hippocampi. More recent examination of his brain, post-mortem, shows that the hippocampus was more intact than first thought, throwing theories drawn from the initial data into question. The hippocampus may be involved in changing neural connections for a period of three months or more after the initial learning.

Research has suggested that long-term memory storage in humans may be maintained by DNA methylation,[28] and the 'prion' gene.[29][30]

The multi-store model (also known as Atkinson–Shiffrin memory model) was first described in 1968 by Atkinson and Shiffrin.

The multi-store model has been criticised for being too simplistic. For instance, long-term memory is believed to be actually made up of multiple subcomponents, such as episodic and procedural memory. It also proposes that rehearsal is the only mechanism by which information eventually reaches long-term storage, but evidence shows us capable of remembering things without rehearsal.

The model also shows all the memory stores as being a single unit whereas research into this shows differently. For example, short-term memory can be broken up into different units such as visual information and acoustic information. In a study by Zlonoga and Gerber (1986), patient 'KF' demonstrated certain deviations from the Atkinson–Shiffrin model. Patient KF was brain damaged, displaying difficulties regarding short-term memory. Recognition of sounds such as spoken numbers, letters, words and easily identifiable noises (such as doorbells and cats meowing) were all impacted. Visual short-term memory was unaffected, suggesting a dichotomy between visual and audial memory.[31]


In 1974 Baddeley and Hitch proposed a "working memory model" that replaced the general concept of short-term memory with an active maintenance of information in the short-term storage. In this model, working memory consists of three basic stores: the central executive, the phonological loop and the visuo-spatial sketchpad. In 2000 this model was expanded with the multimodal episodic buffer (Baddeley's model of working memory).[32]

The central executive essentially acts as an attention sensory store. It channels information to the three component processes: the phonological loop, the visuo-spatial sketchpad, and the episodic buffer.

The phonological loop stores auditory information by silently rehearsing sounds or words in a continuous loop: the articulatory process (for example the repetition of a telephone number over and over again). A short list of data is easier to remember.

The visuospatial sketchpad stores visual and spatial information. It is engaged when performing spatial tasks (such as judging distances) or visual ones (such as counting the windows on a house or imagining images).

The episodic buffer is dedicated to linking information across domains to form integrated units of visual, spatial, and verbal information and chronological ordering (e.g., the memory of a story or a movie scene). The episodic buffer is also assumed to have links to long-term memory and semantical meaning.

The working memory model explains many practical observations, such as why it is easier to do two different tasks (one verbal and one visual) than two similar tasks (e.g., two visual), and the aforementioned word-length effect. Working memory is also the premise for what allows us to do everyday activities involving thought. It is the section of memory where we carry out thought processes and use them to learn and reason about topics.[32]


Researchers distinguish between recognition and recall memory. Recognition memory tasks require individuals to indicate whether they have encountered a stimulus (such as a picture or a word) before. Recall memory tasks require participants to retrieve previously learned information. For example, individuals might be asked to produce a series of actions they have seen before or to say a list of words they have heard before.


Topographic memory involves the ability to orient oneself in space, to recognize and follow an itinerary, or to recognize familiar places.[33] Getting lost when traveling alone is an example of the failure of topographic memory.[34]

Flashbulb memories are clear episodic memories of unique and highly emotional events.[35] People remembering where they were or what they were doing when they first heard the news of President Kennedy's assassination,[36] the Sydney Siege or of 9/11 are examples of flashbulb memories.

Anderson (1976)[37] divides long-term memory into declarative (explicit) and procedural (implicit) memories.

Declarative memory requires conscious recall, in that some conscious process must call back the information. It is sometimes called explicit memory, since it consists of information that is explicitly stored and retrieved.

Declarative memory can be further sub-divided into semantic memory, concerning principles and facts taken independent of context; and episodic memory, concerning information specific to a particular context, such as a time and place. Semantic memory allows the encoding of abstract knowledge about the world, such as "Paris is the capital of France". Episodic memory, on the other hand, is used for more personal memories, such as the sensations, emotions, and personal associations of a particular place or time. Episodic memories often reflect the "firsts" in life such as a first kiss, first day of school or first time winning a championship. These are key events in one's life that can be remembered clearly. Research suggests that declarative memory is supported by several functions of the medial temporal lobe system which includes the hippocampus.[38] Autobiographical memory – memory for particular events within one's own life – is generally viewed as either equivalent to, or a subset of, episodic memory. Visual memory is part of memory preserving some characteristics of our senses pertaining to visual experience. One is able to place in memory information that resembles objects, places, animals or people in sort of a mental image. Visual memory can result in priming and it is assumed some kind of perceptual representational system underlies this phenomenon.[38]


In contrast, procedural memory (or implicit memory) is not based on the conscious recall of information, but on implicit learning. It can best be summarized as remembering how to do something. Procedural memory is primarily used in learning motor skills and can be considered a subset of implicit memory. It is revealed when one does better in a given task due only to repetition – no new explicit memories have been formed, but one is unconsciously accessing aspects of those previous experiences. Procedural memory involved in motor learning depends on the cerebellum and basal ganglia.[39]

A characteristic of procedural memory is that the things remembered are automatically translated into actions, and thus sometimes difficult to describe. Some examples of procedural memory include the ability to ride a bike or tie shoelaces.[40]


Another major way to distinguish different memory functions is whether the content to be remembered is in the past, retrospective memory, or in the future, prospective memory. Thus, retrospective memory as a category includes semantic, episodic and autobiographical memory. In contrast, prospective memory is memory for future intentions, or remembering to remember (Winograd, 1988). Prospective memory can be further broken down into event- and time-based prospective remembering. Time-based prospective memories are triggered by a time-cue, such as going to the doctor (action) at 4pm (cue). Event-based prospective memories are intentions triggered by cues, such as remembering to post a letter (action) after seeing a mailbox (cue). Cues do not need to be related to the action (as the mailbox/letter example), and lists, sticky-notes, knotted handkerchiefs, or string around the finger all exemplify cues that people use as strategies to enhance prospective memory.


Infants do not have the language ability to report on their memories and so verbal reports cannot be used to assess very young children's memory. Throughout the years, however, researchers have adapted and developed a number of measures for assessing both infants' recognition memory and their recall memory. Habituation and operant conditioning techniques have been used to assess infants' recognition memory and the deferred and elicited imitation techniques have been used to assess infants' recall memory.

Techniques used to assess infants' recognition memory include the following:

Visual paired comparison procedure (relies on habituation): infants are first presented with pairs of visual stimuli, such as two black-and-white photos of human faces, for a fixed amount of time; then, after being familiarized with the two photos, they are presented with the "familiar" photo and a new photo. The time spent looking at each photo is recorded. Looking longer at the new photo indicates that they remember the "familiar" one. Studies using this procedure have found that 5- to 6-month-olds can retain information for as long as fourteen days.[41]
Operant conditioning technique: infants are placed in a crib and a ribbon that is connected to a mobile overhead is tied to one of their feet. Infants notice that when they kick their foot the mobile moves – the rate of kicking increases dramatically within minutes. Studies using this technique have revealed that infants' memory substantially improves over the first 18-months. Whereas 2- to 3-month-olds can retain an operant response (such as activating the mobile by kicking their foot) for a week, 6-month-olds can retain it for two weeks, and 18-month-olds can retain a similar operant response for as long as 13 weeks.[42][43][44]
Techniques used to assess infants' recall memory include the following:

Deferred imitation technique: an experimenter shows infants a unique sequence of actions (such as using a stick to push a button on a box) and then, after a delay, asks the infants to imitate the actions. Studies using deferred imitation have shown that 14-month-olds' memories for the sequence of actions can last for as long as four months.[45]
Elicited imitation technique: is very similar to the deferred imitation technique; the difference is that infants are allowed to imitate the actions before the delay. Studies using the elicited imitation technique have shown that 20-month-olds can recall the action sequences twelve months later.[46][47]
To assess children and older adults
Researchers use a variety of tasks to assess older children and adults' memory. Some examples are:

Paired associate learning – when one learns to associate one specific word with another. For example, when given a word such as "safe" one must learn to say another specific word, such as "green". This is stimulus and response.[48][49]
Free recall – during this task a subject would be asked to study a list of words and then later they will be asked to recall or write down as many words that they can remember, similar to free response questions.[50] Earlier items are affected by retroactive interference (RI), which means the longer the list, the greater the interference, and the less likelihood that they are recalled. On the other hand, items that have been presented lastly suffer little RI, but suffer a great deal from proactive interference (PI), which means the longer the delay in recall, the more likely that the items will be lost.[51]
Cued recall – one is given a significant hints to help retrieve information that has been previously encoded into the person's memory; typically this can involve a word relating to the information being asked to remember.[52] This is similar to fill in the blank assessments used in classrooms.
Recognition – subjects are asked to remember a list of words or pictures, after which point they are asked to identify the previously presented words or pictures from among a list of alternatives that were not presented in the original list.[53] This is similar to multiple choice assessments.
Detection paradigm – individuals are shown a number of objects and color samples during a certain period of time. They are then tested on their visual ability to remember as much as they can by looking at testers and pointing out whether the testers are similar to the sample, or if any change is present.
Savings method – compares the speed of originally learning to the speed of relearning it. The amount of time saved measures memory.[54]
Implicit-memory tasks – information is drawn from memory without conscious realization.
Failures


Transience – memories degrade with the passing of time. This occurs in the storage stage of memory, after the information has been stored and before it is retrieved. This can happen in sensory, short-term, and long-term storage. It follows a general pattern where the information is rapidly forgotten during the first couple of days or years, followed by small losses in later days or years.
Absent-mindedness – Memory failure due to the lack of attention. Attention plays a key role in storing information into long-term memory; without proper attention, the information might not be stored, making it impossible to be retrieved later.

Brain areas involved in the neuroanatomy of memory such as the hippocampus, the amygdala, the striatum, or the mammillary bodies are thought to be involved in specific types of memory. For example, the hippocampus is believed to be involved in spatial learning and declarative learning, while the amygdala is thought to be involved in emotional memory.[55]

Damage to certain areas in patients and animal models and subsequent memory deficits is a primary source of information. However, rather than implicating a specific area, it could be that damage to adjacent areas, or to a pathway traveling through the area is actually responsible for the observed deficit. Further, it is not sufficient to describe memory, and its counterpart, learning, as solely dependent on specific brain regions. Learning and memory are usually attributed to changes in neuronal synapses, thought to be mediated by long-term potentiation and long-term depression. However, this has been questioned on computational as well as neurophysiological grounds by the cognitive scientist Charles R. Gallistel and others.[56][57][58]

In general, the more emotionally charged an event or experience is, the better it is remembered; this phenomenon is known as the memory enhancement effect. Patients with amygdala damage, however, do not show a memory enhancement effect.[59][60]

Hebb distinguished between short-term and long-term memory. He postulated that any memory that stayed in short-term storage for a long enough time would be consolidated into a long-term memory. Later research showed this to be false. Research has shown that direct injections of cortisol or epinephrine help the storage of recent experiences. This is also true for stimulation of the amygdala. This proves that excitement enhances memory by the stimulation of hormones that affect the amygdala. Excessive or prolonged stress (with prolonged cortisol) may hurt memory storage. Patients with amygdalar damage are no more likely to remember emotionally charged words than nonemotionally charged ones. The hippocampus is important for explicit memory. The hippocampus is also important for memory consolidation. The hippocampus receives input from different parts of the cortex and sends its output out to different parts of the brain also. The input comes from secondary and tertiary sensory areas that have processed the information a lot already. Hippocampal damage may also cause memory loss and problems with memory storage.[61] This memory loss includes retrograde amnesia which is the loss of memory for events that occurred shortly before the time of brain damage.[54]


Cognitive neuroscientists consider memory as the retention, reactivation, and reconstruction of the experience-independent internal representation. The term of internal representation implies that such a definition of memory contains two components: the expression of memory at the behavioral or conscious level, and the underpinning physical neural changes (Dudai 2007). The latter component is also called engram or memory traces (Semon 1904). Some neuroscientists and psychologists mistakenly equate the concept of engram and memory, broadly conceiving all persisting after-effects of experiences as memory; others argue against this notion that memory does not exist until it is revealed in behavior or thought (Moscovitch 2007).

One question that is crucial in cognitive neuroscience is how information and mental experiences are coded and represented in the brain. Scientists have gained much knowledge about the neuronal codes from the studies of plasticity, but most of such research has been focused on simple learning in simple neuronal circuits; it is considerably less clear about the neuronal changes involved in more complex examples of memory, particularly declarative memory that requires the storage of facts and events (Byrne 2007). Convergence-divergence zones might be the neural networks where memories are stored and retrieved. Considering that there are several kinds of memory, depending on types of represented knowledge, underlying mechanisms, processes functions and modes of acquisition, it is likely that different brain areas support different memory systems and that they are in mutual relationships in neuronal networks: "components of memory representation are distributed widely across different parts of the brain as mediated by multiple neocortical circuits".[62]

Encoding. Encoding of working memory involves the spiking of individual neurons induced by sensory input, which persists even after the sensory input disappears (Jensen and Lisman 2005; Fransen et al. 2002). Encoding of episodic memory involves persistent changes in molecular structures that alter synaptic transmission between neurons. Examples of such structural changes include long-term potentiation (LTP) or spike-timing-dependent plasticity (STDP). The persistent spiking in working memory can enhance the synaptic and cellular changes in the encoding of episodic memory (Jensen and Lisman 2005).
Working memory. Recent functional imaging studies detected working memory signals in both medial temporal lobe (MTL), a brain area strongly associated with long-term memory, and prefrontal cortex (Ranganath et al. 2005), suggesting a strong relationship between working memory and long-term memory. However, the substantially more working memory signals seen in the prefrontal lobe suggest that this area play a more important role in working memory than MTL (Suzuki 2007).
Consolidation and reconsolidation. Short-term memory (STM) is temporary and subject to disruption, while long-term memory (LTM), once consolidated, is persistent and stable. Consolidation of STM into LTM at the molecular level presumably involves two processes: synaptic consolidation and system consolidation. The former involves a protein synthesis process in the medial temporal lobe (MTL), whereas the latter transforms the MTL-dependent memory into an MTL-independent memory over months to years (Ledoux 2007). In recent years, such traditional consolidation dogma has been re-evaluated as a result of the studies on reconsolidation. These studies showed that prevention after retrieval affects subsequent retrieval of the memory (Sara 2000). New studies have shown that post-retrieval treatment with protein synthesis inhibitors and many other compounds can lead to an amnestic state (Nadel et al. 2000b; Alberini 2005; Dudai 2006). These findings on reconsolidation fit with the behavioral evidence that retrieved memory is not a carbon copy of the initial experiences, and memories are updated during retrieval.

Study of the genetics of human memory is in its infancy though many genes have been investigated for their association to memory in humans and non-human animals. A notable initial success was the association of APOE with memory dysfunction in Alzheimer's disease. The search for genes associated with normally varying memory continues. One of the first candidates for normal variation in memory is the protein KIBRA,[63] which appears to be associated with the rate at which material is forgotten over a delay period. There has been some evidence that memories are stored in the nucleus of neurons.[64][non-primary source needed]


Several genes, proteins and enzymes have been extensively researched for their association with memory. Long-term memory, unlike short-term memory, is dependent upon the synthesis of new proteins.[65] This occurs within the cellular body, and concerns the particular transmitters, receptors, and new synapse pathways that reinforce the communicative strength between neurons. The production of new proteins devoted to synapse reinforcement is triggered after the release of certain signaling substances (such as calcium within hippocampal neurons) in the cell. In the case of hippocampal cells, this release is dependent upon the expulsion of magnesium (a binding molecule) that is expelled after significant and repetitive synaptic signaling. The temporary expulsion of magnesium frees NMDA receptors to release calcium in the cell, a signal that leads to gene transcription and the construction of reinforcing proteins.[66] For more information, see long-term potentiation (LTP).

One of the newly synthesized proteins in LTP is also critical for maintaining long-term memory. This protein is an autonomously active form of the enzyme protein kinase C (PKC), known as PKMζ. PKMζ maintains the activity-dependent enhancement of synaptic strength and inhibiting PKMζ erases established long-term memories, without affecting short-term memory or, once the inhibitor is eliminated, the ability to encode and store new long-term memories is restored. Also, BDNF is important for the persistence of long-term memories.[67]

The long-term stabilization of synaptic changes is also determined by a parallel increase of pre- and postsynaptic structures such as axonal bouton, dendritic spine and postsynaptic density.[68] On the molecular level, an increase of the postsynaptic scaffolding proteins PSD-95 and HOMER1c has been shown to correlate with the stabilization of synaptic enlargement.[68] The cAMP response element-binding protein (CREB) is a transcription factor which is believed to be important in consolidating short-term to long-term memories, and which is believed to be downregulated in Alzheimer's disease.[69]


Rats exposed to an intense learning event may retain a life-long memory of the event, even after a single training session. The long-term memory of such an event appears to be initially stored in the hippocampus, but this storage is transient. Much of the long-term storage of the memory seems to take place in the anterior cingulate cortex.[70] When such an exposure was experimentally applied, more than 5,000 differently methylated DNA regions appeared in the hippocampus neuronal genome of the rats at one and at 24 hours after training.[71] These alterations in methylation pattern occurred at many genes that were down-regulated, often due to the formation of new 5-methylcytosine sites in CpG rich regions of the genome. Furthermore, many other genes were upregulated, likely often due to hypomethylation. Hypomethylation often results from the removal of methyl groups from previously existing 5-methylcytosines in DNA. Demethylation is carried out by several proteins acting in concert, including the TET enzymes as well as enzymes of the DNA base excision repair pathway (see Epigenetics in learning and memory). The pattern of induced and repressed genes in brain neurons subsequent to an intense learning event likely provides the molecular basis for a long-term memory of the event.


Main article: Epigenetics in learning and memory
Studies of the molecular basis for memory formation indicate that epigenetic mechanisms operating in brain neurons play a central role in determining this capability. Key epigenetic mechanisms involved in memory include the methylation and demethylation of neuronal DNA, as well as modifications of histone proteins including methylations, acetylations and deacetylations.

Stimulation of brain activity in memory formation is often accompanied by the generation of damage in neuronal DNA that is followed by repair associated with persistent epigenetic alterations. In particular the DNA repair processes of non-homologous end joining and base excision repair are employed in memory formation.[citation needed]


For the inability of adults to retrieve early memories, see Childhood amnesia.
Up until the mid-1980s it was assumed that infants could not encode, retain, and retrieve information.[72] A growing body of research now indicates that infants as young as 6-months can recall information after a 24-hour delay.[73] Furthermore, research has revealed that as infants grow older they can store information for longer periods of time; 6-month-olds can recall information after a 24-hour period, 9-month-olds after up to five weeks, and 20-month-olds after as long as twelve months.[74] In addition, studies have shown that with age, infants can store information faster. Whereas 14-month-olds can recall a three-step sequence after being exposed to it once, 6-month-olds need approximately six exposures in order to be able to remember it.[45][73]

Although 6-month-olds can recall information over the short-term, they have difficulty recalling the temporal order of information. It is only by 9 months of age that infants can recall the actions of a two-step sequence in the correct temporal order – that is, recalling step 1 and then step 2.[75][76] In other words, when asked to imitate a two-step action sequence (such as putting a toy car in the base and pushing in the plunger to make the toy roll to the other end), 9-month-olds tend to imitate the actions of the sequence in the correct order (step 1 and then step 2). Younger infants (6-month-olds) can only recall one step of a two-step sequence.[73] Researchers have suggested that these age differences are probably due to the fact that the dentate gyrus of the hippocampus and the frontal components of the neural network are not fully developed at the age of 6-months.[46][77][78]

In fact, the term 'infantile amnesia' refers to the phenomenon of accelerated forgetting during infancy. Importantly, infantile amnesia is not unique to humans, and preclinical research (using rodent models) provides insight into the precise neurobiology of this phenomenon. A review of the literature from behavioral neuroscientist Dr Jee Hyun Kim suggests that accelerated forgetting during early life is at least partly due to rapid growth of the brain during this period.[79]


One of the key concerns of older adults is the experience of memory loss, especially as it is one of the hallmark symptoms of Alzheimer's disease. However, memory loss is qualitatively different in normal aging from the kind of memory loss associated with a diagnosis of Alzheimer's (Budson & Price, 2005). Research has revealed that individuals' performance on memory tasks that rely on frontal regions declines with age. Older adults tend to exhibit deficits on tasks that involve knowing the temporal order in which they learned information;[80] source memory tasks that require them to remember the specific circumstances or context in which they learned information;[81] and prospective memory tasks that involve remembering to perform an act at a future time. Older adults can manage their problems with prospective memory by using appointment books, for example.

Gene transcription profiles were determined for the human frontal cortex of individuals from age 26 to 106 years. Numerous genes were identified with reduced expression after age 40, and especially after age 70.[82] Genes that play central roles in memory and learning were among those showing the most significant reduction with age. There was also a marked increase in DNA damage, likely oxidative damage, in the promoters of those genes with reduced expression. It was suggested that DNA damage may reduce the expression of selectively vulnerable genes involved in memory and learning.[82]


Much of the current knowledge of memory has come from studying memory disorders, particularly amnesia. Loss of memory is known as amnesia. Amnesia can result from extensive damage to: (a) the regions of the medial temporal lobe, such as the hippocampus, dentate gyrus, subiculum, amygdala, the parahippocampal, entorhinal, and perirhinal cortices[83] or the (b) midline diencephalic region, specifically the dorsomedial nucleus of the thalamus and the mammillary bodies of the hypothalamus.[84] There are many sorts of amnesia, and by studying their different forms, it has become possible to observe apparent defects in individual sub-systems of the brain's memory systems, and thus hypothesize their function in the normally working brain. Other neurological disorders such as Alzheimer's disease and Parkinson's disease[85] can also affect memory and cognition. Hyperthymesia, or hyperthymesic syndrome, is a disorder that affects an individual's autobiographical memory, essentially meaning that they cannot forget small details that otherwise would not be stored.[86] Korsakoff's syndrome, also known as Korsakoff's psychosis, amnesic-confabulatory syndrome, is an organic brain disease that adversely affects memory by widespread loss or shrinkage of neurons within the prefrontal cortex.[54]

While not a disorder, a common temporary failure of word retrieval from memory is the tip-of-the-tongue phenomenon. Sufferers of Anomic aphasia (also called Nominal aphasia or Anomia), however, do experience the tip-of-the-tongue phenomenon on an ongoing basis due to damage to the frontal and parietal lobes of the brain.


Interference can hamper memorization and retrieval. There is retroactive interference, when learning new information makes it harder to recall old information[87] and proactive interference, where prior learning disrupts recall of new information. Although interference can lead to forgetting, it is important to keep in mind that there are situations when old information can facilitate learning of new information. Knowing Latin, for instance, can help an individual learn a related language such as French – this phenomenon is known as positive transfer.[88]


Stress has a significant effect on memory formation and learning. In response to stressful situations, the brain releases hormones and neurotransmitters (ex. glucocorticoids and catecholamines) which affect memory encoding processes in the hippocampus. Behavioural research on animals shows that chronic stress produces adrenal hormones which impact the hippocampal structure in the brains of rats.[89] An experimental study by German cognitive psychologists L. Schwabe and O. Wolf demonstrates how learning under stress also decreases memory recall in humans.[90] In this study, 48 healthy female and male university students participated in either a stress test or a control group. Those randomly assigned to the stress test group had a hand immersed in ice cold water (the reputable SECPT or 'Socially Evaluated Cold Pressor Test') for up to three minutes, while being monitored and videotaped. Both the stress and control groups were then presented with 32 words to memorize. Twenty-four hours later, both groups were tested to see how many words they could remember (free recall) as well as how many they could recognize from a larger list of words (recognition performance). The results showed a clear impairment of memory performance in the stress test group, who recalled 30% fewer words than the control group. The researchers suggest that stress experienced during learning distracts people by diverting their attention during the memory encoding process.

However, memory performance can be enhanced when material is linked to the learning context, even when learning occurs under stress. A separate study by cognitive psychologists Schwabe and Wolf shows that when retention testing is done in a context similar to or congruent with the original learning task (i.e., in the same room), memory impairment and the detrimental effects of stress on learning can be attenuated.[91] Seventy-two healthy female and male university students, randomly assigned to the SECPT stress test or to a control group, were asked to remember the locations of 15 pairs of picture cards – a computerized version of the card game "Concentration" or "Memory". The room in which the experiment took place was infused with the scent of vanilla, as odour is a strong cue for memory. Retention testing took place the following day, either in the same room with the vanilla scent again present, or in a different room without the fragrance. The memory performance of subjects who experienced stress during the object-location task decreased significantly when they were tested in an unfamiliar room without the vanilla scent (an incongruent context); however, the memory performance of stressed subjects showed no impairment when they were tested in the original room with the vanilla scent (a congruent context). All participants in the experiment, both stressed and unstressed, performed faster when the learning and retrieval contexts were similar.[92]

This research on the effects of stress on memory may have practical implications for education, for eyewitness testimony and for psychotherapy: students may perform better when tested in their regular classroom rather than an exam room, eyewitnesses may recall details better at the scene of an event than in a courtroom, and persons suffering from post-traumatic stress may improve when helped to situate their memories of a traumatic event in an appropriate context.

Stressful life experiences may be a cause of memory loss as a person ages. Glucocorticoids that are released during stress, damage neurons that are located in the hippocampal region of the brain. Therefore, the more stressful situations that someone encounters, the more susceptible they are to memory loss later on. The CA1 neurons found in the hippocampus are destroyed due to glucocorticoids decreasing the release of glucose and the reuptake of glutamate. This high level of extracellular glutamate allows calcium to enter NMDA receptors which in return kills neurons. Stressful life experiences can also cause repression of memories where a person moves an unbearable memory to the unconscious mind.[54] This directly relates to traumatic events in one's past such as kidnappings, being prisoners of war or sexual abuse as a child.

The more long term the exposure to stress is, the more impact it may have. However, short term exposure to stress also causes impairment in memory by interfering with the function of the hippocampus. Research shows that subjects placed in a stressful situation for a short amount of time still have blood glucocorticoid levels that have increased drastically when measured after the exposure is completed. When subjects are asked to complete a learning task after short term exposure they often have difficulties. Prenatal stress also hinders the ability to learn and memorize by disrupting the development of the hippocampus and can lead to unestablished long term potentiation in the offspring of severely stressed parents. Although the stress is applied prenatally, the offspring show increased levels of glucocorticoids when they are subjected to stress later on in life.[93]


Making memories occurs through a three-step process, which can be enhanced by sleep. The three steps are as follows:

Acquisition which is the process of storage and retrieval of new information in memory

Sleep affects memory consolidation. During sleep, the neural connections in the brain are strengthened. This enhances the brain's abilities to stabilize and retain memories. There have been several studies which show that sleep improves the retention of memory, as memories are enhanced through active consolidation. System consolidation takes place during slow-wave sleep (SWS).[94] This process implicates that memories are reactivated during sleep, but that the process doesn't enhance every memory. It also implicates that qualitative changes are made to the memories when they are transferred to long-term store during sleep. During sleep, the hippocampus replays the events of the day for the neocortex. The neocortex then reviews and processes memories, which moves them into long-term memory. When one does not get enough sleep it makes it more difficult to learn as these neural connections are not as strong, resulting in a lower retention rate of memories. Sleep deprivation makes it harder to focus, resulting in inefficient learning.[94] Furthermore, some studies have shown that sleep deprivation can lead to false memories as the memories are not properly transferred to long-term memory. One of the primary functions of sleep is thought to be the improvement of the consolidation of information, as several studies have demonstrated that memory depends on getting sufficient sleep between training and test.[95] Additionally, data obtained from neuroimaging studies have shown activation patterns in the sleeping brain that mirror those recorded during the learning of tasks from the previous day,[95] suggesting that new memories may be solidified through such rehearsal.[96]


Although people often think that memory operates like recording equipment, this is not the case. The molecular mechanisms underlying the induction and maintenance of memory are very dynamic and comprise distinct phases covering a time window from seconds to even a lifetime.[97] In fact, research has revealed that our memories are constructed: "current hypotheses suggest that constructive processes allow individuals to simulate and imagine future episodes,[98] happenings, and scenarios. Since the future is not an exact repetition of the past, simulation of future episodes requires a complex system that can draw on the past in a manner that flexibly extracts and recombines elements of previous experiences – a constructive rather than a reproductive system."[62] People can construct their memories when they encode them and/or when they recall them. To illustrate, consider a classic study conducted by Elizabeth Loftus and John Palmer (1974)[99] in which people were instructed to watch a film of a traffic accident and then asked about what they saw. The researchers found that the people who were asked, "How fast were the cars going when they smashed into each other?" gave higher estimates than those who were asked, "How fast were the cars going when they hit each other?" Furthermore, when asked a week later whether they had seen broken glass in the film, those who had been asked the question with smashed were twice more likely to report that they had seen broken glass than those who had been asked the question with hit. There was no broken glass depicted in the film. Thus, the wording of the questions distorted viewers' memories of the event. Importantly, the wording of the question led people to construct different memories of the event – those who were asked the question with smashed recalled a more serious car accident than they had actually seen. The findings of this experiment were replicated around the world, and researchers consistently demonstrated that when people were provided with misleading information they tended to misremember, a phenomenon known as the misinformation effect.[100]

Research has revealed that asking individuals to repeatedly imagine actions that they have never performed or events that they have never experienced could result in false memories. For instance, Goff and Roediger[101] (1998) asked participants to imagine that they performed an act (e.g., break a toothpick) and then later asked them whether they had done such a thing. Findings revealed that those participants who repeatedly imagined performing such an act were more likely to think that they had actually performed that act during the first session of the experiment. Similarly, Garry and her colleagues (1996)[102] asked college students to report how certain they were that they experienced a number of events as children (e.g., broke a window with their hand) and then two weeks later asked them to imagine four of those events. The researchers found that one-fourth of the students asked to imagine the four events reported that they had actually experienced such events as children. That is, when asked to imagine the events they were more confident that they experienced the events.

Research reported in 2013 revealed that it is possible to artificially stimulate prior memories and artificially implant false memories in mice. Using optogenetics, a team of RIKEN-MIT scientists caused the mice to incorrectly associate a benign environment with a prior unpleasant experience from different surroundings. Some scientists believe that the study may have implications in studying false memory formation in humans, and in treating PTSD and schizophrenia.[103][104]

Memory reconsolidation is when previously consolidated memories are recalled or retrieved from long-term memory to your active consciousness. During this process, memories can be further strengthened and added to but there is also risk of manipulation involved. We like to think of our memories as something stable and constant when they are stored in long-term memory but this isn't the case. There are a large number of studies that found that consolidation of memories is not a singular event but are put through the process again, known as reconsolidation.[105] This is when a memory is recalled or retrieved and placed back into your working memory. The memory is now open to manipulation from outside sources and the misinformation effect which could be due to misattributing the source of the inconsistent information, with or without an intact original memory trace (Lindsay and Johnson, 1989).[106] One thing that can be sure is that memory is malleable.

This new research into the concept of reconsolidation has opened the door to methods to help those with unpleasant memories or those that struggle with memories. An example of this is if you had a truly frightening experience and recall that memory in a less arousing environment, the memory will be weaken the next time it is retrieved.[105] "Some studies suggest that over-trained or strongly reinforced memories do not undergo reconsolidation if reactivated the first few days after training, but do become sensitive to reconsolidation interference with time."[105] This, however does not mean that all memory is susceptible to reconsolidation. There is evidence to suggest that memory that has undergone strong training and whether or not is it intentional is less likely to undergo reconsolidation.[107] There was further testing done with rats and mazes that showed that reactivated memories were more susceptible to manipulation, in both good and bad ways, than newly formed memories.[108] It is still not known whether or not these are new memories formed and it's an inability to retrieve the proper one for the situation or if it's a reconsolidated memory. Because the study of reconsolidation is still a newer concept, there is still debate on whether it should be considered scientifically sound.


A UCLA research study published in the June 2008 issue of the American Journal of Geriatric Psychiatry found that people can improve cognitive function and brain efficiency through simple lifestyle changes such as incorporating memory exercises, healthy eating, physical fitness and stress reduction into their daily lives. This study examined 17 subjects, (average age 53) with normal memory performance. Eight subjects were asked to follow a "brain healthy" diet, relaxation, physical, and mental exercise (brain teasers and verbal memory training techniques). After 14 days, they showed greater word fluency (not memory) compared to their baseline performance. No long-term follow-up was conducted; it is therefore unclear if this intervention has lasting effects on memory.[109]

There are a loosely associated group of mnemonic principles and techniques that can be used to vastly improve memory known as the art of memory.

The International Longevity Center released in 2001 a report[110] which includes in pages 14–16 recommendations for keeping the mind in good functionality until advanced age. Some of the recommendations are to stay intellectually active through learning, training or reading, to keep physically active so to promote blood circulation to the brain, to socialize, to reduce stress, to keep sleep time regular, to avoid depression or emotional instability and to observe good nutrition.

Memorization is a method of learning that allows an individual to recall information verbatim. Rote learning is the method most often used. Methods of memorizing things have been the subject of much discussion over the years with some writers, such as Cosmos Rossellius using visual alphabets. The spacing effect shows that an individual is more likely to remember a list of items when rehearsal is spaced over an extended period of time. In contrast to this is cramming: an intensive memorization in a short period of time. the spacing effect is exploited to improve memory in spaced repetition flashcard training. Also relevant is the Zeigarnik effect which states that people remember uncompleted or interrupted tasks better than completed ones. The so-called Method of loci uses spatial memory to memorize non-spatial information.[111]


Plants lack a specialized organ devoted to memory retention, and so plant memory has been a controversial topic in recent years. New advances in the field have identified the presence of neurotransmitters in plants, adding to the hypothesis that plants are capable of remembering.[112] Action potentials, a physiological response characteristic of neurons, have been shown to have an influence on plants as well, including in wound responses and photosynthesis.[112] In addition to these homologous features of memory systems in both plants and animals, plants have also been observed to encode, store and retrieve basic short-term memories.

One of the most well-studied plants to show rudimentary memory is the Venus flytrap. Native to the subtropical wetlands of the eastern United States, Venus Fly Traps have evolved the ability to obtain meat for sustenance, likely due to the lack of nitrogen in the soil.[113] This is done by two trap-forming leaf tips that snap shut once triggered by a potential prey. On each lobe, three triggers hairs await stimulation. In order to maximize the benefit to cost ratio, the plant enables a rudimentary form of memory in which two trigger hairs must be stimulated within 30 seconds in order to result in trap closure.[113] This system ensures that the trap only closes when potential prey is within grasp.

The time lapse between trigger hair stimulations suggests that the plant can remember an initial stimulus long enough for a second stimulus to initiate trap closure. This memory isn't encoded in a brain, as plants lack this specialized organ. Rather, information is stored in the form of cytoplasmic calcium levels. The first trigger causes a subthreshold cytoplasmic calcium influx.[113] This initial trigger isn't enough to activate trap closure, and so a subsequent stimulus allows for a secondary influx of calcium. The latter calcium rise superimposes on the initial one, creating an action potential that passes threshold, resulting in trap closure.[113] Researchers, to prove that an electrical threshold must be met to stimulate trap closure, excited a single trigger hair with a constant mechanical stimulus using Ag/AgCl electrodes.[114] The trap closed after only a few seconds. This experiment gave evidence to demonstrate that the electrical threshold, not necessarily the number of trigger hair stimulations, was the contributing factor in Venus Fly Trap memory. It has been shown that trap closure can be blocked using uncouplers and inhibitors of voltage-gated channels.[114] After trap closure, these electrical signals stimulate glandular production of jasmonic acid and hydrolases, allowing for digestion of the prey.[115]

The field of plant neurobiology has gained a large amount of interest over the past decade, leading to an influx of research regarding plant memory. Although the Venus flytrap is one of the more highly studied, many other plants exhibit the capacity to remember, including the Mimosa pudica through an experiment conducted by Monica Gagliano and colleagues in 2013.[116] To study the Mimosa pudica, Gagliano designed an appartus with which potted mimosa plants could be repeatedly dropped the same distance and at the same speed. It was observed that the plants defensive response of curling up its leaves decreased over the 60 times the experiment was repeated per plant. To confirm that this was a mechanism of memory rather than exhaustion, some of the plants were shaken post experiment and displayed normal defensive responses of leaf curling. This experiment also demonstrated long term memory in the plants, as it was repeated a month later and the plants were observed to remain unfazed by the dropping. As the field expands, it is likely that we will learn more about the capacity of a plant to remember.


Metaphysics is the branch of philosophy that examines the fundamental nature of reality, including the relationship between mind and matter, between substance and attribute, and between potentiality and actuality.[1] The word "metaphysics" comes from two Greek words that, together, literally mean "after or behind or among [the study of] the natural". It has been suggested that the term might have been coined by a first century AD editor who assembled various small selections of Aristotle’s works into the treatise we now know by the name Metaphysics (ta meta ta phusika, 'after the Physics ', another of Aristotle's works).[2]

Metaphysics studies questions related to what it is for something to exist and what types of existence there are. Metaphysics seeks to answer, in an abstract and fully general manner, the questions:[3]

What is there?
What is it like?
Topics of metaphysical investigation include existence, objects and their properties, space and time, cause and effect, and possibility.[4]

Metaphysical study is conducted using deduction from that which is known a priori. Like foundational mathematics (which is sometimes considered a special case of metaphysics applied to the existence of number), it tries to give a coherent account of the structure of the world, capable of explaining our everyday and scientific perception of the world, and being free from contradictions. In mathematics, there are many different ways to define numbers; similarly in metaphysics there are many different ways to define objects, properties, concepts, and other entities which are claimed to make up the world. While metaphysics may, as a special case, study the entities postulated by fundamental science such as atoms and superstrings, its core topic is the set of categories such as object, property and causality which those scientific theories assume. For example: claiming that "electrons have charge" is a scientific theory; while exploring what it means for electrons to be (or at least, to be perceived as) "objects", charge to be a "property", and for both to exist in a topological entity called "space" is the task of metaphysics.[5]

There are two broad stances about what is "the world" studied by metaphysics. The strong, classical view assumes that the objects studied by metaphysics exist independently of any observer, so that the subject is the most fundamental of all sciences. The weak, modern view assumes that the objects studied by metaphysics exist inside the mind of an observer, so the subject becomes a form of introspection and conceptual analysis. Some philosophers, notably Kant, discuss both of these "worlds" and what can be inferred about each one. Some, such as the logical positivists, and many scientists, reject the strong view of metaphysics as meaningless and unverifiable. Others reply that this criticism also applies to any type of knowledge, including hard science, which claims to describe anything other than the contents of human perception, and thus that the world of perception is the objective world in some sense. Metaphysics itself usually assumes that some stance has been taken on these questions and that it may proceed independently of the choice—the question of which stance to take belongs instead to another branch of philosophy, epistemology.

Ontology is the philosophical study of the nature of being, becoming, existence or reality, as well as the basic categories of being and their relations.[6][failed verification] Traditionally listed[by whom?] as the core of metaphysics, ontology often deals with questions concerning what entities exist and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences.

See also: Identity (philosophy) and Philosophy of space and time
Identity is a fundamental metaphysical concern. Metaphysicians investigating identity are tasked with the question of what, exactly, it means for something to be identical to itself, or – more controversially – to something else. Issues of identity arise in the context of time: what does it mean for something to be itself across two moments in time? How do we account for this? Another question of identity arises when we ask what our criteria ought to be for determining identity, and how the reality of identity interfaces with linguistic expressions.

The metaphysical positions one takes on identity have far-reaching implications on issues such as the Mind–body problem, personal identity, ethics, and law.

A few ancient Greeks took extreme positions on the nature of change. Parmenides denied change altogether, while Heraclitus argued that change was ubiquitous: "No man ever steps in the same river twice."

Identity, sometimes called numerical identity, is the relation that a thing bears to itself, and which no thing bears to anything other than itself (cf. sameness).

A modern philosopher who made a lasting impact on the philosophy of identity was Leibniz, whose Law of the Indiscernibility of Identicals is still widely accepted today. It states that if some object x is identical to some object y, then any property that x has, y will have as well.

Put formally, it states

{\displaystyle \forall x\;\forall y\;(x=y\rightarrow \forall P\;(P(x)\leftrightarrow P(y)))}{\displaystyle \forall x\;\forall y\;(x=y\rightarrow \forall P\;(P(x)\leftrightarrow P(y)))}
However, it does seem that objects can change over time. If one were to look at a tree one day, and the tree later lost a leaf, it would seem that one could still be looking at that same tree. Two rival theories to account for the relationship between change and identity are perdurantism, which treats the tree as a series of tree-stages, and endurantism, which maintains that the organism—the same tree—is present at every stage in its history.

By appealing to intrinsic and extrinsic properties, endurantism finds a way to harmonize identity with change. Endurantists believe that objects persist by being strictly numerically identical over time.[7] However, if Leibniz's Law of the Indiscernibility of Identicals is utilized to define numerical identity here, it seems that objects must be completely unchanged in order to persist. Discriminating between intrinsic properties and extrinsic properties, endurantists state that numerical identity means that, if some object x is identical to some object y, then any intrinsic property that x has, y will have as well. Thus, if an object persists, intrinsic properties of it are unchanged, but extrinsic properties can change over time. Besides the object itself, environments and other objects can change over time; properties that relate to other objects would change even if this object does not change.

Perdurantism can harmonize identity with change in another way. In four-dimensionalism, a version of perdurantism, what persists is a four-dimensional object which does not change although three-dimensional slices of the object may differ.


See also: Philosophy of space and time
Objects appear to us in space and time, while abstract entities such as classes, properties, and relations do not. How do space and time serve this function as a ground for objects? Are space and time entities themselves, of some form? Must they exist prior to objects? How exactly can they be defined? How is time related to change; must there always be something changing in order for time to exist?


Classical philosophy recognized a number of causes, including teleological future causes. In special relativity and quantum field theory the notions of space, time and causality become tangled together, with temporal orders of causations becoming dependent on who is observing them.[citation needed] The laws of physics are symmetrical in time, so could equally well be used to describe time as running backwards. Why then do we perceive it as flowing in one direction, the arrow of time, and as containing causation flowing in the same direction?

For that matter, can an effect precede its cause? This was the title of a 1954 paper by Michael Dummett,[8] which sparked a discussion that continues today.[9] Earlier, in 1947, C. S. Lewis had argued that one can meaningfully pray concerning the outcome of, e.g., a medical test while recognizing that the outcome is determined by past events: "My free act contributes to the cosmic shape."[10] Likewise, some interpretations of quantum mechanics, dating to 1945, involve backward-in-time causal influences.[11]

Causality is linked by many philosophers to the concept of counterfactuals. To say that A caused B means that if A had not happened then B would not have happened. This view was advanced by David Lewis in his 1973 paper "Causation".[12] His subsequent papers[13] further develop his theory of causation.

Causality is usually required as a foundation for philosophy of science, if science aims to understand causes and effects and make predictions about them.


Metaphysicians investigate questions about the ways the world could have been. David Lewis, in On the Plurality of Worlds, endorsed a view called Concrete Modal realism, according to which facts about how things could have been are made true by other concrete worlds in which things are different. Other philosophers, including Gottfried Leibniz, have dealt with the idea of possible worlds as well. A necessary fact is true across all possible worlds. A possible fact is true in some possible world, even if not in the actual world. For example, it is possible that cats could have had two tails, or that any particular apple could have not existed. By contrast, certain propositions seem necessarily true, such as analytic propositions, e.g., "All bachelors are unmarried." The view that any analytic truth is necessary is not universally held among philosophers. A less controversial view is that self-identity is necessary, as it seems fundamentally incoherent to claim that any x is not identical to itself; this is known as the law of identity, a putative "first principle". Similarly, Aristotle describes the principle of non-contradiction:

It is impossible that the same quality should both belong and not belong to the same thing ... This is the most certain of all principles ... Wherefore they who demonstrate refer to this as an ultimate opinion. For it is by nature the source of all the other axioms.
Peripheral questions
What is "central" and "peripheral" to metaphysics has varied over time and schools; however contemporary analytic philosophy as taught in USA and UK universities generally regards the above as "central" and the following as "applications" or "peripheral" topics; or in some cases as distinct subjects which have grown out of and depend upon metaphysics:[citation needed]


Metaphysical cosmology is the branch of metaphysics that deals with the world as the totality of all phenomena in space and time. Historically, it formed a major part of the subject alongside Ontology, though its role is more peripheral in contemporary philosophy. It has had a broad scope, and in many cases was founded in religion. The ancient Greeks drew no distinction between this use and their model for the cosmos. However, in modern times it addresses questions about the Universe which are beyond the scope of the physical sciences. It is distinguished from religious cosmology in that it approaches these questions using philosophical methods (e.g. dialectics).

Cosmogony deals specifically with the origin of the universe. Modern metaphysical cosmology and cosmogony try to address questions such as:

What is the origin of the Universe? What is its first cause? Is its existence necessary? (see monism, pantheism, emanationism and creationism)
What are the ultimate material components of the Universe? (see mechanism, dynamism, hylomorphism, atomism)
What is the ultimate reason for the existence of the Universe? Does the cosmos have a purpose? (see teleology)


Different approaches toward resolving the mind–body problem
Accounting for the existence of mind in a world largely composed of matter is a metaphysical problem which is so large and important as to have become a specialized subject of study in its own right, philosophy of mind.

Substance dualism is a classical theory in which mind and body are essentially different, with the mind having some of the attributes traditionally assigned to the soul, and which creates an immediate conceptual puzzle about how the two interact. This form of substance dualism differs from the dualism of some eastern philosophical traditions (like Nyāya), which also posit a soul; for the soul, under their view, is ontologically distinct from the mind.[14] Idealism postulates that material objects do not exist unless perceived and only as perceptions. Adherents of panpsychism, a kind of property dualism, hold that everything has a mental aspect, but not that everything exists in a mind. Neutral monism postulates that existence consists of a single substance that in itself is neither mental nor physical, but is capable of mental and physical aspects or attributes – thus it implies a dual-aspect theory. For the last century, the dominant theories have been science-inspired including materialistic monism, type identity theory, token identity theory, functionalism, reductive physicalism, nonreductive physicalism, eliminative materialism, anomalous monism, property dualism, epiphenomenalism and emergence.


Determinism is the philosophical proposition that every event, including human cognition, decision and action, is causally determined by an unbroken chain of prior occurrences. It holds that nothing happens that has not already been determined. The principal consequence of the deterministic claim is that it poses a challenge to the existence of free will.

The problem of free will is the problem of whether rational agents exercise control over their own actions and decisions. Addressing this problem requires understanding the relation between freedom and causation, and determining whether the laws of nature are causally deterministic. Some philosophers, known as incompatibilists, view determinism and free will as mutually exclusive. If they believe in determinism, they will therefore believe free will to be an illusion, a position known as Hard Determinism. Proponents range from Baruch Spinoza to Ted Honderich. Henri Bergson defended free will in his dissertation Time and Free Will from 1889.

Others, labeled compatibilists (or "soft determinists"), believe that the two ideas can be reconciled coherently. Adherents of this view include Thomas Hobbes and many modern philosophers such as John Martin Fischer, Gary Watson, Harry Frankfurt, and the like.

Incompatibilists who accept free will but reject determinism are called libertarians, a term not to be confused with the political sense. Robert Kane and Alvin Plantinga are modern defenders of this theory.


The earliest type of classification of social construction traces back to Plato in his dialogue Phaedrus where he claims that the biological classification system seems to carve nature at the joints.[15] In contrast, later philosophers such as Michel Foucault and Jorge Luis Borges have challenged the capacity of natural and social classification. In his essay The Analytical Language of John Wilkins, Borges makes us imagine a certain encyclopedia where the animals are divided into (a) those that belong to the emperor; (b) embalmed ones; (c) those that are trained;... and so forth, in order to bring forward the ambiguity of natural and social kinds.[16] According to metaphysics author Alyssa Ney: "the reason all this is interesting is that there seems to be a metaphysical difference between the Borgesian system and Plato's".[17] The difference is not obvious but one classification attempts to carve entities up according to objective distinction while the other does not. According to Quine this notion is closely related to the notion of similarity.[18]


There are different ways to set up the notion of number in metaphysics theories. Platonist theories postulate number as a fundamental category itself. Others consider it to be a property of an entity called a "group" comprising other entities; or to be a relation held between several groups of entities, such as "the number four is the set of all sets of four things". Many of the debates around universals are applied to the study of number, and are of particular importance due to its status as a foundation for the philosophy of mathematics and for mathematics itself.


Although metaphysics as a philosophical enterprise is highly hypothetical, it also has practical application in most other branches of philosophy, science, and now also information technology. Such areas generally assume some basic ontology (such as a system of objects, properties, classes, and space time) as well as other metaphysical stances on topics such as causality and agency, then build their own particular theories upon these.

In science, for example, some theories are based on the ontological assumption of objects with properties (such as electrons having charge) while others may reject objects completely (such as quantum field theories, where spread-out "electronness" becomes a property of space time rather than an object).

"Social" branches of philosophy such as philosophy of morality, aesthetics and philosophy of religion (which in turn give rise to practical subjects such as ethics, politics, law, and art) all require metaphysical foundations, which may be considered as branches or applications of metaphysics. For example, they may postulate the existence of basic entities such as value, beauty, and God. Then they use these postulates to make their own arguments about consequences resulting from them. When philosophers in these subjects make their foundations they are doing applied metaphysics, and may draw upon its core topics and methods to guide them, including ontology and other core and peripheral topics. As in science, the foundations chosen will in turn depend on the underlying ontology used, so philosophers in these subjects may have to dig right down to the ontological layer of metaphysics to find what is possible for their theories. For example, a contradiction obtained in a theory of God or Beauty might be due to an assumption that it is an object rather than some other kind of ontological entity.


Prior to the modern history of science, scientific questions were addressed as a part of natural philosophy. Originally, the term "science" (Latin: scientia) simply meant "knowledge". The scientific method, however, transformed natural philosophy into an empirical activity deriving from experiment, unlike the rest of philosophy. By the end of the 18th century, it had begun to be called "science" to distinguish it from other branches of philosophy. Science and philosophy have been considered separated disciplines ever since. Thereafter, metaphysics denoted philosophical enquiry of a non-empirical character into the nature of existence.[19]

Metaphysics continues asking "why" where science leaves off. For example, any theory of fundamental physics is based on some set of axioms, which may postulate the existence of entities such as atoms, particles, forces, charges, mass, or fields. Stating such postulates is considered to be the "end" of a science theory. Metaphysics takes these postulates and explores what they mean as human concepts. For example, do all theories of physics require the existence of space and time,[20] objects, and properties? Or can they be expressed using only objects, or only properties? Do the objects have to retain their identity over time or can they change?[21] If they change, then are they still the same object? Can theories be reformulated by converting properties or predicates (such as "red") into entities (such as redness or redness fields) or processes ('there is some redding happening over there' appears in some human languages in place of the use of properties). Is the distinction between objects and properties fundamental to the physical world or to our perception of it?

Much recent work has been devoted to analyzing the role of metaphysics in scientific theorizing. Alexandre Koyré led this movement, declaring in his book Metaphysics and Measurement, "It is not by following experiment, but by outstripping experiment, that the scientific mind makes progress."[22] That metaphysical propositions can influence scientific theorizing is John Watkins' most lasting contribution to philosophy. Since 1957[23][24] "he showed the ways in which some un-testable and hence, according to Popperian ideas, non-empirical propositions can nevertheless be influential in the development of properly testable and hence scientific theories. These profound results in applied elementary logic...represented an important corrective to positivist teachings about the meaninglessness of metaphysics and of normative claims".[25] Imre Lakatos maintained that all scientific theories have a metaphysical "hard core" essential for the generation of hypotheses and theoretical assumptions.[26] Thus, according to Lakatos, "scientific changes are connected with vast cataclysmic metaphysical revolutions."[27]

An example from biology of Lakatos' thesis: David Hull has argued that changes in the ontological status of the species concept have been central in the development of biological thought from Aristotle through Cuvier, Lamarck, and Darwin. Darwin's ignorance of metaphysics made it more difficult for him to respond to his critics because he could not readily grasp the ways in which their underlying metaphysical views differed from his own.[28]

In physics, new metaphysical ideas have arisen in connection with quantum mechanics, where subatomic particles arguably do not have the same sort of individuality as the particulars with which philosophy has traditionally been concerned.[29] Also, adherence to a deterministic metaphysics in the face of the challenge posed by the quantum-mechanical uncertainty principle led physicists such as Albert Einstein to propose alternative theories that retained determinism.[30] A.N. Whitehead is famous for creating a process philosophy metaphysics inspired by electromagnetism and special relativity.[31]

In chemistry, Gilbert Newton Lewis addressed the nature of motion, arguing that an electron should not be said to move when it has none of the properties of motion.[32]

Katherine Hawley notes that the metaphysics even of a widely accepted scientific theory may be challenged if it can be argued that the metaphysical presuppositions of the theory make no contribution to its predictive success.[33]


Metametaphysics is the branch of philosophy that is concerned with the foundations of metaphysics.[34] A number of individuals have suggested that much or all of metaphysics should be rejected, a metametaphysical position known as metaphysical deflationism[a][35] or ontological deflationism.[36]

In the 16th century, Francis Bacon rejected scholastic metaphysics, and argued strongly for what is now called empiricism, being seen later as the father of modern empirical science. In the 18th century, David Hume took a strong position, arguing that all genuine knowledge involves either mathematics or matters of fact and that metaphysics, which goes beyond these, is worthless. He concludes his Enquiry Concerning Human Understanding with the statement:

If we take in our hand any volume; of divinity or school metaphysics, for instance; let us ask, Does it contain any abstract reasoning concerning quantity or number? No. Does it contain any experimental reasoning concerning matter of fact and existence? No. Commit it then to the flames: for it can contain nothing but sophistry and illusion.[37]

Thirty-three years after Hume's Enquiry appeared, Immanuel Kant published his Critique of Pure Reason. Although he followed Hume in rejecting much of previous metaphysics, he argued that there was still room for some synthetic a priori knowledge, concerned with matters of fact yet obtainable independent of experience.[38] These included fundamental structures of space, time, and causality. He also argued for the freedom of the will and the existence of "things in themselves", the ultimate (but unknowable) objects of experience.

Wittgenstein introduced the concept that metaphysics could be influenced by theories of aesthetics, via logic, vis. a world composed of "atomical facts".[39][40]

In the 1930s, A.J. Ayer and Rudolf Carnap endorsed Hume's position; Carnap quoted the passage above.[41] They argued that metaphysical statements are neither true nor false but meaningless since, according to their verifiability theory of meaning, a statement is meaningful only if there can be empirical evidence for or against it. Thus, while Ayer rejected the monism of Spinoza, he avoided a commitment to pluralism, the contrary position, by holding both views to be without meaning.[42] Carnap took a similar line with the controversy over the reality of the external world.[43] While the logical positivism movement is now considered dead (with Ayer, a major proponent, admitting in a 1979 TV interview that "nearly all of it was false"),[44] it has continued to influence philosophy development.[45]

Arguing against such rejections, the Scholastic philosopher Edward Feser held that Hume's critique of metaphysics, and specifically Hume's fork, is "notoriously self-refuting".[46] Feser argues that Hume's fork itself is not a conceptual truth and is not empirically testable.

Some living philosophers, such as Amie Thomasson, have argued that many metaphysical questions can be dissolved just by looking at the way we use words; others, such as Ted Sider, have argued that metaphysical questions are substantive, and that we can make progress toward answering them by comparing theories according to a range of theoretical virtues inspired by the sciences, such as simplicity and explanatory power.[47]

Etymology
The word "metaphysics" derives from the Greek words μετά (metá, "after") and φυσικά (physiká, "physics").[48] It was first used as the title for several of Aristotle's works, because they were usually anthologized after the works on physics in complete editions. The prefix meta- ("after") indicates that these works come "after" the chapters on physics. However, Aristotle himself did not call the subject of these books metaphysics: he referred to it as "first philosophy" (Greek: πρώτη φιλοσοφία; Latin: philosophia prima). The editor of Aristotle's works, Andronicus of Rhodes, is thought to have placed the books on first philosophy right after another work, Physics, and called them τὰ μετὰ τὰ φυσικὰ βιβλία (tà metà tà physikà biblía) or "the books [that come] after the [books on] physics".

However, once the name was given, the commentators sought to find other reasons for its appropriateness. For instance, Thomas Aquinas understood it to refer to the chronological or pedagogical order among our philosophical studies, so that the "metaphysical sciences" would mean "those that we study after having mastered the sciences that deal with the physical world".[49]

The term was misread by other medieval commentators, who thought it meant "the science of what is beyond the physical".[50] Following this tradition, the prefix meta- has more recently been prefixed to the names of sciences to designate higher sciences dealing with ulterior and more fundamental problems: hence metamathematics, metaphysiology, etc.[51]

A person who creates or develops metaphysical theories is called a metaphysician.[52]

Common parlance also uses the word "metaphysics" for a different referent from that of the present article, namely for beliefs in arbitrary non-physical or magical entities. For example, "Metaphysical healing" to refer to healing by means of remedies that are magical rather than scientific.[53] This usage stemmed from the various historical schools of speculative metaphysics which operated by postulating all manner of physical, mental and spiritual entities as bases for particular metaphysical systems. Metaphysics as a subject does not preclude beliefs in such magical entities but neither does it promote them. Rather, it is the subject which provides the vocabulary and logic with which such beliefs might be analyzed and studied, for example to search for inconsistencies both within themselves and with other accepted systems such as Science.


Cognitive archeology such as analysis of cave paintings and other pre-historic art and customs suggests that a form of perennial philosophy or Shamanic metaphysics may stretch back to the birth of behavioral modernity, all around the world. Similar beliefs are found in present-day "stone age" cultures such as Australian aboriginals. Perennial philosophy postulates the existence of a spirit or concept world alongside the day-to-day world, and interactions between these worlds during dreaming and ritual, or on special days or at special places. It has been argued that perennial philosophy formed the basis for Platonism, with Plato articulating, rather than creating, much older widespread beliefs.[54][55]


Bronze Age cultures such as ancient Mesopotamia and ancient Egypt (along with similarly structured but chronologically later cultures such as Mayans and Aztecs) developed belief systems based on mythology, anthropomorphic gods, mind–body dualism, and a spirit world, to explain causes and cosmology. These cultures appear to have been interested in astronomy and may have associated or identified the stars with some of these entities. In ancient Egypt, the ontological distinction between order (maat) and chaos (Isfet) seems to have been important.[56]

The circled dot was used by the Pythagoreans and later Greeks to represent the first metaphysical being, the Monad or The Absolute.
The first named Greek philosopher, according to Aristotle, is Thales of Miletus, early 6th century BCE. He made use of purely physical explanations to explain the phenomena of the world rather than the mythological and divine explanations of tradition. He is thought to have posited water as the single underlying principle (or Arche in later Aristotelian terminology) of the material world. His fellow, but younger Miletians, Anaximander and Anaximenes, also posited monistic underlying principles, namely apeiron (the indefinite or boundless) and air respectively.

Another school was the Eleatics, in southern Italy. The group was founded in the early fifth century BCE by Parmenides, and included Zeno of Elea and Melissus of Samos. Methodologically, the Eleatics were broadly rationalist, and took logical standards of clarity and necessity to be the criteria of truth. Parmenides' chief doctrine was that reality is a single unchanging and universal Being. Zeno used reductio ad absurdum, to demonstrate the illusory nature of change and time in his paradoxes.

Heraclitus of Ephesus, in contrast, made change central, teaching that "all things flow". His philosophy, expressed in brief aphorisms, is quite cryptic. For instance, he also taught the unity of opposites.

Democritus and his teacher Leucippus, are known for formulating an atomic theory for the cosmos.[57] They are considered forerunners of the scientific method.


The modern "yin and yang symbol" (taijitu)
Metaphysics in Chinese philosophy can be traced back to the earliest Chinese philosophical concepts from the Zhou Dynasty such as Tian (Heaven) and Yin and Yang. The fourth century BCE saw a turn towards cosmogony with the rise of Taoism (in the Daodejing and Zhuangzi) and sees the natural world as dynamic and constantly changing processes which spontaneously arise from a single immanent metaphysical source or principle (Tao).[58] Another philosophical school which arose around this time was the School of Naturalists which saw the ultimate metaphysical principle as the Taiji, the "supreme polarity" composed of the forces of Yin and Yang which were always in a state of change seeking balance. Another concern of Chinese metaphysics, especially Taoism, is the relationship and nature of Being and non-Being (you 有 and wu 無). The Taoists held that the ultimate, the Tao, was also non-being or no-presence.[58] Other important concepts were those of spontaneous generation or natural vitality (Ziran) and "correlative resonance" (Ganying).

After the fall of the Han Dynasty (220 CE), China saw the rise of the Neo-Taoist Xuanxue school. This school was very influential in developing the concepts of later Chinese metaphysics.[58] Buddhist philosophy entered China (c. 1st century) and was influenced by the native Chinese metaphysical concepts to develop new theories. The native Tiantai and Huayen schools of philosophy maintained and reinterpreted the Indian theories of shunyata (emptiness, kong 空) and Buddha-nature (Fo xing 佛性) into the theory of interpenetration of phenomena. Neo-Confucians like Zhang Zai under the influence of other schools developed the concepts of "principle" (li) and vital energy (qi).

Socrates is known for his dialectic or questioning approach to philosophy rather than a positive metaphysical doctrine.

His pupil, Plato is famous for his theory of forms (which he places in the mouth of Socrates in his dialogues). Platonic realism (also considered a form of idealism)[59] is considered to be a solution to the problem of universals; i.e., what particular objects have in common is that they share a specific Form which is universal to all others of their respective kind.

The theory has a number of other aspects:

Epistemological: knowledge of the Forms is more certain than mere sensory data.
Ethical: The Form of the Good sets an objective standard for morality.
Time and Change: The world of the Forms is eternal and unchanging. Time and change belong only to the lower sensory world. "Time is a moving image of Eternity".
Abstract objects and mathematics: Numbers, geometrical figures, etc., exist mind-independently in the World of Forms.
Platonism developed into Neoplatonism, a philosophy with a monotheistic and mystical flavour that survived well into the early Christian era.

Plato's pupil Aristotle wrote widely on almost every subject, including metaphysics. His solution to the problem of universals contrasts with Plato's. Whereas Platonic Forms are existentially apparent in the visible world, Aristotelian essences dwell in particulars.

Potentiality and Actuality[60] are principles of a dichotomy which Aristotle used throughout his philosophical works to analyze motion, causality and other issues.

The Aristotelian theory of change and causality stretches to four causes: the material, formal, efficient and final. The efficient cause corresponds to what is now known as a cause simplicity. Final causes are explicitly teleological, a concept now regarded as controversial in science.[61] The Matter/Form dichotomy was to become highly influential in later philosophy as the substance/essence distinction.

The opening arguments in Aristotle's Metaphysics, Book I, revolve around the senses, knowledge, experience, theory, and wisdom. The first main focus in the Metaphysics is attempting to determine how intellect "advances from sensation through memory, experience, and art, to theoretical knowledge".[62] Aristotle claims that eyesight provides us with the capability to recognize and remember experiences, while sound allows us to learn.

Sāṃkhya is an ancient system of Indian philosophy based on a dualism involving the ultimate principles of consciousness and matter.[63] It is described as the rationalist school of Indian philosophy.[64] It is most related to the Yoga school of Hinduism, and its method was most influential on the development of Early Buddhism.[65]

The Sāmkhya is an enumerationist philosophy whose epistemology accepts three of six pramanas (proofs) as the only reliable means of gaining knowledge. These include pratyakṣa (perception), anumāṇa (inference) and śabda (āptavacana, word/testimony of reliable sources).[66][67][68]

Samkhya is strongly dualist.[69][70][71] Sāmkhya philosophy regards the universe as consisting of two realities; puruṣa (consciousness) and prakṛti (matter). Jiva (a living being) is that state in which puruṣa is bonded to prakṛti in some form.[72] This fusion, state the Samkhya scholars, led to the emergence of buddhi ("spiritual awareness") and ahaṅkāra (ego consciousness). The universe is described by this school as one created by purusa-prakṛti entities infused with various permutations and combinations of variously enumerated elements, senses, feelings, activity and mind.[72] During the state of imbalance, one of more constituents overwhelm the others, creating a form of bondage, particularly of the mind. The end of this imbalance, bondage is called liberation, or moksha, by the Samkhya school.[73]

The existence of God or supreme being is not directly asserted, nor considered relevant by the Samkhya philosophers. Sāṃkhya denies the final cause of Ishvara (God).[74] While the Samkhya school considers the Vedas as a reliable source of knowledge, it is an atheistic philosophy according to Paul Deussen and other scholars.[75][76] A key difference between Samkhya and Yoga schools, state scholars,[76][77] is that Yoga school accepts a "personal, yet essentially inactive, deity" or "personal god".[78]

Samkhya is known for its theory of guṇas (qualities, innate tendencies).[79] Guṇa, it states, are of three types: sattva being good, compassionate, illuminating, positive, and constructive; rajas is one of activity, chaotic, passion, impulsive, potentially good or bad; and tamas being the quality of darkness, ignorance, destructive, lethargic, negative. Everything, all life forms and human beings, state Samkhya scholars, have these three guṇas, but in different proportions. The interplay of these guṇas defines the character of someone or something, of nature and determines the progress of life.[80][81] The Samkhya theory of guṇas was widely discussed, developed and refined by various schools of Indian philosophies, including Buddhism.[82] Samkhya's philosophical treatises also influenced the development of various theories of Hindu ethics.[65]

Realization of the nature of Self-identity is the principal object of the Vedanta system of Indian metaphysics. In the Upanishads, self-consciousness is not the first-person indexical self-awareness or the self-awareness which is self-reference without identification,[83] and also not the self-consciousness which as a kind of desire is satisfied by another self-consciousness.[84] It is Self-realisation; the realisation of the Self consisting of consciousness that leads all else.[85]

The word Self-consciousness in the Upanishads means the knowledge about the existence and nature of Brahman. It means the consciousness of our own real being, the primary reality.[86] Self-consciousness means Self-knowledge, the knowledge of Prajna i.e. of Prana which is Brahman.[87] According to the Upanishads the Atman or Paramatman is phenomenally unknowable; it is the object of realisation. The Atman is unknowable in its essential nature; it is unknowable in its essential nature because it is the eternal subject who knows about everything including itself. The Atman is the knower and also the known.[88]

Metaphysicians regard the Self either to be distinct from the Absolute or entirely identical with the Absolute. They have given form to three schools of thought – a) the Dualistic school, b) the Quasi-dualistic school and c) the Monistic school, as the result of their varying mystical experiences. Prakrti and Atman, when treated as two separate and distinct aspects form the basis of the Dualism of the Shvetashvatara Upanishad.[89] Quasi-dualism is reflected in the Vaishnavite-monotheism of Ramanuja and the absolute Monism, in the teachings of Adi Shankara.[90]

Self-consciousness is the Fourth state of consciousness or Turiya, the first three being Vaisvanara, Taijasa and Prajna. These are the four states of individual consciousness.

There are three distinct stages leading to Self-realisation. The First stage is in mystically apprehending the glory of the Self within us as though we were distinct from it. The Second stage is in identifying the "I-within" with the Self, that we are in essential nature entirely identical with the pure Self. The Third stage is in realising that the Atman is Brahman, that there is no difference between the Self and the Absolute. The Fourth stage is in realising "I am the Absolute" – Aham Brahman Asmi. The Fifth stage is in realising that Brahman is the "All" that exists, as also that which does not exist.[91]

In Buddhist philosophy there are various metaphysical traditions that have proposed different questions about the nature of reality based on the teachings of the Buddha in the early Buddhist texts. The Buddha of the early texts does not focus on metaphysical questions but on ethical and spiritual training and in some cases, he dismisses certain metaphysical questions as unhelpful and indeterminate Avyakta, which he recommends should be set aside. The development of systematic metaphysics arose after the Buddha's death with the rise of the Abhidharma traditions.[92] The Buddhist Abhidharma schools developed their analysis of reality based on the concept of dharmas which are the ultimate physical and mental events that make up experience and their relations to each other. Noa Ronkin has called their approach "phenomenological".[93]

Later philosophical traditions include the Madhyamika school of Nagarjuna, which further developed the theory of the emptiness (shunyata) of all phenomena or dharmas which rejects any kind of substance. This has been interpreted as a form of anti-foundationalism and anti-realism which sees reality as having no ultimate essence or ground.[94] The Yogacara school meanwhile promoted a theory called "awareness only" (vijnapti-matra) which has been interpreted as a form of Idealism or Phenomenology and denies the split between awareness itself and the objects of awareness.[95]

Major ideas in Sufi metaphysics have surrounded the concept of weḥdah (وحدة) meaning "unity", or in Arabic توحيد tawhid. waḥdat al-wujūd literally means the "Unity of Existence" or "Unity of Being." The phrase has been translated "pantheism."[96] Wujud (i.e. existence or presence) here refers to Allah's wujud (compare tawhid). On the other hand, waḥdat ash-shuhūd, meaning "Apparentism" or "Monotheism of Witness", holds that God and his creation are entirely separate.

Between about 1100 and 1500, philosophy as a discipline took place as part of the Catholic church's teaching system, known as scholasticism. Scholastic philosophy took place within an established framework blending Christian theology with Aristotelian teachings. Although fundamental orthodoxies were not commonly challenged, there were nonetheless deep metaphysical disagreements, particularly over the problem of universals, which engaged Duns Scotus and Pierre Abelard. William of Ockham is remembered for his principle of ontological parsimony.

In the early modern period (17th and 18th centuries), the system-building scope of philosophy is often linked to the rationalist method of philosophy, that is the technique of deducing the nature of the world by pure reason. The scholastic concepts of substance and accident were employed.

Leibniz proposed in his Monadology a plurality of non-interacting substances.
Descartes is famous for his dualism of material and mental substances.
Spinoza believed reality was a single substance of God-or-nature.
Wolff
Christian Wolff had theoretical philosophy divided into an ontology or philosophia prima as a general metaphysics,[97] which arises as a preliminary to the distinction of the three "special metaphysics"[98] on the soul, world and God:[99][100] rational psychology,[101][102] rational cosmology[103] and rational theology.[104] The three disciplines are called empirical and rational because they are independent of revelation. This scheme, which is the counterpart of religious tripartition in creature, creation, and Creator, is best known to philosophical students by Kant's treatment of it in the Critique of Pure Reason. In the "Preface" of the 2nd edition of Kant's book, Wolff is defined "the greatest of all dogmatic philosophers."[105]

British empiricism marked something of a reaction to rationalist and system-building metaphysics, or speculative metaphysics as it was pejoratively termed. The skeptic David Hume famously declared that most metaphysics should be consigned to the flames (see below). Hume was notorious among his contemporaries as one of the first philosophers to openly doubt religion, but is better known now for his critique of causality. John Stuart Mill, Thomas Reid and John Locke were less skeptical, embracing a more cautious style of metaphysics based on realism, common sense and science. Other philosophers, notably George Berkeley were led from empiricism to idealistic metaphysics.

Immanuel Kant attempted a grand synthesis and revision of the trends already mentioned: scholastic philosophy, systematic metaphysics, and skeptical empiricism, not to forget the burgeoning science of his day. As did the systems builders, he had an overarching framework in which all questions were to be addressed. Like Hume, who famously woke him from his 'dogmatic slumbers', he was suspicious of metaphysical speculation, and also places much emphasis on the limitations of the human mind. Kant described his shift in metaphysics away from making claims about an objective noumenal world, towards exploring the subjective phenomenal world, as a Copernican Revolution, by analogy to (though opposite in direction to) Copernicus' shift from man (the subject) to the sun (an object) at the center of the universe.

Kant saw rationalist philosophers as aiming for a kind of metaphysical knowledge he defined as the synthetic apriori—that is knowledge that does not come from the senses (it is a priori) but is nonetheless about reality (synthetic). Inasmuch as it is about reality, it differs from abstract mathematical propositions (which he terms analytical apriori), and being apriori it is distinct from empirical, scientific knowledge (which he terms synthetic aposteriori). The only synthetic apriori knowledge we can have is of how our minds organise the data of the senses; that organising framework is space and time, which for Kant have no mind-independent existence, but nonetheless operate uniformly in all humans. Apriori knowledge of space and time is all that remains of metaphysics as traditionally conceived. There is a reality beyond sensory data or phenomena, which he calls the realm of noumena; however, we cannot know it as it is in itself, but only as it appears to us. He allows himself to speculate that the origins of phenomenal God, morality, and free will might exist in the noumenal realm, but these possibilities have to be set against its basic unknowability for humans. Although he saw himself as having disposed of metaphysics, in a sense, he has generally been regarded in retrospect as having a metaphysics of his own, and as beginning the modern analytical conception of the subject.

Nineteenth century philosophy was overwhelmingly influenced by Kant and his successors. Schopenhauer, Schelling, Fichte and Hegel all purveyed their own panoramic versions of German Idealism, Kant's own caution about metaphysical speculation, and refutation of idealism, having fallen by the wayside. The idealistic impulse continued into the early twentieth century with British idealists such as F.H. Bradley and J.M.E. McTaggart. Followers of Karl Marx took Hegel's dialectic view of history and re-fashioned it as materialism.

Early analytic philosophy and positivism
During the period when idealism was dominant in philosophy, science had been making great advances. The arrival of a new generation of scientifically minded philosophers led to a sharp decline in the popularity of idealism during the 1920s.

Analytic philosophy was spearheaded by Bertrand Russell and G.E. Moore. Russell and William James tried to compromise between idealism and materialism with the theory of neutral monism.

The early to mid twentieth century philosophy saw a trend to reject metaphysical questions as meaningless. The driving force behind this tendency was the philosophy of logical positivism as espoused by the Vienna Circle, which argued that the meaning of a statement was its prediction of observable results of an experiment, and thus that there is no need to postulate the existence of any objects other than these perceptual observations.

At around the same time, the American pragmatists were steering a middle course between materialism and idealism. System-building metaphysics, with a fresh inspiration from science, was revived by A.N. Whitehead and Charles Hartshorne.

The forces that shaped analytic philosophy—the break with idealism, and the influence of science—were much less significant outside the English speaking world, although there was a shared turn toward language. Continental philosophy continued in a trajectory from post Kantianism.

The phenomenology of Husserl and others was intended as a collaborative project for the investigation of the features and structure of consciousness common to all humans, in line with Kant's basing his synthetic apriori on the uniform operation of consciousness. It was officially neutral with regards to ontology, but was nonetheless to spawn a number of metaphysical systems. Brentano's concept of intentionality would become widely influential, including on analytic philosophy.

Heidegger, author of Being and Time, saw himself as re-focusing on Being-qua-being, introducing the novel concept of Dasein in the process. Classing himself an existentialist, Sartre wrote an extensive study of Being and Nothingness.

The speculative realism movement marks a return to full blooded realism.

There are two fundamental aspects of everyday experience: change and persistence. Until recently, the Western philosophical tradition has arguably championed substance and persistence, with some notable exceptions, however. According to process thinkers, novelty, flux and accident do matter, and sometimes they constitute the ultimate reality.

In a broad sense, process metaphysics is as old as Western philosophy, with figures such as Heraclitus, Plotinus, Duns Scotus, Leibniz, David Hume, Georg Wilhelm Friedrich Hegel, Friedrich Wilhelm Joseph von Schelling, Gustav Theodor Fechner, Friedrich Adolf Trendelenburg, Charles Renouvier, Karl Marx, Ernst Mach, Friedrich Wilhelm Nietzsche, Émile Boutroux, Henri Bergson, Samuel Alexander and Nicolas Berdyaev. It seemingly remains an open question whether major "Continental" figures such as the late Martin Heidegger, Maurice Merleau-Ponty, Gilles Deleuze, Michel Foucault, or Jacques Derrida should be included.[106]

In a strict sense, process metaphysics may be limited to the works of a few founding fathers: G.W.F. Hegel, Charles Sanders Peirce, William James, Henri Bergson, A.N. Whitehead, and John Dewey. From a European perspective, there was a very significant and early Whiteheadian influence on the works of outstanding scholars such as Émile Meyerson (1859–1933), Louis Couturat (1868–1914), Jean Wahl (1888–1974), Robin George Collingwood (1889–1943), Philippe Devaux (1902–1979), Hans Jonas (1903–1993), Dorothy M. Emmett (1904–2000), Maurice Merleau Ponty (1908–1961), Enzo Paci (1911–1976), Charlie Dunbar Broad (1887–1971), Wolfe Mays (1912–2005), Ilya Prigogine (1917–2003), Jules Vuillemin (1920–2001), Jean Ladrière (1921–2007), Gilles Deleuze (1925–1995), Wolfhart Pannenberg (1928–2014), and Reiner Wiehl (1929–2010).[107]

While early analytic philosophy tended to reject metaphysical theorizing, under the influence of logical positivism, it was revived in the second half of the twentieth century. Philosophers such as David K. Lewis and David Armstrong developed elaborate theories on a range of topics such as universals, causation, possibility and necessity and abstract objects. However, the focus of analytic philosophy generally is away from the construction of all-encompassing systems and toward close analysis of individual ideas.

Among the developments that led to the revival of metaphysical theorizing were Quine's attack on the analytic–synthetic distinction, which was generally taken to undermine Carnap's distinction between existence questions internal to a framework and those external to it.[108]

The philosophy of fiction, the problem of empty names, and the debate over existence's status as a property have all come of relative obscurity into the limelight, while perennial issues such as free will, possible worlds, and the philosophy of time have had new life breathed into them.[109][110]

The analytic view is of metaphysics as studying phenomenal human concepts rather than making claims about the noumenal world, so its style often blurs into philosophy of language and introspective psychology. Compared to system-building, it can seem very dry, stylistically similar to computer programming, mathematics or even accountancy (as a common stated goal is to "account for" entities in the world).[citation needed]



Modern philosophy is philosophy developed in the modern era and associated with modernity. It is not a specific doctrine or school (and thus should not be confused with Modernism), although there are certain assumptions common to much of it, which helps to distinguish it from earlier philosophy.[1]

The 17th and early 20th centuries roughly mark the beginning and the end of modern philosophy. How much of the Renaissance should be included is a matter for dispute; likewise modernity may or may not have ended in the twentieth century and been replaced by postmodernity. How one decides these questions will determine the scope of one's use of the term "modern philosophy."

How much of Renaissance intellectual history is part of modern philosophy is disputed:[2] the Early Renaissance is often considered less modern and more medieval compared to the later High Renaissance. By the 17th and 18th centuries the major figures in philosophy of mind, epistemology, and metaphysics were roughly divided into two main groups. The "Rationalists," mostly in France and Germany, argued all knowledge must begin from certain "innate ideas" in the mind. Major rationalists were Descartes, Baruch Spinoza, Gottfried Leibniz, and Nicolas Malebranche. The "Empiricists," by contrast, held that knowledge must begin with sensory experience. Major figures in this line of thought are John Locke, George Berkeley, and David Hume (These are retrospective categories, for which Kant is largely responsible). Ethics and political philosophy are usually not subsumed under these categories, though all these philosophers worked in ethics, in their own distinctive styles. Other important figures in political philosophy include Thomas Hobbes and Jean-Jacques Rousseau.

In the late eighteenth century Immanuel Kant set forth a groundbreaking philosophical system which claimed to bring unity to rationalism and empiricism. Whether or not he was right, he did not entirely succeed in ending philosophical dispute. Kant sparked a storm of philosophical work in Germany in the early nineteenth century, beginning with German idealism. The characteristic theme of idealism was that the world and the mind equally must be understood according to the same categories; it culminated in the work of Georg Wilhelm Friedrich Hegel, who among many other things said that "The real is rational; the rational is real."

Hegel's work was carried in many directions by his followers and critics. Karl Marx appropriated both Hegel's philosophy of history and the empirical ethics dominant in Britain, transforming Hegel's ideas into a strictly materialist form, setting the grounds for the development of a science of society. Søren Kierkegaard, in contrast, dismissed all systematic philosophy as an inadequate guide to life and meaning. For Kierkegaard, life is meant to be lived, not a mystery to be solved. Arthur Schopenhauer took idealism to the conclusion that the world was nothing but the futile endless interplay of images and desires, and advocated atheism and pessimism. Schopenhauer's ideas were taken up and transformed by Nietzsche, who seized upon their various dismissals of the world to proclaim "God is dead" and to reject all systematic philosophy and all striving for a fixed truth transcending the individual. Nietzsche found in this not grounds for pessimism, but the possibility of a new kind of freedom.

19th-century British philosophy came increasingly to be dominated by strands of neo-Hegelian thought, and as a reaction against this, figures such as Bertrand Russell and George Edward Moore began moving in the direction of analytic philosophy, which was essentially an updating of traditional empiricism to accommodate the new developments in logic of the German mathematician Gottlob Frege.

Renaissance humanism emphasized the value of human beings (see Oration on the Dignity of Man) and opposed dogma and scholasticism. This new interest in human activities led to the development of political science with The Prince of Niccolò Machiavelli.[3] Humanists differed from Medieval scholars also because they saw the natural world as mathematically ordered and pluralistic, instead of thinking of it in terms of purposes and goals. Renaissance philosophy is perhaps best explained by two propositions made by Leonardo da Vinci in his notebooks:

All of our knowledge has its origins in our perceptions
There is no certainty where one can neither apply any of the mathematical sciences nor any of those which are based upon the mathematical sciences.
In a similar way, Galileo Galilei based his scientific method on experiments but also developed mathematical methods for application to problems in physics. These two ways to conceive human knowledge formed the background for the principle of Empiricism and Rationalism respectively.[4]

Modern philosophy traditionally begins with René Descartes and his dictum "I think, therefore I am". In the early seventeenth century the bulk of philosophy was dominated by Scholasticism, written by theologians and drawing upon Plato, Aristotle, and early Church writings. Descartes argued that many predominant Scholastic metaphysical doctrines were meaningless or false. In short, he proposed to begin philosophy from scratch. In his most important work, Meditations on First Philosophy, he attempts just this, over six brief essays. He tries to set aside as much as he possibly can of all his beliefs, to determine what if anything he knows for certain. He finds that he can doubt nearly everything: the reality of physical objects, God, his memories, history, science, even mathematics, but he cannot doubt that he is, in fact, doubting. He knows what he is thinking about, even if it is not true, and he knows that he is there thinking about it. From this basis he builds his knowledge back up again. He finds that some of the ideas he has could not have originated from him alone, but only from God; he proves that God exists. He then demonstrates that God would not allow him to be systematically deceived about everything; in essence, he vindicates ordinary methods of science and reasoning, as fallible but not false.

Empiricism is a theory of knowledge which opposes other theories of knowledge, such as rationalism, idealism and historicism. Empiricism asserts that knowledge comes (only or primarily) via sensory experience as opposed to rationalism, which asserts that knowledge comes (also) from pure thinking. Both empiricism and rationalism are individualist theories of knowledge, whereas historicism is a social epistemology. While historicism also acknowledges the role of experience, it differs from empiricism by assuming that sensory data cannot be understood without considering the historical and cultural circumstances in which observations are made. Empiricism should not be mixed up with empirical research because different epistemologies should be considered competing views on how best to do studies, and there is near consensus among researchers that studies should be empirical. Today empiricism should therefore be understood as one among competing ideals of getting knowledge or how to do studies. As such empiricism is first and foremost characterized by the ideal to let observational data "speak for themselves", while the competing views are opposed to this ideal. The term empiricism should thus not just be understood in relation to how this term has been used in the history of philosophy. It should also be constructed in a way which makes it possible to distinguish empiricism among other epistemological positions in contemporary science and scholarship. In other words: Empiricism as a concept has to be constructed along with other concepts, which together make it possible to make important discriminations between different ideals underlying contemporary science.

Empiricism is one of several competing views that predominate in the study of human knowledge, known as epistemology. Empiricism emphasizes the role of experience and evidence, especially sensory perception, in the formation of ideas, over the notion of innate ideas or tradition[5] in contrast to, for example, rationalism which relies upon reason and can incorporate innate knowledge.

Political philosophy is the study of such topics as politics, liberty, justice, property, rights, law, and the enforcement of a legal code by authority: what they are, why (or even if) they are needed, what, if anything, makes a government legitimate, what rights and freedoms it should protect and why, what form it should take and why, what the law is, and what duties citizens owe to a legitimate government, if any, and when it may be legitimately overthrown—if ever. In a vernacular sense, the term "political philosophy" often refers to a general view, or specific ethic, political belief or attitude, about politics that does not necessarily belong to the technical discipline of philosophy.[6]

Idealism refers to the group of philosophies which assert that reality, or reality as we can know it, is fundamentally a construct of the mind or otherwise immaterial. Epistemologically, idealism manifests as a skepticism about the possibility of knowing any mind-independent thing. In a sociological sense, idealism emphasizes how human ideas—especially beliefs and values—shape society.[7] As an ontological doctrine, idealism goes further, asserting that all entities are composed of mind or spirit.[8] Idealism thus rejects physicalist and dualist theories that fail to ascribe priority to the mind. An extreme version of this idealism can exist in the philosophical notion of solipsism.

Existentialism is generally considered to be the philosophical and cultural movement which holds that the starting point of philosophical thinking must be the individual and the experiences of the individual. Building on that, existentialists hold that moral thinking and scientific thinking together do not suffice to understand human existence, and, therefore, a further set of categories, governed by the norm of authenticity, is necessary to understand human existence.[9][10][11]

Phenomenology is the study of the structure of experience. It is a broad philosophical movement founded in the early years of the 20th century by Edmund Husserl, expanded upon by a circle of his followers at the universities of Göttingen and Munich in Germany. The philosophy then spread to France, the United States, and elsewhere, often in contexts far removed from Husserl's early work.[12]


Pragmatism is a philosophical tradition centered on the linking of practice and theory. It describes a process where theory is extracted from practice, and applied back to practice to form what is called intelligent practice.[citation needed] Important positions characteristic of pragmatism include instrumentalism, radical empiricism, verificationism, conceptual relativity, and fallibilism.[citation needed] There is general consensus among pragmatists that philosophy should take the methods and insights of modern science into account.[13] Charles Sanders Peirce (and his pragmatic maxim) deserves most of the credit for pragmatism,[14] along with later twentieth century contributors William James and John Dewey.[13]

Analytic philosophy came to dominate English-speaking countries in the 20th century. In the United States, United Kingdom, Canada, Scandinavia, Australia, and New Zealand, the overwhelming majority of university philosophy departments identify themselves as "analytic" departments.[15] The term generally refers to a broad philosophical tradition[16][17] characterized by an emphasis on clarity and argument (often achieved via modern formal logic and analysis of language) and a respect for the natural sciences.[18][19][20]

Various philosophical movements in Asia arose in the modern period including:

Liberty Enlightening the World (known as the Statue of Liberty) was donated to the US by France in 1886 as an artistic personification of liberty.
Broadly speaking, liberty is the ability to do as one pleases.[1] It is a synonym for the word freedom. In modern politics, liberty is the state of being free within society from oppressive restrictions imposed by authority on one's way of life, behaviour, or political views.[2][3][4] In philosophy, liberty involves free will as contrasted with determinism.[5] In theology, liberty is freedom from the effects of "sin, spiritual servitude, [or] worldly ties".[6] Sometimes liberty is differentiated from freedom by using the word "freedom" primarily, if not exclusively, to mean the ability to do as one wills and what one has the power to do; and using the word "liberty" to mean the absence of arbitrary restraints, taking into account the rights of all involved. In this sense, the exercise of liberty is subject to capability and limited by the rights of others.[7] Thus liberty entails the responsible use of freedom under the rule of law without depriving anyone else of their freedom. Freedom is more broad in that it represents a total lack of restraint or the unrestrained ability to fulfill one's desires. For example, a person can have the freedom to murder, but not have the liberty to murder, as the latter example deprives others of their right not to be harmed. Liberty can be taken away as a form of punishment. In many countries, people can be deprived of their liberty if they are convicted of criminal acts.

The word "liberty" is often used in slogans, such as "life, liberty, and the pursuit of happiness"[8] or "Liberty, Equality, Fraternity".[9]

Liberty originates from the Latin word libertas, derived from the name of the goddess Libertas, who, along with the Goddess of Liberty, usually portrays the concept, and the archaic Roman god Liber.


Philosophers from earliest times have considered the question of liberty. Roman Emperor Marcus Aurelius (121–180 AD) wrote:

a polity in which there is the same law for all, a polity administered with regard to equal rights and equal freedom of speech, and the idea of a kingly government which respects most of all the freedom of the governed.[10]


a free man is he that in those things which by his strength and wit he is able to do is not hindered to do what he hath the will to do.

John Locke (1632–1704) rejected that definition of liberty. While not specifically mentioning Hobbes, he attacks Sir Robert Filmer who had the same definition. According to Locke:

In the state of nature, liberty consists of being free from any superior power on Earth. People are not under the will or lawmaking authority of others but have only the law of nature for their rule. In political society, liberty consists of being under no other lawmaking power except that established by consent in the commonwealth. People are free from the dominion of any will or legal restraint apart from that enacted by their own constituted lawmaking power according to the trust put in it. Thus, freedom is not as Sir Robert Filmer defines it: 'A liberty for everyone to do what he likes, to live as he pleases, and not to be tied by any laws.' Freedom is constrained by laws in both the state of nature and political society. Freedom of nature is to be under no other restraint but the law of nature. Freedom of people under government is to be under no restraint apart from standing rules to live by that are common to everyone in the society and made by the lawmaking power established in it. Persons have a right or liberty to (1) follow their own will in all things that the law has not prohibited and (2) not be subject to the inconstant, uncertain, unknown, and arbitrary wills of others.[11]


John Stuart Mill (1806–1873), in his work, On Liberty, was the first to recognize the difference between liberty as the freedom to act and liberty as the absence of coercion.[12]

In his book Two Concepts of Liberty, Isaiah Berlin formally framed the differences between two perspectives as the distinction between two opposite concepts of liberty: positive liberty and negative liberty. The latter designates a negative condition in which an individual is protected from tyranny and the arbitrary exercise of authority, while the former refers to the liberty that comes from self-mastery, the freedom from inner compulsions such as weakness and fear.[13]

The Magna Carta (originally known as the Charter of Liberties) of 1215, written in iron gall ink on parchment in medieval Latin, using standard abbreviations of the period. This document is held at the British Library and is identified as "British Library Cotton MS Augustus II.106".
History

A romanticised 19th-century recreation of King John signing the Magna Carta. The charter would have been sealed rather than signed.
The modern concept of political liberty has its origins in the Greek concepts of freedom and slavery.[14] To be free, to the Greeks, was not to have a master, to be independent from a master (to live as one likes).[15][16] That was the original Greek concept of freedom. It is closely linked with the concept of democracy, as Aristotle put it:

"This, then, is one note of liberty which all democrats affirm to be the principle of their state. Another is that a man should live as he likes. This, they say, is the privilege of a freeman, since, on the other hand, not to live as a man likes is the mark of a slave. This is the second characteristic of democracy, whence has arisen the claim of men to be ruled by none, if possible, or, if this is impossible, to rule and be ruled in turns; and so it contributes to the freedom based upon equality."[17]
This applied only to free men. In Athens, for instance, women could not vote or hold office and were legally and socially dependent on a male relative.[18]

The populations of the Persian Empire enjoyed some degree of freedom. Citizens of all religions and ethnic groups were given the same rights and had the same freedom of religion, women had the same rights as men, and slavery was abolished (550 BC). All the palaces of the kings of Persia were built by paid workers in an era when slaves typically did such work.[19]

In the Maurya Empire of ancient India, citizens of all religions and ethnic groups had some rights to freedom, tolerance, and equality. The need for tolerance on an egalitarian basis can be found in the Edicts of Ashoka the Great, which emphasize the importance of tolerance in public policy by the government. The slaughter or capture of prisoners of war also appears to have been condemned by Ashoka.[20] Slavery also appears to have been non-existent in the Maurya Empire.[21] However, according to Hermann Kulke and Dietmar Rothermund, "Ashoka's orders seem to have been resisted right from the beginning."[22]

Roman law also embraced certain limited forms of liberty, even under the rule of the Roman Emperors. However, these liberties were accorded only to Roman citizens. Many of the liberties enjoyed under Roman law endured through the Middle Ages, but were enjoyed solely by the nobility, rarely by the common man.[citation needed] The idea of inalienable and universal liberties had to wait until the Age of Enlightenment.


In French Liberty. British Slavery (1792), James Gillray caricatured French "liberty" as the opportunity to starve and British "slavery" as bloated complaints about taxation.
The social contract theory, most influentially formulated by Hobbes, John Locke and Rousseau (though first suggested by Plato in The Republic), was among the first to provide a political classification of rights, in particular through the notion of sovereignty and of natural rights. The thinkers of the Enlightenment reasoned that law governed both heavenly and human affairs, and that law gave the king his power, rather than the king's power giving force to law. This conception of law would find its culmination in the ideas of Montesquieu. The conception of law as a relationship between individuals, rather than families, came to the fore, and with it the increasing focus on individual liberty as a fundamental reality, given by "Nature and Nature's God," which, in the ideal state, would be as universal as possible.

In On Liberty, John Stuart Mill sought to define the "...nature and limits of the power which can be legitimately exercised by society over the individual," and as such, he describes an inherent and continuous antagonism between liberty and authority and thus, the prevailing question becomes "how to make the fitting adjustment between individual independence and social control".[7]

England (and, following the Act of Union 1707, Great Britain), laid down the cornerstones of the concept of individual liberty.

In 1066 as a condition of his coronation William the Conqueror assented to the London Charter of Liberties which guaranteed the "Saxon" liberties of the City of London.

In 1100 the Charter of Liberties is passed which sets out certain liberties of nobles, church officials and individuals.

In 1166 Henry II of England transformed English law by passing the Assize of Clarendon. The act, a forerunner to trial by jury, started the abolition of trial by combat and trial by ordeal.[23]

1187-1189 sees the publication of Tractatus de legibus et consuetudinibus regni Anglie which contains authoritative definitions of freedom and servitude:

Freedom is the natural faculty of doing what each person pleases to do according to his will, except what is prohibited to him of right or by force. Sevitude on the other hand may be said to be the contrary, as if any person contrary to freedom should be bound upon a covenant to do something, or not to do it.[24]

In 1215 Magna Carta was enacted, arguably becoming the cornerstone of liberty in first England, then Great Britain, and later the world.[25][26]

In 1628 the English Parliament passed the Petition of Right which set out specific liberties of English subjects.

In 1679 the English Parliament passed the Habeas Corpus Act which outlawed unlawful or arbitrary imprisonment.

In 1689 the Bill of Rights granted "freedom of speech in Parliament", and reinforced many existing civil rights in England. The Scots law equivalent the Claim of Right is also passed.[27]

In 1772 the Somerset v Stewart judgement found that slavery was unsupported by common law in England and Wales.

In 1859 an essay by the philosopher John Stuart Mill, entitled On Liberty, argued for toleration and individuality. "If any opinion is compelled to silence, that opinion may, for aught we can certainly know, be true. To deny this is to assume our own infallibility."[28][29]

In 1958 Two Concepts of Liberty, by Isaiah Berlin, identified "negative liberty" as an obstacle, as distinct from "positive liberty" which promotes self-mastery and the concepts of freedom.[30]

In 1948 British representatives attempted to but were prevented from adding a legal framework to the Universal Declaration of Human Rights. (It was not until 1976 that the International Covenant on Civil and Political Rights came into force, giving a legal status to most of the Declaration.)[31]

The depiction of Liberty on the Walking Liberty Half Dollar.
According to the 1776 United States Declaration of Independence, all men have a natural right to "life, liberty, and the pursuit of happiness". But this declaration of liberty was troubled from the outset by the institutionalization of legalized Black slavery. Slave owners argued that their liberty was paramount since it involved property, their slaves, and that Blacks had no rights that any White man was obliged to recognize. The Supreme Court, in the Dred Scott decision, upheld this principle. It was not until 1866, following the Civil War, that the US Constitution was amended to extend these rights to persons of color, and not until 1920 that these rights were extended to women.[32]

By the later half of the 20th century, liberty was expanded further to prohibit government interference with personal choices. In the United States Supreme Court decision Griswold v. Connecticut, Justice William O. Douglas argued that liberties relating to personal relationships, such as marriage, have a unique primacy of place in the hierarchy of freedoms.[33] Jacob M. Appel has summarized this principle:

I am grateful that I have rights in the proverbial public square – but, as a practical matter, my most cherished rights are those that I possess in my bedroom and hospital room and death chamber. Most people are far more concerned that they can control their own bodies than they are about petitioning Congress.[34]

In modern America, various competing ideologies have divergent views about how best to promote liberty. Liberals in the original sense of the word see equality as a necessary component of freedom. Progressives stress freedom from business monopoly as essential. Libertarians disagree, and see economic freedom as best. The Tea Party movement sees the undefined "big government" as the enemy of freedom.[35][36]

France supported the Americans in their revolt against English rule and, in 1789, overthrew their own monarchy, with the cry of "Liberté, égalité, fraternité". The bloodbath that followed, known as the reign of terror, soured many people on the idea of liberty. Edmund Burke, considered one of the fathers of conservatism, wrote "The French had shewn themselves the ablest architects of ruin that had hitherto existed in the world."[37]

According to the Concise Oxford Dictionary of Politics, liberalism is "the belief that it is the aim of politics to preserve individual rights and to maximize freedom of choice". But they point out that there is considerable discussion about how to achieve those goals. Every discussion of freedom depends on three key components: who is free, what they are free to do, and what forces restrict their freedom.[38] John Gray argues that the core belief of liberalism is toleration. Liberals allow others freedom to do what they want, in exchange for having the same freedom in return. This idea of freedom is personal rather than political.[39] William Safire points out that liberalism is attacked by both the Right and the Left: by the Right for defending such practices as abortion, homosexuality, and atheism, and by the Left for defending free enterprise and the rights of the individual over the collective.[40]

Socialists view freedom as a concrete situation as opposed to a purely abstract ideal. Freedom is a state of being where individuals have agency to pursue their creative interests unhindered by coercive social relationships, specifically those they are forced to engage in as a requisite for survival under a given social system. Freedom thus requires both the material economic conditions that make freedom possible alongside social relationships and institutions conducive to freedom.[47]

The socialist conception of freedom is closely related to the socialist view of creativity and individuality. Influenced by Karl Marx's concept of alienated labor, socialists understand freedom to be the ability for an individual to engage in creative work in the absence of alienation, where "alienated labor" refers to work people are forced to perform and un-alienated work refers to individuals pursuing their own creative interests.[48]


For Karl Marx, meaningful freedom is only attainable in a communist society characterized by superabundance and free access. Such a social arrangement would eliminate the need for alienated labor and enable individuals to pursue their own creative interests, leaving them to develop and maximize their full potentialities. This goes alongside Marx's emphasis on the ability of socialism and communism progressively reducing the average length of the workday to expand the "realm of freedom", or discretionary free time, for each person.[49][50] Marx's notion of communist society and human freedom is thus radically individualistic.[51]


While many anarchists see freedom slightly differently, all oppose authority, including the authority of the state, of capitalism, and of nationalism.[52] For the Russian revolutionary anarchist Mikhail Bakunin, liberty did not mean an abstract ideal but a concrete reality based on the equal liberty of others. In a positive sense, liberty consists of "the fullest development of all the faculties and powers of every human being, by education, by scientific training, and by material prosperity." Such a conception of liberty is "eminently social, because it can only be realized in society," not in isolation. In a negative sense, liberty is "the revolt of the individual against all divine, collective, and individual authority."[53]

Some authors have suggested that a virtuous culture must exist as a prerequisite for liberty. Benjamin Franklin stated that "only a virtuous people are capable of freedom. As nations become corrupt and vicious, they have more need of masters."[54] Madison likewise declared: "To suppose that any form of government will secure liberty or happiness without any virtue in the people, is a chimerical idea."[55] John Adams acknowledged: "Our constitution was made only for a moral and religious people. It is wholly inadequate to the government of any other."[56]

J
Authority
From Wikipedia, the free encyclopedia
Jump to navigationJump to search
This article is about authority as a political concept. For other uses, see Authority (disambiguation).
Authority is the legitimate power which one person or group possesses and practices over another.[1] A civil state usually makes this formal by way of a judicial branch and an executive branch of government.[2] In the exercise of governance, the terms authority and power sometimes are inaccurately used as synonyms. The term authority identifies political legitimacy, which grants and justifies the right to exercise power of government; and the term power identifies the ability to accomplish an authorized goal, by way either of compliance or of obedience; hence, authority is the power to make decisions and the legitimacy to make such legal decisions and order their execution.[3]

Ancient understandings of authority trace back to Rome and draw later from Catholic (Thomistic) thought and other traditional understandings. In more modern terms, forms of authority include transitional authority exhibited in for example Cambodia,[4] public authority in the form of popular power, and, in more administrative terms, bureaucratic or managerial techniques. In terms of bureaucratic governance, one limitation of the governmental agents of the executive branch, as outlined by George A. Krause, is that they are not as close to the popular will as elected representatives are.[5] The claims of authority can extend to national or individual sovereignty, which is broadly or provisionally understood as a claim to political authority that is legitimated.[6]

Historical applications of authority in political terms include the formation of the city-state of Geneva, and experimental treatises involving the topic of authority in relation to education include Emile by Jean-Jacques Rousseau. As David Laitin defines, authority is a key concept to be defined in determining the range and role of political theory, science and inquiry.[7] The relevance of a grounded understanding of authority includes the basic foundation and formation of political, civil and/or ecclesiastical institutions or representatives. In recent years, however, authority in political contexts has been challenged or questioned.

Political philosophy
There have been several contributions to the debate of political authority. Among others, Hannah Arendt, Carl Joachim Friedrich, Thomas Hobbes, Alexandre Kojève and Carl Schmitt have provided some of the most remarkable texts.

In political philosophy, the jurisdiction of political authority, the location of sovereignty, the balancing of freedom and authority,[8] and the requirements of political obligations have been core questions from the time of Plato and Aristotle to the present. Most democratic societies are engaged in an ongoing discussion regarding the legitimate extent of the exercise of governmental authority. In the United States, for instance, there is a prevailing belief that the political system as instituted by the Founding Fathers should accord the populace as much freedom as reasonable, and that government should limit its authority accordingly, known as limited government.

In the discussion regarding the legitimacy of political authority, there are two beliefs at the respective ends of the spectrum. The first is the belief in the absolute freedom of the individual, otherwise known as political anarchism. The second is the belief that there must be a central authority in the form of a sovereign that claims ownership and control over the masses. This belief is known as statism. Sovereignty, in modern terms, can refer either to the adherence to a form of sovereign rule or the individual sovereignty, or autonomy, of a nation-state. The argument for political anarchy and anti-statism is made by Michael Huemer in his book The Problem of Political Authority. On the other side, one of the main arguments for the legitimacy of the state is some form of the “social contract theory” developed by Thomas Hobbes in his 1668 book, Leviathan, or by Jean-Jacques Rousseau in his political writings on the social contract.

Sociology
Since the emergence of the social sciences, authority has become a subject of research in a variety of empirical settings: the family (parental authority), small groups (informal authority of leadership), intermediate organizations such as schools, churches, armies, industries and bureaucracies (organizational and bureaucratic authority), and society-wide or inclusive organizations, ranging from the most primitive tribal society to the modern nation-state and intermediate organization (political authority).

The definition of authority in contemporary social science remains a matter of debate. Max Weber in his essay "Politics as a Vocation" (1919) divided legitimate authority into three types. Others, like Howard Bloom, suggest a parallel between authority and respect/reverence for ancestors.[9]

The United Kingdom and the Commonwealth realms
The political authority in the British context can be traced to James VI and I of Scotland who wrote two political treatise called Basilikon Doron and The Trve Lawe of free Monarchies: Or, The Reciprock and Mvtvall Dvtie Betwixt a free King, and his naturall Subiectes which advocated his right to rule on the basis of the concept of the divine right of kings, a theological concept that has basis in multiple religions, but in this case, Christianity, tracing this right to the apostolic succession. The King in the United Kingdom and the Commonwealth states are considered the foundations of judicial, legislative and executive authority.

The United States
The understanding of political authority and the exercise of political powers in the American context traces back to the writings of the Founding Fathers, including the arguments put forward in The Federalist Papers by James Madison, Alexander Hamilton and the First Chief Justice of the United States John Jay, and later speeches by the 16th President of the United States Abraham Lincoln. "Our government rests in public opinion," President Abraham Lincoln said in 1856.[10] In his 1854 Speech at Peoria, Illinois, Lincoln espoused "the proposition that each man should do precisely as he pleases with all which is exclusively his own," a principle existing "at the foundation of the sense of justice."[11] This sense of personal ownership and stewardship was integral to the practice of self-government as Abraham Lincoln saw it by a Republican nation and its people. This was because, as Abraham Lincoln also declared, "No man is good enough to govern another man, without that other's consent."[12]





Reason is the capacity of consciously making sense of things, applying logic, and adapting or justifying practices, institutions, and beliefs based on new or existing information.[1] It is closely associated with such characteristically human activities as philosophy, science, language, mathematics, and art, and is normally considered to be a distinguishing ability possessed by humans.[2] Reason is sometimes referred to as rationality.[3]

Reasoning is associated with thinking, cognition, and intellect. The field of logic studies ways in which humans reason formally through argument.[4] Reasoning may be subdivided into forms of logical reasoning, such as: deductive reasoning, inductive reasoning, and abductive reasoning. Aristotle drew a distinction between logical discursive reasoning (reason proper), and intuitive reasoning,[5] in which the reasoning process through intuition—however valid—may tend toward the personal and the subjectively opaque. In some social and political settings logical and intuitive modes of reasoning may clash, while in other contexts intuition and formal reason are seen as complementary rather than adversarial. For example, in mathematics, intuition is often necessary for the creative processes involved with arriving at a formal proof, arguably the most difficult of formal reasoning tasks.

Reasoning, like habit or intuition, is one of the ways by which thinking moves from one idea to a related idea. For example, reasoning is the means by which rational individuals understand sensory information from their environments, or conceptualize abstract dichotomies such as cause and effect, truth and falsehood, or ideas regarding notions of good or evil. Reasoning, as a part of executive decision making, is also closely identified with the ability to self-consciously change, in terms of goals, beliefs, attitudes, traditions, and institutions, and therefore with the capacity for freedom and self-determination.[6]

In contrast to the use of "reason" as an abstract noun, a reason is a consideration given which either explains or justifies events, phenomena, or behavior.[7] Reasons justify decisions, reasons support explanations of natural phenomena; reasons can be given to explain the actions (conduct) of individuals.

Using reason, or reasoning, can also be described more plainly as providing good, or the best, reasons. For example, when evaluating a moral decision, "morality is, at the very least, the effort to guide one's conduct by reason—that is, doing what there are the best reasons for doing—while giving equal [and impartial] weight to the interests of all those affected by what one does."[8]

Psychologists and cognitive scientists have attempted to study and explain how people reason, e.g. which cognitive and neural processes are engaged, and how cultural factors affect the inferences that people draw. The field of automated reasoning studies how reasoning may or may not be modeled computationally. Animal psychology considers the question of whether animals other than humans can reason.


In the English language and other modern European languages, "reason", and related words, represent words which have always been used to translate Latin and classical Greek terms in the sense of their philosophical usage.

The original Greek term was "λόγος" logos, the root of the modern English word "logic" but also a word which could mean for example "speech" or "explanation" or an "account" (of money handled).[9]
As a philosophical term logos was translated in its non-linguistic senses in Latin as ratio. This was originally not just a translation used for philosophy, but was also commonly a translation for logos in the sense of an account of money.[10]
French raison is derived directly from Latin, and this is the direct source of the English word "reason".[7]
The earliest major philosophers to publish in English, such as Francis Bacon, Thomas Hobbes, and John Locke also routinely wrote in Latin and French, and compared their terms to Greek, treating the words "logos", "ratio", "raison" and "reason" as interchangeable. The meaning of the word "reason" in senses such as "human reason" also overlaps to a large extent with "rationality" and the adjective of "reason" in philosophical contexts is normally "rational", rather than "reasoned" or "reasonable".[11] Some philosophers, Thomas Hobbes for example, also used the word ratiocination as a synonym for "reasoning".

Francisco de Goya, The Sleep of Reason Produces Monsters (El sueño de la razón produce monstruos), c. 1797
The proposal that reason gives humanity a special position in nature has been argued to be a defining characteristic of western philosophy and later western modern science, starting with classical Greece. Philosophy can be described as a way of life based upon reason, and in the other direction reason has been one of the major subjects of philosophical discussion since ancient times. Reason is often said to be reflexive, or "self-correcting", and the critique of reason has been a persistent theme in philosophy.[12] It has been defined in different ways, at different times, by different thinkers about human nature.

For many classical philosophers, nature was understood teleologically, meaning that every type of thing had a definitive purpose which fit within a natural order that was itself understood to have aims. Perhaps starting with Pythagoras or Heraclitus, the cosmos is even said to have reason.[13] Reason, by this account, is not just one characteristic that humans happen to have, and that influences happiness amongst other characteristics. Reason was considered of higher stature than other characteristics of human nature, such as sociability, because it is something humans share with nature itself, linking an apparently immortal part of the human mind with the divine order of the cosmos itself. Within the human mind or soul (psyche), reason was described by Plato as being the natural monarch which should rule over the other parts, such as spiritedness (thumos) and the passions. Aristotle, Plato's student, defined human beings as rational animals, emphasizing reason as a characteristic of human nature. He defined the highest human happiness or well being (eudaimonia) as a life which is lived consistently, excellently and completely in accordance with reason.[14]

The conclusions to be drawn from the discussions of Aristotle and Plato on this matter are amongst the most debated in the history of philosophy.[15] But teleological accounts such as Aristotle's were highly influential for those who attempt to explain reason in a way which is consistent with monotheism and the immortality and divinity of the human soul. For example, in the neo-platonist account of Plotinus, the cosmos has one soul, which is the seat of all reason, and the souls of all individual humans are part of this soul. Reason is for Plotinus both the provider of form to material things, and the light which brings individuals souls back into line with their source.[16] Such neo-Platonist accounts of the rational part of the human soul were standard amongst medieval Islamic philosophers, and under this influence, mainly via Averroes, came to be debated seriously in Europe until well into the renaissance, and they remain important in Iranian philosophy.[15]

Subject-centred reason in early modern philosophy
The early modern era was marked by a number of significant changes in the understanding of reason, starting in Europe. One of the most important of these changes involved a change in the metaphysical understanding of human beings. Scientists and philosophers began to question the teleological understanding of the world.[17] Nature was no longer assumed to be human-like, with its own aims or reason, and human nature was no longer assumed to work according to anything other than the same "laws of nature" which affect inanimate things. This new understanding eventually displaced the previous world view that derived from a spiritual understanding of the universe.


Accordingly, in the 17th century, René Descartes explicitly rejected the traditional notion of humans as "rational animals", suggesting instead that they are nothing more than "thinking things" along the lines of other "things" in nature. Any grounds of knowledge outside that understanding was, therefore, subject to doubt.

In his search for a foundation of all possible knowledge, Descartes deliberately decided to throw into doubt all knowledge – except that of the mind itself in the process of thinking:

At this time I admit nothing that is not necessarily true. I am therefore precisely nothing but a thinking thing; that is a mind, or intellect, or understanding, or reason – words of whose meanings I was previously ignorant.[18]

This eventually became known as epistemological or "subject-centred" reason, because it is based on the knowing subject, who perceives the rest of the world and itself as a set of objects to be studied, and successfully mastered by applying the knowledge accumulated through such study. Breaking with tradition and many thinkers after him, Descartes explicitly did not divide the incorporeal soul into parts, such as reason and intellect, describing them as one indivisible incorporeal entity.

A contemporary of Descartes, Thomas Hobbes described reason as a broader version of "addition and subtraction" which is not limited to numbers.[19] This understanding of reason is sometimes termed "calculative" reason. Similar to Descartes, Hobbes asserted that "No discourse whatsoever, can end in absolute knowledge of fact, past, or to come" but that "sense and memory" is absolute knowledge.[20]

In the late 17th century, through the 18th century, John Locke and David Hume developed Descartes' line of thought still further. Hume took it in an especially skeptical direction, proposing that there could be no possibility of deducing relationships of cause and effect, and therefore no knowledge is based on reasoning alone, even if it seems otherwise.[21][22]

Hume famously remarked that, "We speak not strictly and philosophically when we talk of the combat of passion and of reason. Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them."[23] Hume also took his definition of reason to unorthodox extremes by arguing, unlike his predecessors, that human reason is not qualitatively different from either simply conceiving individual ideas, or from judgments associating two ideas,[24] and that "reason is nothing but a wonderful and unintelligible instinct in our souls, which carries us along a certain train of ideas, and endows them with particular qualities, according to their particular situations and relations."[25] It followed from this that animals have reason, only much less complex than human reason.

In the 18th century, Immanuel Kant attempted to show that Hume was wrong by demonstrating that a "transcendental" self, or "I", was a necessary condition of all experience. Therefore, suggested Kant, on the basis of such a self, it is in fact possible to reason both about the conditions and limits of human knowledge. And so long as these limits are respected, reason can be the vehicle of morality, justice, aesthetics, theories of knowledge (epistemology), and understanding.

In the formulation of Kant, who wrote some of the most influential modern treatises on the subject, the great achievement of reason (German: Vernunft) is that it is able to exercise a kind of universal law-making. Kant was able therefore to reformulate the basis of moral-practical, theoretical and aesthetic reasoning, on "universal" laws.

Here practical reasoning is the self-legislating or self-governing formulation of universal norms, and theoretical reasoning the way humans posit universal laws of nature.[26]

Under practical reason, the moral autonomy or freedom of human beings depends on their ability to behave according to laws that are given to them by the proper exercise of that reason. This contrasted with earlier forms of morality, which depended on religious understanding and interpretation, or nature for their substance.[27]

According to Kant, in a free society each individual must be able to pursue their goals however they see fit, so long as their actions conform to principles given by reason. He formulated such a principle, called the "categorical imperative", which would justify an action only if it could be universalized:

Act only according to that maxim whereby you can, at the same time, will that it should become a universal law.[28]

In contrast to Hume then, Kant insists that reason itself (German Vernunft) has natural ends itself, the solution to the metaphysical problems, especially the discovery of the foundations of morality. Kant claimed that this problem could be solved with his "transcendental logic" which unlike normal logic is not just an instrument, which can be used indifferently, as it was for Aristotle, but a theoretical science in its own right and the basis of all the others.[29]

According to Jürgen Habermas, the "substantive unity" of reason has dissolved in modern times, such that it can no longer answer the question "How should I live?" Instead, the unity of reason has to be strictly formal, or "procedural". He thus described reason as a group of three autonomous spheres (on the model of Kant's three critiques):

Cognitive–instrumental reason is the kind of reason employed by the sciences. It is used to observe events, to predict and control outcomes, and to intervene in the world on the basis of its hypotheses;
Moral–practical reason is what we use to deliberate and discuss issues in the moral and political realm, according to universalizable procedures (similar to Kant's categorical imperative); and
Aesthetic reason is typically found in works of art and literature, and encompasses the novel ways of seeing the world and interpreting things that those practices embody.
For Habermas, these three spheres are the domain of experts, and therefore need to be mediated with the "lifeworld" by philosophers. In drawing such a picture of reason, Habermas hoped to demonstrate that the substantive unity of reason, which in pre-modern societies had been able to answer questions about the good life, could be made up for by the unity of reason's formalizable procedures.[30]

Hamann, Herder, Kant, Hegel, Kierkegaard, Nietzsche, Heidegger, Foucault, Rorty, and many other philosophers have contributed to a debate about what reason means, or ought to mean. Some, like Kierkegaard, Nietzsche, and Rorty, are skeptical about subject-centred, universal, or instrumental reason, and even skeptical toward reason as a whole. Others, including Hegel, believe that it has obscured the importance of intersubjectivity, or "spirit" in human life, and attempt to reconstruct a model of what reason should be.

Some thinkers, e.g. Foucault, believe there are other forms of reason, neglected but essential to modern life, and to our understanding of what it means to live a life according to reason.[12]

In the last several decades, a number of proposals have been made to "re-orient" this critique of reason, or to recognize the "other voices" or "new departments" of reason:

For example, in opposition to subject-centred reason, Habermas has proposed a model of communicative reason that sees it as an essentially cooperative activity, based on the fact of linguistic intersubjectivity.[31]

Nikolas Kompridis has proposed a widely encompassing view of reason as "that ensemble of practices that contributes to the opening and preserving of openness" in human affairs, and a focus on reason's possibilities for social change.[32]

The philosopher Charles Taylor, influenced by the 20th century German philosopher Martin Heidegger, has proposed that reason ought to include the faculty of disclosure, which is tied to the way we make sense of things in everyday life, as a new "department" of reason.[33]

In the essay "What is Enlightenment?", Michel Foucault proposed a concept of critique based on Kant's distinction between "private" and "public" uses of reason. This distinction, as suggested, has two dimensions:

Private reason is the reason that is used when an individual is "a cog in a machine" or when one "has a role to play in society and jobs to do: to be a soldier, to have taxes to pay, to be in charge of a parish, to be a civil servant".
Public reason is the reason used "when one is reasoning as a reasonable being (and not as a cog in a machine), when one is reasoning as a member of reasonable humanity". In these circumstances, "the use of reason must be free and public."[34]

The terms "logic" or "logical" are sometimes used as if they were identical with the term "reason" or with the concept of being "rational", or sometimes logic is seen as the most pure or the defining form of reason. For example in modern economics, rational choice is assumed to equate to logically consistent choice.

Reason and logic can however be thought of as distinct, although logic is one important aspect of reason. Author Douglas Hofstadter, in Gödel, Escher, Bach, characterizes the distinction in this way. Logic is done inside a system while reason is done outside the system by such methods as skipping steps, working backward, drawing diagrams, looking at examples, or seeing what happens if you change the rules of the system.[35]

Reason is a type of thought, and the word "logic" involves the attempt to describe rules or norms by which reasoning operates, so that orderly reasoning can be taught. The oldest surviving writing to explicitly consider the rules by which reason operates are the works of the Greek philosopher Aristotle, especially Prior Analysis and Posterior Analysis.[36] Although the Ancient Greeks had no separate word for logic as distinct from language and reason, Aristotle's newly coined word "syllogism" (syllogismos) identified logic clearly for the first time as a distinct field of study. When Aristotle referred to "the logical" (hē logikē), he was referring more broadly to rational thought.[37]

Reason compared to cause-and-effect thinking, and symbolic thinking
Main articles: Causality and Symbols
As pointed out by philosophers such as Hobbes, Locke and Hume, some animals are also clearly capable of a type of "associative thinking", even to the extent of associating causes and effects. A dog once kicked, can learn how to recognize the warning signs and avoid being kicked in the future, but this does not mean the dog has reason in any strict sense of the word. It also does not mean that humans acting on the basis of experience or habit are using their reason.[38]

Human reason requires more than being able to associate two ideas, even if those two ideas might be described by a reasoning human as a cause and an effect, perceptions of smoke, for example, and memories of fire. For reason to be involved, the association of smoke and the fire would have to be thought through in a way which can be explained, for example as cause and effect. In the explanation of Locke, for example, reason requires the mental use of a third idea in order to make this comparison by use of syllogism.[39]

More generally, reason in the strict sense requires the ability to create and manipulate a system of symbols, as well as indices and icons, according to Charles Sanders Peirce, the symbols having only a nominal, though habitual, connection to either smoke or fire.[40] One example of such a system of artificial symbols and signs is language.

The connection of reason to symbolic thinking has been expressed in different ways by philosophers. Thomas Hobbes described the creation of "Markes, or Notes of remembrance" (Leviathan Ch. 4) as speech. He used the word speech as an English version of the Greek word logos so that speech did not need to be communicated.[41] When communicated, such speech becomes language, and the marks or notes or remembrance are called "Signes" by Hobbes. Going further back, although Aristotle is a source of the idea that only humans have reason (logos), he does mention that animals with imagination, for whom sense perceptions can persist, come closest to having something like reasoning and nous, and even uses the word "logos" in one place to describe the distinctions which animals can perceive in such cases.[42]

Reason, imagination, mimesis, and memory
Main articles: Imagination, Mimesis, Memory, and Recollection
Reason and imagination rely on similar mental processes.[43] Imagination is not only found in humans. Aristotle, for example, stated that phantasia (imagination: that which can hold images or phantasmata) and phronein (a type of thinking that can judge and understand in some sense) also exist in some animals.[44] According to him, both are related to the primary perceptive ability of animals, which gathers the perceptions of different senses and defines the order of the things that are perceived without distinguishing universals, and without deliberation or logos. But this is not yet reason, because human imagination is different.

The recent modern writings of Terrence Deacon and Merlin Donald, writing about the origin of language, also connect reason connected to not only language, but also mimesis.[45] More specifically they describe the ability to create language as part of an internal modeling of reality specific to humankind. Other results are consciousness, and imagination or fantasy. In contrast, modern proponents of a genetic predisposition to language itself include Noam Chomsky and Steven Pinker, to whom Donald and Deacon can be contrasted.

As reason is symbolic thinking, and peculiarly human, then this implies that humans have a special ability to maintain a clear consciousness of the distinctness of "icons" or images and the real things they represent. Starting with a modern author, Merlin Donald writes[46]

A dog might perceive the "meaning" of a fight that was realistically play-acted by humans, but it could not reconstruct the message or distinguish the representation from its referent (a real fight). [...] Trained apes are able to make this distinction; young children make this distinction early – hence, their effortless distinction between play-acting an event and the event itself

In classical descriptions, an equivalent description of this mental faculty is eikasia, in the philosophy of Plato.[47] This is the ability to perceive whether a perception is an image of something else, related somehow but not the same, and therefore allows humans to perceive that a dream or memory or a reflection in a mirror is not reality as such. What Klein refers to as dianoetic eikasia is the eikasia concerned specifically with thinking and mental images, such as those mental symbols, icons, signes, and marks discussed above as definitive of reason. Explaining reason from this direction: human thinking is special in the way that we often understand visible things as if they were themselves images of our intelligible "objects of thought" as "foundations" (hypothēses in Ancient Greek). This thinking (dianoia) is "...an activity which consists in making the vast and diffuse jungle of the visible world depend on a plurality of more 'precise' noēta".[48]

Both Merlin Donald and the Socratic authors such as Plato and Aristotle emphasize the importance of mimesis, often translated as imitation or representation. Donald writes[49]

Imitation is found especially in monkeys and apes [... but ...] Mimesis is fundamentally different from imitation and mimicry in that it involves the invention of intentional representations. [...] Mimesis is not absolutely tied to external communication.

Mimēsis is a concept, now popular again in academic discussion, that was particularly prevalent in Plato's works, and within Aristotle, it is discussed mainly in the Poetics. In Michael Davis's account of the theory of man in this work.[50]

It is the distinctive feature of human action, that whenever we choose what we do, we imagine an action for ourselves as though we were inspecting it from the outside. Intentions are nothing more than imagined actions, internalizings of the external. All action is therefore imitation of action; it is poetic...[51]

Donald like Plato (and Aristotle, especially in On Memory and Recollection), emphasizes the peculiarity in humans of voluntary initiation of a search through one's mental world. The ancient Greek anamnēsis, normally translated as "recollection" was opposed to mneme or memory. Memory, shared with some animals,[52] requires a consciousness not only of what happened in the past, but also that something happened in the past, which is in other words a kind of eikasia[53] "...but nothing except man is able to recollect."[54] Recollection is a deliberate effort to search for and recapture something once known. Klein writes that, "To become aware of our having forgotten something means to begin recollecting."[55] Donald calls the same thing autocueing, which he explains as follows:[56] "Mimetic acts are reproducible on the basis of internal, self-generated cues. This permits voluntary recall of mimetic representations, without the aid of external cues – probably the earliest form of representational thinking."

In a celebrated paper in modern times, the fantasy author and philologist J.R.R. Tolkien wrote in his essay "On Fairy Stories" that the terms "fantasy" and "enchantment" are connected to not only "....the satisfaction of certain primordial human desires...." but also "...the origin of language and of the mind".

A subdivision of philosophy is logic. Logic is the study of reasoning. Looking at logical categorizations of different types of reasoning, the traditional main division made in philosophy is between deductive reasoning and inductive reasoning. Formal logic has been described as the science of deduction.[57] The study of inductive reasoning is generally carried out within the field known as informal logic or critical thinking.

Deduction is a form of reasoning in which a conclusion follows necessarily from the stated premises. A deduction is also the conclusion reached by a deductive reasoning process. One classic example of deductive reasoning is that found in syllogisms like the following:

Premise 1: All humans are mortal.
Premise 2: Socrates is a human.
Conclusion: Socrates is mortal.
The reasoning in this argument is deductively valid because there is no way in which the premises, 1 and 2, could be true and the conclusion, 3, be false.

Induction is a form of inference producing propositions about unobserved objects or types, either specifically or generally, based on previous observation. It is used to ascribe properties or relations to objects or types based on previous observations or experiences, or to formulate general statements or laws based on limited observations of recurring phenomenal patterns.

Inductive reasoning contrasts strongly with deductive reasoning in that, even in the best, or strongest, cases of inductive reasoning, the truth of the premises does not guarantee the truth of the conclusion. Instead, the conclusion of an inductive argument follows with some degree of probability. Relatedly, the conclusion of an inductive argument contains more information than is already contained in the premises. Thus, this method of reasoning is ampliative.

A classic example of inductive reasoning comes from the empiricist David Hume:

Premise: The sun has risen in the east every morning up until now.
Conclusion: The sun will also rise in the east tomorrow.
Analogical reasoning
Main article: Analogical reasoning
Analogical reasoning is a form of inductive reasoning from a particular to a particular. It is often used in case-based reasoning, especially legal reasoning.[58] An example follows:

Premise 1: Socrates is human and mortal.
Premise 2: Plato is human.
Conclusion: Plato is mortal.
Analogical reasoning is a weaker form of inductive reasoning from a single example, because inductive reasoning typically uses a large number of examples to reason from the particular to the general.[59] Analogical reasoning often leads to wrong conclusions. For example:

Premise 1: Socrates is human and male.
Premise 2: Ada Lovelace is human.
Conclusion: Therefore Ada Lovelace is male.

Abductive reasoning, or argument to the best explanation, is a form of reasoning that doesn't fit in deductive or inductive, since it starts with incomplete set of observations and proceeds with likely possible explanations so the conclusion in an abductive argument does not follow with certainty from its premises and concerns something unobserved. What distinguishes abduction from the other forms of reasoning is an attempt to favour one conclusion above others, by subjective judgement or attempting to falsify alternative explanations or by demonstrating the likelihood of the favoured conclusion, given a set of more or less disputable assumptions. For example, when a patient displays certain symptoms, there might be various possible causes, but one of these is preferred above others as being more probable.

Flawed reasoning in arguments is known as fallacious reasoning. Bad reasoning within arguments can be because it commits either a formal fallacy or an informal fallacy.

Formal fallacies occur when there is a problem with the form, or structure, of the argument. The word "formal" refers to this link to the form of the argument. An argument that contains a formal fallacy will always be invalid.

An informal fallacy is an error in reasoning that occurs due to a problem with the content, rather than mere structure, of the argument.

Traditional problems raised concerning reason
Philosophy is sometimes described as a life of reason, with normal human reason pursued in a more consistent and dedicated way than usual. Two categories of problem concerning reason have long been discussed by philosophers concerning reason, essentially being reasonings about reasoning itself as a human aim, or philosophizing about philosophizing. The first question is concerning whether we can be confident that reason can achieve knowledge of truth better than other ways of trying to achieve such knowledge. The other question is whether a life of reason, a life that aims to be guided by reason, can be expected to achieve a happy life more so than other ways of life (whether such a life of reason results in knowledge or not).

Since classical times a question has remained constant in philosophical debate (which is sometimes seen as a conflict between movements called Platonism and Aristotelianism) concerning the role of reason in confirming truth. People use logic, deduction, and induction, to reach conclusions they think are true. Conclusions reached in this way are considered, according to Aristotle, more certain than sense perceptions on their own.[60] On the other hand, if such reasoned conclusions are only built originally upon a foundation of sense perceptions, then, our most logical conclusions can never be said to be certain because they are built upon the very same fallible perceptions they seek to better.[61]

This leads to the question of what types of first principles, or starting points of reasoning, are available for someone seeking to come to true conclusions. In Greek, "first principles" are archai, "starting points",[62] and the faculty used to perceive them is sometimes referred to in Aristotle[63] and Plato[64] as nous which was close in meaning to awareness or consciousness.[65]

Empiricism (sometimes associated with Aristotle[66] but more correctly associated with British philosophers such as John Locke and David Hume, as well as their ancient equivalents such as Democritus) asserts that sensory impressions are the only available starting points for reasoning and attempting to attain truth. This approach always leads to the controversial conclusion that absolute knowledge is not attainable. Idealism, (associated with Plato and his school), claims that there is a "higher" reality, from which certain people can directly arrive at truth without needing to rely only upon the senses, and that this higher reality is therefore the primary source of truth.

Philosophers such as Plato, Aristotle, Al-Farabi, Avicenna, Averroes, Maimonides, Aquinas and Hegel are sometimes said to have argued that reason must be fixed and discoverable—perhaps by dialectic, analysis, or study. In the vision of these thinkers, reason is divine or at least has divine attributes. Such an approach allowed religious philosophers such as Thomas Aquinas and Étienne Gilson to try to show that reason and revelation are compatible. According to Hegel, "...the only thought which Philosophy brings with it to the contemplation of History, is the simple conception of reason; that reason is the Sovereign of the World; that the history of the world, therefore, presents us with a rational process."[67]

Since the 17th century rationalists, reason has often been taken to be a subjective faculty, or rather the unaided ability (pure reason) to form concepts. For Descartes, Spinoza and Leibniz, this was associated with mathematics. Kant attempted to show that pure reason could form concepts (time and space) that are the conditions of experience. Kant made his argument in opposition to Hume, who denied that reason had any role to play in experience.

Reason versus emotion or passion
See also: Emotion and Passion (emotion)
After Plato and Aristotle, western literature often treated reason as being the faculty that trained the passions and appetites.[citation needed] Stoic philosophy by contrast considered all passions undesirable.[citation needed] After the critiques of reason in the early Enlightenment the appetites were rarely discussed or conflated with the passions.[citation needed] Some Enlightenment camps took after the Stoics to say Reason should oppose Passion rather than order it, while others like the Romantics believed that Passion displaces Reason, as in the maxim "follow your heart".[citation needed]

Reason has been seen as a slave, or judge, of the passions, notably in the work of David Hume, and more recently of Freud.[citation needed] Reasoning which claims that the object of a desire is demanded by logic alone is called rationalization.[citation needed]

Rousseau first proposed, in his second Discourse, that reason and political life is not natural and possibly harmful to mankind.[68] He asked what really can be said about what is natural to mankind. What, other than reason and civil society, "best suits his constitution"? Rousseau saw "two principles prior to reason" in human nature. First we hold an intense interest in our own well-being. Secondly we object to the suffering or death of any sentient being, especially one like ourselves.[69] These two passions lead us to desire more than we could achieve. We become dependent upon each other, and on relationships of authority and obedience. This effectively puts the human race into slavery. Rousseau says that he almost dares to assert that nature does not destine men to be healthy. According to Velkley, "Rousseau outlines certain programs of rational self-correction, most notably the political legislation of the Contrat Social and the moral education in Émile. All the same, Rousseau understands such corrections to be only ameliorations of an essentially unsatisfactory condition, that of socially and intellectually corrupted humanity."

This quandary presented by Rousseau led to Kant's new way of justifying reason as freedom to create good and evil. These therefore are not to be blamed on nature or God. In various ways, German Idealism after Kant, and major later figures such Nietzsche, Bergson, Husserl, Scheler, and Heidegger, remain preoccupied with problems coming from the metaphysical demands or urges of reason.[70] The influence of Rousseau and these later writers is also large upon art and politics. Many writers (such as Nikos Kazantzakis) extol passion and disparage reason. In politics modern nationalism comes from Rousseau's argument that rationalist cosmopolitanism brings man ever further from his natural state.[71]

Another view on reason and emotion was proposed in the 1994 book titled Descartes' Error by Antonio Damasio. In it, Damasio presents the "Somatic Marker Hypothesis" which states that emotions guide behavior and decision-making. Damasio argues that these somatic markers (known collectively as "gut feelings") are "intuitive signals" that direct our decision making processes in a certain way that cannot be solved with rationality alone. Damasio further argues that rationality requires emotional input in order to function.

There are many religious traditions, some of which are explicitly fideist and others of which claim varying degrees of rationalism. Secular critics sometimes accuse all religious adherents of irrationality, since they claim such adherents are guilty of ignoring, suppressing, or forbidding some kinds of reasoning concerning some subjects (such as religious dogmas, moral taboos, etc.).[72] Though the theologies and religions such as classical monotheism typically do not claim to be irrational, there is often a perceived conflict or tension between faith and tradition on the one hand, and reason on the other, as potentially competing sources of wisdom, law and truth.[73][74]

Religious adherents sometimes respond by arguing that faith and reason can be reconciled, or have different non-overlapping domains, or that critics engage in a similar kind of irrationalism:

Reconciliation: Philosopher Alvin Plantinga argues that there is no real conflict between reason and classical theism because classical theism explains (among other things) why the universe is intelligible and why reason can successfully grasp it.[75][76]
Non-overlapping magisteria: Evolutionary biologist Stephen Jay Gould argues that there need not be conflict between reason and religious belief because they are each authoritative in their own domain (or "magisterium").[77][78] For example, perhaps reason alone is not enough to explain such big questions as the origins of the universe, the origin of life, the origin of consciousness,[79] the foundation of morality, or the destiny of the human race. If so, reason can work on those problems over which it has authority while other sources of knowledge or opinion can have authority on the big questions.[80]
Tu quoque: Philosophers Alasdair MacIntyre and Charles Taylor argue that those critics of traditional religion who are adherents of secular liberalism are also sometimes guilty of ignoring, suppressing, and forbidding some kinds of reasoning about subjects.[81][82] Similarly, philosophers of science such as Paul Feyarabend argue that scientists sometimes ignore or suppress evidence contrary to the dominant paradigm.
Unification: Theologian Joseph Ratzinger, later Benedict XVI, asserted that "Christianity has understood itself as the religion of the Logos, as the religion according to reason," referring to John 1:Ἐν ἀρχῇ ἦν ὁ λόγος, usually translated as "In the beginning was the Word (Logos)." Thus, he said that the Christian faith is "open to all that is truly rational", and that the rationality of Western Enlightenment "is of Christian origin".[83]
Some commentators have claimed that Western civilization can be almost defined by its serious testing of the limits of tension between "unaided" reason and faith in "revealed" truths—figuratively summarized as Athens and Jerusalem, respectively.[84][85] Leo Strauss spoke of a "Greater West" that included all areas under the influence of the tension between Greek rationalism and Abrahamic revelation, including the Muslim lands. He was particularly influenced by the great Muslim philosopher Al-Farabi. To consider to what extent Eastern philosophy might have partaken of these important tensions, Strauss thought it best to consider whether dharma or tao may be equivalent to Nature (by which we mean physis in Greek). According to Strauss the beginning of philosophy involved the "discovery or invention of nature" and the "pre-philosophical equivalent of nature" was supplied by "such notions as 'custom' or 'ways'", which appear to be really universal in all times and places. The philosophical concept of nature or natures as a way of understanding archai (first principles of knowledge) brought about a peculiar tension between reasoning on the one hand, and tradition or faith on the other.[86]

Although there is this special history of debate concerning reason and faith in the Islamic, Christian and Jewish traditions, the pursuit of reason is sometimes argued to be compatible with the other practice of other religions of a different nature, such as Hinduism, because they do not define their tenets in such an absolute way.[87]

Aristotle famously described reason (with language) as a part of human nature, which means that it is best for humans to live "politically" meaning in communities of about the size and type of a small city state (polis in Greek). For example...

It is clear, then, that a human being is more of a political [politikon = of the polis] animal [zōion] than is any bee or than any of those animals that live in herds. For nature, as we say, makes nothing in vain, and humans are the only animals who possess reasoned speech [logos]. Voice, of course, serves to indicate what is painful and pleasant; that is why it is also found in other animals, because their nature has reached the point where they can perceive what is painful and pleasant and express these to each other. But speech [logos] serves to make plain what is advantageous and harmful and so also what is just and unjust. For it is a peculiarity of humans, in contrast to the other animals, to have perception of good and bad, just and unjust, and the like; and the community in these things makes a household or city [polis]. [...] By nature, then, the drive for such a community exists in everyone, but the first to set one up is responsible for things of very great goodness. For as humans are the best of all animals when perfected, so they are the worst when divorced from law and right. The reason is that injustice is most difficult to deal with when furnished with weapons, and the weapons a human being has are meant by nature to go along with prudence and virtue, but it is only too possible to turn them to contrary uses. Consequently, if a human being lacks virtue, he is the most unholy and savage thing, and when it comes to sex and food, the worst. But justice is something political [to do with the polis], for right is the arrangement of the political community, and right is discrimination of what is just. (Aristotle's Politics 1253a 1.2. Peter Simpson's translation, with Greek terms inserted in square brackets.)

The concept of human nature being fixed in this way, implied, in other words, that we can define what type of community is always best for people. This argument has remained a central argument in all political, ethical and moral thinking since then, and has become especially controversial since firstly Rousseau's Second Discourse, and secondly, the Theory of Evolution. Already in Aristotle there was an awareness that the polis had not always existed and had needed to be invented or developed by humans themselves. The household came first, and the first villages and cities were just extensions of that, with the first cities being run as if they were still families with Kings acting like fathers.[88]

Friendship [philia] seems to prevail [in] man and woman according to nature [kata phusin]; for people are by nature [tēi phusei] pairing [sunduastikon] more than political [politikon = of the polis], in as much as the household [oikos] is prior [proteron = earlier] and more necessary than the polis and making children is more common [koinoteron] with the animals. In the other animals, community [koinōnia] goes no further than this, but people live together [sumoikousin] not only for the sake of making children, but also for the things for life; for from the start the functions [erga] are divided, and are different [for] man and woman. Thus they supply each other, putting their own into the common [eis to koinon]. It is for these [reasons] that both utility [chrēsimon] and pleasure [hēdu] seem to be found in this kind of friendship. (Nicomachean Ethics, VIII.12.1162a. Rough literal translation with Greek terms shown in square brackets.)

Rousseau in his Second Discourse finally took the shocking step of claiming that this traditional account has things in reverse: with reason, language and rationally organized communities all having developed over a long period of time merely as a result of the fact that some habits of cooperation were found to solve certain types of problems, and that once such cooperation became more important, it forced people to develop increasingly complex cooperation—often only to defend themselves from each other.

In other words, according to Rousseau, reason, language and rational community did not arise because of any conscious decision or plan by humans or gods, nor because of any pre-existing human nature. As a result, he claimed, living together in rationally organized communities like modern humans is a development with many negative aspects compared to the original state of man as an ape. If anything is specifically human in this theory, it is the flexibility and adaptability of humans. This view of the animal origins of distinctive human characteristics later received support from Charles Darwin's Theory of Evolution.

The two competing theories concerning the origins of reason are relevant to political and ethical thought because, according to the Aristotelian theory, a best way of living together exists independently of historical circumstances. According to Rousseau, we should even doubt that reason, language and politics are a good thing, as opposed to being simply the best option given the particular course of events that lead to today. Rousseau's theory, that human nature is malleable rather than fixed, is often taken to imply, for example by Karl Marx, a wider range of possible ways of living together than traditionally known.

However, while Rousseau's initial impact encouraged bloody revolutions against traditional politics, including both the French Revolution and the Russian Revolution, his own conclusions about the best forms of community seem to have been remarkably classical, in favor of city-states such as Geneva, and rural living.

Scientific research into reasoning is carried out within the fields of psychology and cognitive science. Psychologists attempt to determine whether or not people are capable of rational thought in a number of different circumstances.

Assessing how well someone engages in reasoning is the project of determining the extent to which the person is rational or acts rationally. It is a key research question in the psychology of reasoning. Rationality is often divided into its respective theoretical and practical counterparts.

Behavioral experiments on human reasoning
Experimental cognitive psychologists carry out research on reasoning behaviour. Such research may focus, for example, on how people perform on tests of reasoning such as intelligence or IQ tests, or on how well people's reasoning matches ideals set by logic (see, for example, the Wason test).[89] Experiments examine how people make inferences from conditionals e.g., If A then B and how they make inferences about alternatives, e.g., A or else B.[90] They test whether people can make valid deductions about spatial and temporal relations, e.g., A is to the left of B, or A happens after B, and about quantified assertions, e.g., All the A are B.[91] Experiments investigate how people make inferences about factual situations, hypothetical possibilities, probabilities, and counterfactual situations.[92]

Developmental psychologists investigate the development of reasoning from birth to adulthood. Piaget's theory of cognitive development was the first complete theory of reasoning development. Subsequently, several alternative theories were proposed, including the neo-Piagetian theories of cognitive development.[93]

The biological functioning of the brain is studied by neurophysiologists and neuropsychologists. Research in this area includes research into the structure and function of normally functioning brains, and of damaged or otherwise unusual brains. In addition to carrying out research into reasoning, some psychologists, for example, clinical psychologists and psychotherapists work to alter people's reasoning habits when they are unhelpful.


In artificial intelligence and computer science, scientists study and use automated reasoning for diverse applications including automated theorem proving the formal semantics of programming languages, and formal specification in software engineering.

Meta-reasoning is reasoning about reasoning. In computer science, a system performs meta-reasoning when it is reasoning about its own operation.[94] This requires a programming language capable of reflection, the ability to observe and modify its own structure and behaviour.

Dan Sperber believes that reasoning in groups is more effective and promotes their evolutionary fitness.
A species could benefit greatly from better abilities to reason about, predict and understand the world. French social and cognitive scientists Dan Sperber and Hugo Mercier argue that there could have been other forces driving the evolution of reason. They point out that reasoning is very difficult for humans to do effectively, and that it is hard for individuals to doubt their own beliefs (confirmation bias). Reasoning is most effective when it is done as a collective – as demonstrated by the success of projects like science. They suggest that there are not just individual, but group selection pressures at play. Any group that managed to find ways of reasoning effectively would reap benefits for all its members, increasing their fitness. This could also help explain why humans, according to Sperber, are not optimized to reason effectively alone. Their argumentative theory of reasoning claims that reason may have more to do with winning arguments than with the search for the truth.[95][96]

Critical thinking is the analysis of facts to form a judgment.[1] The subject is complex, and several different definitions exist, which generally include the rational, skeptical, unbiased analysis, or evaluation of factual evidence. Critical thinking is self-directed, self-disciplined, self-monitored, and self-corrective thinking.[2] It presupposes assent to rigorous standards of excellence and mindful command of their use. It entails effective communication and problem-solving abilities as well as a commitment to overcome native egocentrism[3][4] and sociocentrism.

The earliest records of critical thinking are the teachings of Socrates recorded by Plato. These included a part in Plato's early dialogues, where Socrates engages with one or more interlocutors on the issue of ethics such as question whether it was right for Socrates to escape from prison.[5] The philosopher considered and reflected on this question and came to the conclusion that escape violates all the things that he holds higher than himself: the laws of Athens and the guiding voice that Socrates claims to hear.[5]

Socrates established the fact that one cannot depend upon those in "authority" to have sound knowledge and insight. He demonstrated that persons may have power and high position and yet be deeply confused and irrational. Socrates maintained that for an individual to have a good life or to have one that is worth living, he must be critical questioner or must have an interrogative soul.[6] He established the importance of asking deep questions that probe profoundly into thinking before we accept ideas as worthy of belief.

Socrates established the importance of "seeking evidence, closely examining reasoning and assumptions, analyzing basic concepts, and tracing out implications not only of what is said but of what is done as well".[7] His method of questioning is now known as "Socratic questioning" and is the best known critical thinking teaching strategy. In his mode of questioning, Socrates highlighted the need for thinking for clarity and logical consistency. He asked people questions to reveal their irrational thinking or lack of reliable knowledge. Socrates demonstrated that having authority does not ensure accurate knowledge. He established the method of questioning beliefs, closely inspecting assumptions and relying on evidence and sound rationale. Plato recorded Socrates' teachings and carried on the tradition of critical thinking. Aristotle and subsequent Greek skeptics refined Socrates' teachings, using systematic thinking and asking questions to ascertain the true nature of reality beyond the way things appear from a glance.[8]

Socrates set the agenda for the tradition of critical thinking, namely, to reflectively question common beliefs and explanations, carefully distinguishing beliefs that are reasonable and logical from those that—however appealing to our native egocentrism, however much they serve our vested interests, however comfortable or comforting they may be—lack adequate evidence or rational foundation to warrant belief.

Critical thinking was described by Richard W. Paul as a movement in two waves (1994).[9] The "first wave" of critical thinking is often referred to as a 'critical analysis' that is clear, rational thinking involving critique. Its details vary amongst those who define it. According to Barry K. Beyer (1995), critical thinking means making clear, reasoned judgments. During the process of critical thinking, ideas should be reasoned, well thought out, and judged.[10] The U.S. National Council for Excellence in Critical Thinking[11] defines critical thinking as the "intellectually disciplined process of actively and skillfully conceptualizing, applying, analyzing, synthesizing, or evaluating information gathered from, or generated by, observation, experience, reflection, reasoning, or communication, as a guide to belief and action."[12]

In the term critical thinking, the word critical, (Grk. κριτικός = kritikos = "critic") derives from the word critic and implies a critique; it identifies the intellectual capacity and the means "of judging", "of judgement", "for judging", and of being "able to discern".[13] The intellectual roots of critical[14] thinking are as ancient as its etymology, traceable, ultimately, to the teaching practice and vision of Socrates[15] 2,500 years ago who discovered by a method of probing questioning that people could not rationally justify their confident claims to knowledge.

Traditionally, critical thinking has been variously defined as follows:

"The process of actively and skillfully conceptualizing, applying, analyzing, synthesizing, and evaluating information to reach an answer or conclusion"[16]
"Disciplined thinking that is clear, rational, open-minded, and informed by evidence"[16]
"Purposeful, self-regulatory judgment which results in interpretation, analysis, evaluation, and inference, as well as explanation of the evidential, conceptual, methodological, criteriological, or contextual considerations upon which that judgment is based"[17]
"Includes a commitment to using reason in the formulation of our beliefs"[18]
The skill and propensity to engage in an activity with reflective scepticism (McPeck, 1981)[19]
Thinking about one's thinking in a manner designed to organize and clarify, raise the efficiency of, and recognize errors and biases in one's own thinking. Critical thinking is not 'hard' thinking nor is it directed at solving problems (other than 'improving' one's own thinking). Critical thinking is inward-directed with the intent of maximizing the rationality of the thinker. One does not use critical thinking to solve problems—one uses critical thinking to improve one's process of thinking.[20]
"An appraisal based on careful analytical evaluation"[21]
"Critical thinking is a type of thinking pattern that requires people to be reflective, and pay attention to decision-making which guides their beliefs and actions. Critical thinking allows people to deduct with more logic, to process sophisticated information and to look at various sides of issues, so that they can produce more solid conclusions."[22]
Critical thinking has seven critical features: being inquisitive and curious, being open-minded to different sides, being able to think systematically, being analytical, being persistent to truth, being confident about critical thinking itself, and lastly, being mature.[23]
Although critical thinking could be defined in several different ways, there is a general agreement in its key component—the desire to reach for a satisfactory result, and this should be achieved by rational thinking and result-driven manner. Halpern thinks that critical thinking firstly involves learned abilities such as problem-solving, calculation and successful probability application. It also includes a tendency to engage the thinking process. In recent times, Stanovich believed that modern IQ test could hardly measure the ability of critical thinking.[24]
Contemporary critical thinking scholars have expanded these traditional definitions to include qualities, concepts, and processes such as creativity, imagination, discovery, reflection, empathy, connecting knowing, feminist theory, subjectivity, ambiguity, and inconclusiveness. Some definitions of critical thinking exclude these subjective practices.[25][16]

According to Ennis, "Critical thinking is the intellectually disciplined process of actively and skillfully conceptualizing, applying, analyzing, synthesizing, and/or evaluating information gathered from, or generated by, observation, experience, reflection, reasoning, or communication, as a guide to belief and action."[26] This definition Ennis provided is highly agreed by Harvey Siegel,[27] Peter Facione,[23] and Deanna Kuhn.[28]
According to the definition by Ennis, critical thinking requires a lot of attention and brain function. When critical thinking approach toward education it will help the students brain to be more function-able and understand texts differently.
Critical thinking can be specifically identified into many different portions, different fields of student requires different types of critical thinking. Critical thinking provides more angles and perspectives upon the same material,

The study of logical argumentation is relevant to the study of critical thinking. Logic is concerned with the analysis of arguments, including the appraisal of their correctness or incorrectness.[29] In the field of epistemology, critical thinking is considered to be logically correct thinking, which allows for differentiation between logically true and logically false statements.[30]

In "First wave" logical thinking, the thinker is removed from the train of thought, and the analysis of connections between concepts or points in thought is ostencibly free of any bias. In his essay Beyond Logicism in Critical Thinking Kerry S. Walters describes this ideology thus: "A logistic approach to critical thinking conveys the message to students that thinking is legitimate only when it conforms to the procedures of informal (and, to a lesser extent, formal) logic and that the good thinker necessarily aims for styles of examination and appraisal that are analytical, abstract, universal, and objective. This model of thinking has become so entrenched in conventional academic wisdom that many educators accept it as canon".[31] Such principles are concomitant with the increasing dependence on a quantitative understanding of the world.[32]

In the 'second wave' of critical thinking, authors consciously moved away from the logocentric mode of critical thinking characteristic of the 'first wave'. Although many scholars began to take a less exclusive view of what constitutes critical thinking, rationality and logic remain widely accepted as essential bases for critical thinking. Walters argues that exclusive logicism in the first wave sense is based on "the unwarranted assumption that good thinking is reducible to logical thinking".[31]

There are three types of logical reasoning. Informally, two kinds of logical reasoning can be distinguished in addition to formal deduction, which are induction and abduction.

Deduction
Deduction is the conclusion drawn from the structure of an argument's premises, by use of rules of inference formally those of propositional calculus. For example: X is human and all humans have a face, so X has a face.
Induction
Induction is drawing a conclusion from a pattern that is guaranteed by the strictness of the structure to which it applies. For example: The sum of even integers is even. Let {\displaystyle x,y,z\in \mathbb {Z} }{\displaystyle x,y,z\in \mathbb {Z} } then {\displaystyle 2x,2y,2z}{\displaystyle 2x,2y,2z} are even by definition. {\displaystyle 2x+2y=2(x+y)=2z}{\displaystyle 2x+2y=2(x+y)=2z}, which is even; so summing two even numbers results in an even number.
Abduction
Abduction is drawing a conclusion using a heuristic that is likely, but not inevitable given some foreknowledge. For example: I observe sheep in a field, and they appear white from my viewing angle, so sheep are white. Contrast with the deductive statement: Some sheep are white on at least one side.
Critical thinking and rationality
Kerry S. Walters, an emeritus philosophy professor from Gettysburg College, argues that rationality demands more than just logical or traditional methods of problem solving and analysis or what he calls the "calculus of justification" but also considers "cognitive acts such as imagination, conceptual creativity, intuition and insight" (p. 63). These "functions" are focused on discovery, on more abstract processes instead of linear, rules-based approaches to problem-solving. The linear and non-sequential mind must both be engaged in the rational mind.[31]

The ability to critically analyze an argument—to dissect structure and components, thesis and reasons—is essential. But so is the ability to be flexible and consider non-traditional alternatives and perspectives. These complementary functions are what allow for critical thinking to be a practice encompassing imagination and intuition in cooperation with traditional modes of deductive inquiry.[31]

Functions
The list of core critical thinking skills includes observation, interpretation, analysis, inference, evaluation, explanation, and metacognition. According to Reynolds (2011), an individual or group engaged in a strong way of critical thinking gives due consideration to establish for instance:[33]

Evidence through reality
Context skills to isolate the problem from context
Relevant criteria for making the judgment well
Applicable methods or techniques for forming the judgment
Applicable theoretical constructs for understanding the problem and the question at hand
In addition to possessing strong critical-thinking skills, one must be disposed to engage problems and decisions using those skills. Critical thinking employs not only logic but broad intellectual criteria such as clarity, credibility, accuracy, precision, relevance, depth, breadth, significance, and fairness.[34]

Critical thinking calls for the ability to:

Recognize problems, to find workable means for meeting those problems
Understand the importance of prioritization and order of precedence in problem-solving
Gather and marshal pertinent (relevant) information
Recognize unstated assumptions and values
Comprehend and use language with accuracy, clarity, and discernment
Interpret data, to appraise evidence and evaluate arguments
Recognize the existence (or non-existence) of logical relationships between propositions
Draw warranted conclusions and generalizations
Put to test the conclusions and generalizations at which one arrives
Reconstruct one's patterns of beliefs on the basis of wider experience
Render accurate judgments about specific things and qualities in everyday life
In sum:

"A persistent effort to examine any belief or supposed form of knowledge in the light of the evidence that supports or refutes it and the further conclusions to which it tends."[35]

The habits of mind that characterize a person strongly disposed toward critical thinking include a desire to follow reason and evidence wherever they may lead, a systematic approach to problem solving, inquisitiveness, even-handedness, and confidence in reasoning.[36]

According to a definition analysis by Kompf & Bond (2001), critical thinking involves problem solving, decision making, metacognition,[37] rationality, rational thinking, reasoning, knowledge, intelligence and also a moral component such as reflective thinking. Critical thinkers therefore need to have reached a level of maturity in their development, possess a certain attitude as well as a set of taught skills.

There is a postulation by some writers that the tendencies from habits of mind should be thought as virtues to demonstrate the characteristics of a critical thinker.[38] These intellectual virtues are ethical qualities that encourage motivation to think in particular ways towards specific circumstances. However, these virtues have also been criticized by skeptics, who argue that there is lacking evidence for this specific mental basis that are causative to critical thinking.[39]

Edward M. Glaser proposed that the ability to think critically involves three elements:[35]

An attitude of being disposed to consider in a thoughtful way the problems and subjects that come within the range of one's experiences
Knowledge of the methods of logical inquiry and reasoning
Some skill in applying those methods.
Educational programs aimed at developing critical thinking in children and adult learners, individually or in group problem solving and decision making contexts, continue to address these same three central elements.

The Critical Thinking project at Human Science Lab, London, is involved in the scientific study of all major educational systems in prevalence today to assess how the systems are working to promote or impede critical thinking.[40]

Contemporary cognitive psychology regards human reasoning as a complex process that is both reactive and reflective.[41] This presents a problem which is detailed as a division of a critical mind in juxtaposition to sensory data and memory.

The psychological theory disposes of the absolute nature of the rational mind, in reference to conditions, abstract problems and discursive limitations. Where the relationship between critical thinking skills and critical thinking dispositions is an empirical question, the ability to attain causal domination exists, for which Socrates was known to be largely disposed against as the practice of Sophistry. Accounting for a measure of "critical thinking dispositions" is the California Measure of Mental Motivation[42] and the California Critical Thinking Dispositions Inventory.[43] The Critical Thinking Toolkit is an alternative measure that examines student beliefs and attitudes about critical thinking[44]

John Dewey is one of many educational leaders who recognized that a curriculum aimed at building thinking skills would benefit the individual learner, the community, and the entire democracy.[45]

Critical thinking is significant in the learning process of internalization, in the construction of basic ideas, principles, and theories inherent in content. And critical thinking is significant in the learning process of application, whereby those ideas, principles, and theories are implemented effectively as they become relevant in learners' lives.

Each discipline adapts its use of critical thinking concepts and principles. The core concepts are always there, but they are embedded in subject-specific content. For students to learn content, intellectual engagement is crucial. All students must do their own thinking, their own construction of knowledge. Good teachers recognize this and therefore focus on the questions, readings, activities that stimulate the mind to take ownership of key concepts and principles underlying the subject.

Historically, the teaching of critical thinking focused only on logical procedures such as formal and informal logic. This emphasized to students that good thinking is equivalent to logical thinking. However, a second wave of critical thinking, urges educators to value conventional techniques, meanwhile expanding what it means to be a critical thinker. In 1994, Kerry Walters[46] compiled a conglomeration of sources surpassing this logical restriction to include many different authors' research regarding connected knowing, empathy, gender-sensitive ideals, collaboration, world views, intellectual autonomy, morality and enlightenment. These concepts invite students to incorporate their own perspectives and experiences into their thinking.

In the English and Welsh school systems, Critical Thinking is offered as a subject that 16- to 18-year-olds can take as an A-Level. Under the OCR exam board, students can sit two exam papers for the AS: "Credibility of Evidence" and "Assessing and Developing Argument". The full Advanced GCE is now available: in addition to the two AS units, candidates sit the two papers "Resolution of Dilemmas" and "Critical Reasoning". The A-level tests candidates on their ability to think critically about, and analyze, arguments on their deductive or inductive validity, as well as producing their own arguments. It also tests their ability to analyze certain related topics such as credibility and ethical decision-making. However, due to its comparative lack of subject content, many universities do not accept it as a main A-level for admissions.[47] Nevertheless, the AS is often useful in developing reasoning skills, and the full Advanced GCE is useful for degree courses in politics, philosophy, history or theology, providing the skills required for critical analysis that are useful, for example, in biblical study.

There used to also be an Advanced Extension Award offered in Critical Thinking in the UK, open to any A-level student regardless of whether they have the Critical Thinking A-level. Cambridge International Examinations have an A-level in Thinking Skills.[48]

From 2008, Assessment and Qualifications Alliance has also been offering an A-level Critical Thinking specification.[49] OCR exam board have also modified theirs for 2008. Many examinations for university entrance set by universities, on top of A-level examinations, also include a critical thinking component, such as the LNAT, the UKCAT, the BioMedical Admissions Test and the Thinking Skills Assessment.

In Qatar, critical thinking was offered by AL-Bairaq—an outreach, non-traditional educational program that targets high school students and focuses on a curriculum based on STEM fields. The idea behind AL-Bairaq is to offer high school students the opportunity to connect with the research environment in the Center for Advanced Materials (CAM) at Qatar University. Faculty members train and mentor the students and help develop and enhance their critical thinking, problem-solving, and teamwork skills.[50][failed verification]

In 1995, a meta-analysis of the literature on teaching effectiveness in higher education was undertaken.[51] The study noted concerns from higher education, politicians, and business that higher education was failing to meet society's requirements for well-educated citizens. It concluded that although faculty may aspire to develop students' thinking skills, in practice they have tended to aim at facts and concepts utilizing lowest levels of cognition, rather than developing intellect or values.

In a more recent meta-analysis, researchers reviewed 341 quasi- or true-experimental studies, all of which used some form of standardized critical thinking measure to assess the outcome variable.[52] The authors describe the various methodological approaches and attempt to categorize the differing assessment tools, which include standardized tests (and second-source measures), tests developed by teachers, tests developed by researchers, and tests developed by teachers who also serve the role as the researcher. The results emphasized the need for exposing students to real-world problems and the importance of encouraging open dialogue within a supportive environment. Effective strategies for teaching critical thinking are thought to be possible in a wide variety of educational settings.[52] One attempt to assess the humanities' role in teaching critical thinking and reducing belief in pseudoscientific claims was made at North Carolina State University. Some success was noted and the researchers emphasized the value of the humanities in providing the skills to evaluate current events and qualitative data in context.[53]

Scott Lilienfeld notes that there is some evidence to suggest that basic critical thinking skills might be successfully taught to children at a younger age than previously thought.[54]

Critical thinking is an important element of all professional fields and academic disciplines (by referencing their respective sets of permissible questions, evidence sources, criteria, etc.). Within the framework of scientific skepticism, the process of critical thinking involves the careful acquisition and interpretation of information and use of it to reach a well-justified conclusion. The concepts and principles of critical thinking can be applied to any context or case but only by reflecting upon the nature of that application. Critical thinking forms, therefore, a system of related, and overlapping, modes of thought such as anthropological thinking, sociological thinking, historical thinking, political thinking, psychological thinking, philosophical thinking, mathematical thinking, chemical thinking, biological thinking, ecological thinking, legal thinking, ethical thinking, musical thinking, thinking like a painter, sculptor, engineer, business person, etc. In other words, though critical thinking principles are universal, their application to disciplines requires a process of reflective contextualization. Psychology offerings, for example, have included courses such as Critical Thinking about the Paranormal, in which students are subjected to a series of cold readings and tested on their belief of the "psychic", who is eventually announced to be a fake.[55]

Critical thinking is considered important in the academic fields for enabling one to analyze, evaluate, explain, and restructure thinking, thereby ensuring the act of thinking without false belief. However, even with knowledge of the methods of logical inquiry and reasoning, mistakes occur, and due to a thinker's inability to apply the methodology consistently, and because of overruling character traits such as egocentrism. Critical thinking includes identification of prejudice, bias, propaganda, self-deception, distortion, misinformation, etc.[56] Given research in cognitive psychology, some educators believe that schools should focus on teaching their students critical thinking skills and cultivation of intellectual traits.[57]

Critical thinking skills can be used to help nurses during the assessment process. Through the use of critical thinking, nurses can question, evaluate, and reconstruct the nursing care process by challenging the established theory and practice. Critical thinking skills can help nurses problem solve, reflect, and make a conclusive decision about the current situation they face. Critical thinking creates "new possibilities for the development of the nursing knowledge".[58] Due to the sociocultural, environmental, and political issues that are affecting healthcare delivery, it would be helpful to embody new techniques in nursing. Nurses can also engage their critical thinking skills through the Socratic method of dialogue and reflection. This practice standard is even part of some regulatory organizations such as the College of Nurses of Ontario's Professional Standards for Continuing Competencies (2006).[59] It requires nurses to engage in Reflective Practice and keep records of this continued professional development for possible review by the College.

Critical thinking is also considered important for human rights education for toleration. The Declaration of Principles on Tolerance adopted by UNESCO in 1995 affirms that "education for tolerance could aim at countering factors that lead to fear and exclusion of others, and could help young people to develop capacities for independent judgement, critical thinking and ethical reasoning".[60]

The advent and rising popularity of online courses have prompted some to ask if computer-mediated communication (CMC) promotes, hinders, or has no effect on the amount and quality of critical thinking in a course (relative to face-to-face communication). There is some evidence to suggest a fourth, more nuanced possibility: that CMC may promote some aspects of critical thinking but hinder others. For example, Guiller et al. (2008)[61] found that, relative to face-to-face discourse, online discourse featured more justifications, while face-to-face discourse featured more instances of students expanding on what others had said. The increase in justifications may be due to the asynchronous nature of online discussions, while the increase in expanding comments may be due to the spontaneity of 'real-time' discussion. Newman et al. (1995)[62] showed similar differential effects. They found that while CMC boasted more important statements and linking of ideas, it lacked novelty. The authors suggest that this may be due to difficulties participating in a brainstorming-style activity in an asynchronous environment. Rather, the asynchrony may promote users to put forth "considered, thought out contributions".

Researchers assessing critical thinking in online discussion forums often employ a technique called Content Analysis,[62][61] where the text of online discourse (or the transcription of face-to-face discourse) is systematically coded for different kinds of statements relating to critical thinking. For example, a statement might be coded as "Discuss ambiguities to clear them up" or "Welcoming outside knowledge" as positive indicators of critical thinking. Conversely, statements reflecting poor critical thinking may be labeled as "Sticking to prejudice or assumptions" or "Squashing attempts to bring in outside knowledge". The frequency of these codes in CMC and face-to-face discourse can be compared to draw conclusions about the quality of critical thinking.

Searching for evidence of critical thinking in discourse has roots in a definition of critical thinking put forth by Kuhn (1991),[63] which emphasizes the social nature of discussion and knowledge construction. There is limited research on the role of social experience in critical thinking development, but there is some evidence to suggest it is an important factor. For example, research has shown that 3- to 4-year-old children can discern, to some extent, the differential creditability[64] and expertise[65] of individuals. Further evidence for the impact of social experience on the development of critical thinking skills comes from work that found that 6- to 7-year-olds from China have similar levels of skepticism to 10- and 11-year-olds in the United States.[66] If the development of critical thinking skills was solely due to maturation, it is unlikely we would see such dramatic differences across cultures.

Abstraction in its main sense is a conceptual process where general rules and concepts are derived from the usage and classification of specific examples, literal ("real" or "concrete") signifiers, first principles, or other methods.

"An abstraction" is the outcome of this process—a concept that acts as a common noun for all subordinate concepts, and connects any related concepts as a group, field, or category.[1]

Conceptual abstractions may be formed by filtering the information content of a concept or an observable phenomenon, selecting only the aspects which are relevant for a particular subjectively valued purpose. For example, abstracting a leather soccer ball to the more general idea of a ball selects only the information on general ball attributes and behavior, excluding, but not eliminating, the other phenomenal and cognitive characteristics of that particular ball.[1] In a type–token distinction, a type (e.g., a 'ball') is more abstract than its tokens (e.g., 'that leather soccer ball').

Abstraction in its secondary use is a material process,[2] discussed in the themes below.

Thinking in abstractions is considered by anthropologists, archaeologists, and sociologists to be one of the key traits in modern human behaviour, which is believed to have developed between 50,000 and 100,000 years ago. Its development is likely to have been closely connected with the development of human language, which (whether spoken or written) appears to both involve and facilitate abstract thinking.

Abstraction involves induction of ideas or the synthesis of particular facts into one general theory about something. It is the opposite of specification, which is the analysis or breaking-down of a general idea or abstraction into concrete facts. Abstraction can be illustrated with Francis Bacon's Novum Organum (1620), a book of modern scientific philosophy written in the late Jacobean era[3] of England to encourage modern thinkers to collect specific facts before making any generalizations.

Bacon used and promoted induction as an abstraction tool, and it countered the ancient deductive-thinking approach that had dominated the intellectual world since the times of Greek philosophers like Thales, Anaximander, and Aristotle.[4] Thales (c. 624–546 BCE) believed that everything in the universe comes from one main substance, water. He deduced or specified from a general idea, "everything is water", to the specific forms of water such as ice, snow, fog, and rivers.

Modern scientists can also use the opposite approach of abstraction, or going from particular facts collected into one general idea, such as the motion of the planets (Newton (1642–1727)). When determining that the sun is the center of our solar system (Copernicus (1473–1543)), scientists had to utilize thousands of measurements to finally conclude that Mars moves in an elliptical orbit about the sun (Kepler (1571–1630)), or to assemble multiple specific facts into the law of falling bodies (Galileo (1564–1642)).

An abstraction can be seen as a compression process,[5] mapping multiple different pieces of constituent data to a single piece of abstract data;[6] based on similarities in the constituent data, for example, many different physical cats map to the abstraction "CAT". This conceptual scheme emphasizes the inherent equality of both constituent and abstract data, thus avoiding problems arising from the distinction between "abstract" and "concrete". In this sense the process of abstraction entails the identification of similarities between objects, and the process of associating these objects with an abstraction (which is itself an object).

For example, picture 1 below illustrates the concrete relationship "Cat sits on Mat".
Chains of abstractions can be construed,[7] moving from neural impulses arising from sensory perception to basic abstractions such as color or shape, to experiential abstractions such as a specific cat, to semantic abstractions such as the "idea" of a CAT, to classes of objects such as "mammals" and even categories such as "object" as opposed to "action".

For example, graph 1 below expresses the abstraction "agent sits on location". This conceptual scheme entails no specific hierarchical taxonomy (such as the one mentioned involving cats and mammals), only a progressive exclusion of detail.
Instantiation
Non-existent things in any particular place and time are often seen as abstract. By contrast, instances, or members, of such an abstract thing might exist in many different places and times.

Those abstract things are then said to be multiply instantiated, in the sense of picture 1, picture 2, etc., shown below. It is not sufficient, however, to define abstract ideas as those that can be instantiated and to define abstraction as the movement in the opposite direction to instantiation. Doing so would make the concepts "cat" and "telephone" abstract ideas since despite their varying appearances, a particular cat or a particular telephone is an instance of the concept "cat" or the concept "telephone". Although the concepts "cat" and "telephone" are abstractions, they are not abstract in the sense of the objects in graph 1 below. We might look at other graphs, in a progression from cat to mammal to animal, and see that animal is more abstract than mammal; but on the other hand mammal is a harder idea to express, certainly in relation to marsupial or monotreme.

Perhaps confusingly, some philosophies refer to tropes (instances of properties) as abstract particulars—e.g., the particular redness of a particular apple is an abstract particular. This is similar to qualia and sumbebekos.

Still retaining the primary meaning of 'abstrere' or 'to draw away from', the abstraction of money, for example, works by drawing away from the particular value of things allowing completely incommensurate objects to be compared (see the section on 'Physicality' below). Karl Marx's writing on the commodity abstraction recognizes a parallel process.

The state (polity) as both concept and material practice exemplifies the two sides of this process of abstraction. Conceptually, 'the current concept of the state is an abstraction from the much more concrete early-modern use as the standing or status of the prince, his visible estates'. At the same time, materially, the 'practice of statehood is now constitutively and materially more abstract than at the time when princes ruled as the embodiment of extended power'.[8]

The way that physical objects, like rocks and trees, have being differs from the way that properties of abstract concepts or relations have being, for example the way the concrete, particular, individuals pictured in picture 1 exist differs from the way the concepts illustrated in graph 1 exist. That difference accounts for the ontological usefulness of the word "abstract". The word applies to properties and relations to mark the fact that, if they exist, they do not exist in space or time, but that instances of them can exist, potentially in many different places and times.

Further information: History of accounting § Ancient history
A physical object (a possible referent of a concept or word) is considered concrete (not abstract) if it is a particular individual that occupies a particular place and time. However, in the secondary sense of the term 'abstraction', this physical object can carry materially abstracting processes. For example, record-keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in containers. According to Schmandt-Besserat & (1981), these clay containers contained tokens, the total of which were the count of objects being transferred. The containers thus served as something of a bill of lading or an accounts book. In order to avoid breaking open the containers for the count, marks were placed on the outside of the containers. These physical marks, in other words, acted as material abstractions of a materially abstract process of accounting, using conceptual abstractions (numbers) to communicate its meaning.[9][10]

Abstract things are sometimes defined as those things that do not exist in reality or exist only as sensory experiences, like the color red. That definition, however, suffers from the difficulty of deciding which things are real (i.e. which things exist in reality). For example, it is difficult to agree to whether concepts like God, the number three, and goodness are real, abstract, or both.

An approach to resolving such difficulty is to use predicates as a general term for whether things are variously real, abstract, concrete, or of a particular property (e.g., good). Questions about the properties of things are then propositions about predicates, which propositions remain to be evaluated by the investigator. In the graph 1 below, the graphical relationships like the arrows joining boxes and ellipses might denote predicates.

Abstractions sometimes have ambiguous referents; for example, "happiness" (when used as an abstraction) can refer to as many things as there are people and events or states of being which make them happy. Likewise, "architecture" refers not only to the design of safe, functional buildings, but also to elements of creation and innovation which aim at elegant solutions to construction problems, to the use of space, and to the attempt to evoke an emotional response in the builders, owners, viewers and users of the building.

Abstraction uses a strategy of simplification, wherein formerly concrete details are left ambiguous, vague, or undefined; thus effective communication about things in the abstract requires an intuitive or common experience between the communicator and the communication recipient. This is true for all verbal/abstract communication.

For example, many different things can be red. Likewise, many things sit on surfaces (as in picture 1, to the right). The property of redness and the relation sitting-on are therefore abstractions of those objects. Specifically, the conceptual diagram graph 1 identifies only three boxes, two ellipses, and four arrows (and their five labels), whereas the picture 1 shows much more pictorial detail, with the scores of implied relationships as implicit in the picture rather than with the nine explicit details in the graph.

Graph 1 details some explicit relationships between the objects of the diagram. For example, the arrow between the agent and CAT:Elsie depicts an example of an is-a relationship, as does the arrow between the location and the MAT. The arrows between the gerund/present participle SITTING and the nouns agent and location express the diagram's basic relationship; "agent is SITTING on location"; Elsie is an instance of CAT.[11]

Although the description sitting-on (graph 1) is more abstract than the graphic image of a cat sitting on a mat (picture 1), the delineation of abstract things from concrete things is somewhat ambiguous; this ambiguity or vagueness is characteristic of abstraction. Thus something as simple as a newspaper might be specified to six levels, as in Douglas Hofstadter's illustration of that ambiguity, with a progression from abstract to concrete in Gödel, Escher, Bach (1979):[12]

An abstraction can thus encapsulate each of these levels of detail with no loss of generality. But perhaps a detective or philosopher/scientist/engineer might seek to learn about something, at progressively deeper levels of detail, to solve a crime or a puzzle.

In philosophical terminology, abstraction is the thought process wherein ideas[13] are distanced from objects.

Typically, abstraction is used in the arts as a synonym for abstract art in general. Strictly speaking, it refers to art unconcerned with the literal depiction of things from the visible world—it can, however, refer to an object or image which has been distilled from the real world, or indeed, another work of art.[14] Artwork that reshapes the natural world for expressive purposes is called abstract; that which derives from, but does not imitate a recognizable subject is called nonobjective abstraction. In the 20th century the trend toward abstraction coincided with advances in science, technology, and changes in urban life, eventually reflecting an interest in psychoanalytic theory.[15] Later still, abstraction was manifest in more purely formal terms, such as color, freedom from objective context, and a reduction of form to basic geometric designs.[16]

Computer scientists use abstraction to make models that can be used and re-used without having to re-write all the program code for each new application on every different type of computer. They communicate their solutions with the computer by writing source code in some particular computer language which can be translated into machine code for different types of computers to execute. Abstraction allows program designers to separate a framework (categorical concepts related to computing problems) from specific instances which implement details. This means that the program code can be written so that code doesn't have to depend on the specific details of supporting applications, operating system software, or hardware, but on a categorical concept of the solution. A solution to the problem can then be integrated into the system framework with minimal additional work. This allows programmers to take advantage of another programmer's work, while requiring only an abstract understanding of the implementation of another's work, apart from the problem that it solves.

Abstractions and levels of abstraction play an important role in the theory of general semantics originated by Alfred Korzybski. Anatol Rapoport wrote: "Abstracting is a mechanism by which an infinite variety of experiences can be mapped on short noises (words)."[17]

Francis Fukuyama defines history as "a deliberate attempt of abstraction in which we separate out important from unimportant events".[18]

Researchers in linguistics frequently apply abstraction so as to allow analysis of the phenomena of language at the desired level of detail. A commonly used abstraction, the phoneme, abstracts speech sounds in such a way as to neglect details that cannot serve to differentiate meaning. Other analogous kinds of abstractions (sometimes called "emic units") considered by linguists include morphemes, graphemes, and lexemes.

Abstraction also arises in the relation between syntax, semantics, and pragmatics. Pragmatics involves considerations that make reference to the user of the language; semantics considers expressions and what they denote (the designata) abstracted from the language user; and syntax considers only the expressions themselves, abstracted from the designata.

Abstraction in mathematics is the process of extracting the underlying structures, patterns or properties of a mathematical concept or object[19], removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena.

The advantages of abstraction in mathematics are:

It reveals deep connections between different areas of mathematics.
Known results in one area can suggest conjectures in another related area.
Techniques and methods from one area can be applied to prove results in other related area.
Patterns from one mathematical object can be generalized to other similar objects in the same class.
The main disadvantage of abstraction is that highly abstract concepts are more difficult to learn, and might require a degree of mathematical maturity and experience before they can be assimilated.

In music, the term abstraction can be used to describe improvisatory approaches to interpretation, and may sometimes indicate abandonment of tonality. Atonal music has no key signature, and is characterized by the exploration of internal numeric relationships.[20]

Further information: Intelligence, Mental rotation, and Mental operations
A recent meta-analysis suggests that the verbal system has greater engagement for abstract concepts when the perceptual system is more engaged for processing of concrete concepts. This is because abstract concepts elicit greater brain activity in the inferior frontal gyrus and middle temporal gyrus compared to concrete concepts which elicit greater activity in the posterior cingulate, precuneus, fusiform gyrus, and parahippocampal gyrus.[21] Other research into the human brain suggests that the left and right hemispheres differ in their handling of abstraction. For example, one meta-analysis reviewing human brain lesions has shown a left hemisphere bias during tool usage.[22]

Abstraction in philosophy is the process (or, to some, the alleged process) in concept formation of recognizing some set of common features in individuals, and on that basis forming a concept of that feature. The notion of abstraction is important to understanding some philosophical controversies surrounding empiricism and the problem of universals. It has also recently become popular in formal logic under predicate abstraction. Another philosophical tool for discussion of abstraction is thought space.

John Locke defined abstraction in An Essay Concerning Human Understanding:

'So words are used to stand as outward marks of our internal ideas, which are taken from particular things; but if every particular idea that we take in had its own special name, there would be no end to names. To prevent this, the mind makes particular ideas received from particular things become general; which it does by considering them as they are in the mind—mental appearances—separate from all other existences, and from the circumstances of real existence, such as time, place, and so on. This procedure is called abstraction. In it, an idea taken from a particular thing becomes a general representative of all of the same kind, and its name becomes a general name that is applicable to any existing thing that fits that abstract idea.' 2.11.9

Carl Jung's definition of abstraction broadened its scope beyond the thinking process to include exactly four mutually exclusive, different complementary psychological functions: sensation, intuition, feeling, and thinking. Together they form a structural totality of the differentiating abstraction process. Abstraction operates in one of these functions when it excludes the simultaneous influence of the other functions and other irrelevancies, such as emotion. Abstraction requires selective use of this structural split of abilities in the psyche. The opposite of abstraction is concretism. Abstraction is one of Jung's 57 definitions in Chapter XI of Psychological Types.

There is an abstract thinking, just as there is abstract feeling, sensation and intuition. Abstract thinking singles out the rational, logical qualities ... Abstract feeling does the same with ... its feeling-values. ... I put abstract feelings on the same level as abstract thoughts. ... Abstract sensation would be aesthetic as opposed to sensuous sensation and abstract intuition would be symbolic as opposed to fantastic intuition. (Jung, [1921] (1971): par. 678).

In social theory, abstraction is used as both an ideational and material process. Alfred Sohn-Rethel, asked "Can there be abstraction other than by thought?"[2] He used the example of commodity abstraction to show that abstraction occurs in practice as people create systems of abstract exchange that extend beyond the immediate physicality of the object and yet have real and immediate consequences. This work was extended through the 'Constitutive Abstraction' approach of writers associated with the Journal Arena. Two books that have taken this theme of the abstraction of social relations as an organizing process in human history are Nation Formation: Towards a Theory of Abstract Community.(1996) and the second volume of Towards a Theory of Abstract Community, published in 2006: Globalism, Nationalism, Tribalism: Bringing Theory Back In – Volume 2 of Towards a Theory of Abstract Community. These books argue that the nation is an abstract community bringing together strangers who will never meet as such; thus constituting materially real and substantial, but abstracted and mediated relations. The books suggest that contemporary processes of globalization and mediatization have contributed to materially abstracting relations between people, with major consequences for how we live our lives.

It can be easily argued that abstraction is an elementary methodological tool in several disciplines of social science. These disciplines have definite and different man concepts that highlight those aspects of man and his behaviour by idealization that are relevant for the given human science. For example, homo sociologicus is the man as sociology abstracts and idealizes it, depicting man as a social being. Moreover, we could talk about homo cyber sapiens[23] (the man who can extend his biologically determined intelligence thanks to new technologies), or homo creativus[24] (who is simply creative).

Abstraction (combined with Weberian idealization) plays a crucial role in economics. Breaking away from directly experienced reality was a common trend in 19th century sciences (especially physics), and this was the effort which was fundamentally determined the way economics tried and still tries to approach the economic aspects of social life. It is abstraction we meet in the case of both Newton's physics and the neoclassical theory, since the goal was to grasp the unchangeable and timeless essence of phenomena. For example, Newton created the concept of the material point by following the abstraction method so that he abstracted from the dimension and shape of any perceptible object, preserving only inertial and translational motion. Material point is the ultimate and common feature of all bodies. Neoclassical economists created the indefinitely abstract notion of homo economicus by following the same procedure. Economists abstract from all individual and personal qualities in order to get to those characteristics that embody the essence of economic activity. Eventually, it is the substance of the economic man that they try to grasp. Any characteristic beyond it only disturbs the functioning of this essential core.[25]

A picture of a lightbulb is associated to someone having an idea, a sign of creativity.
Creativity is a phenomenon whereby something new and somehow valuable is formed. The created item may be intangible (such as an idea, a scientific theory, a musical composition, or a joke) or a physical object (such as an invention, a printed literary work, or a painting).

Scholarly interest in creativity is found in a number of disciplines, primarily psychology, business studies, and cognitive science, but also education, technology, engineering, philosophy (particularly philosophy of science), theology, sociology, linguistics, economics, and mathematics, covering the relations between creativity and general intelligence, personality type, mental and neural processes, mental health, or artificial intelligence; the potential for fostering creativity through education and training; the fostering of creativity for national economic benefit, and the application of creative resources to improve the effectiveness of teaching and learning.

The English word creativity comes from the Latin term creare, "to create, make": its derivational suffixes also come from Latin. The word "create" appeared in English as early as the 14th century, notably in Chaucer, to indicate divine creation[1] (in The Parson's Tale[2])

However, its modern meaning as an act of human creation did not emerge until after the Enlightenment.[1]

In a summary of scientific research into creativity, Michael Mumford suggested: "Over the course of the last decade, however, we seem to have reached a general agreement that creativity involves the production of novel, useful products" (Mumford, 2003, p. 110),[3] or, in Robert Sternberg's words, the production of "something original and worthwhile".[4] Authors have diverged dramatically in their precise definitions beyond these general commonalities: Peter Meusburger reckons that over a hundred different analyses can be found in the literature.[5] As an illustration, one definition given by Dr. E. Paul Torrance described it as "a process of becoming sensitive to problems, deficiencies, gaps in knowledge, missing elements, disharmonies, and so on; identifying the difficulty; searching for solutions, making guesses, or formulating hypotheses about the deficiencies: testing and retesting these hypotheses and possibly modifying and retesting them; and finally communicating the results."[6]

Creativity in general is usually distinguished from innovation in particular, where the stress is on implementation. For example, Teresa Amabile and Pratt (2016) defines creativity as production of novel and useful ideas and innovation as implementation of creative ideas,[7] while the OECD and Eurostat state that "Innovation is more than a new idea or an invention. An innovation requires implementation, either by being put into active use or by being made available for use by other parties, firms, individuals or organisations."[8]

There is also an Emotional creativity (EC)[9] which is described as a pattern of cognitive abilities and personality traits related to originality and appropriateness in emotional experience.[10] Research has shown emotional creativity is interrelated with the real-life involvement in different types of specific creative leisure activities.[11]

Theories of creativity (particularly investigation of why some people are more creative than others) have focused on a variety of aspects. The dominant factors are usually identified as "the four Ps" — process, product, person, and place (according to Mel Rhodes).[12] A focus on process is shown in cognitive approaches that try to describe thought mechanisms and techniques for creative thinking. Theories invoking divergent rather than convergent thinking (such as Guilford), or those describing the staging of the creative process (such as Wallas) are primarily theories of creative process. A focus on creative product usually appears in attempts to measure creativity (psychometrics, see below) and in creative ideas framed as successful memes.[13] The psychometric approach to creativity reveals that it also involves the ability to produce more.[14] A focus on the nature of the creative person considers more general intellectual habits, such as openness, levels of ideation, autonomy, expertise, exploratory behavior, and so on. A focus on place considers the circumstances in which creativity flourishes, such as degrees of autonomy, access to resources, and the nature of gatekeepers. Creative lifestyles are characterized by nonconforming attitudes and behaviors as well as flexibility.[14]

Greek philosophers like Plato rejected the concept of creativity, preferring to see art as a form of discovery. Asked in The Republic, "Will we say, of a painter, that he makes something?", Plato answers, "Certainly not, he merely imitates."[15]
Most ancient cultures, including thinkers of Ancient Greece,[15] Ancient China, and Ancient India,[16] lacked the concept of creativity, seeing art as a form of discovery and not creation. The ancient Greeks had no terms corresponding to "to create" or "creator" except for the expression "poiein" ("to make"), which only applied to poiesis (poetry) and to the poietes (poet, or "maker") who made it. Plato did not believe in art as a form of creation. Asked in The Republic,[17] "Will we say, of a painter, that he makes something?", he answers, "Certainly not, he merely imitates."[15]

It is commonly argued that the notion of "creativity" originated in Western culture through Christianity, as a matter of divine inspiration.[1] According to the historian Daniel J. Boorstin, "the early Western conception of creativity was the Biblical story of creation given in the Genesis."[18] However, this is not creativity in the modern sense, which did not arise until the Renaissance. In the Judaeo-Christian tradition, creativity was the sole province of God; humans were not considered to have the ability to create something new except as an expression of God's work.[19] A concept similar to that of Christianity existed in Greek culture, for instance, Muses were seen as mediating inspiration from the Gods.[20] Romans and Greeks invoked the concept of an external creative "daemon" (Greek) or "genius" (Latin), linked to the sacred or the divine. However, none of these views are similar to the modern concept of creativity, and the individual was not seen as the cause of creation until the Renaissance.[21] It was during the Renaissance that creativity was first seen, not as a conduit for the divine, but from the abilities of "great men".[21]

The rejection of creativity in favor of discovery and the belief that individual creation was a conduit of the divine would dominate the West probably until the Renaissance and even later.[19] The development of the modern concept of creativity begins in the Renaissance, when creation began to be perceived as having originated from the abilities of the individual, and not God. This could be attributed to the leading intellectual movement of the time, aptly named humanism, which developed an intensely human-centric outlook on the world, valuing the intellect and achievement of the individual.[22] From this philosophy arose the Renaissance man (or polymath), an individual who embodies the principals of humanism in their ceaseless courtship with knowledge and creation.[23] One of the most well-known and immensely accomplished examples is Leonardo da Vinci.

However, this shift was gradual and would not become immediately apparent until the Enlightenment.[21] By the 18th century and the Age of Enlightenment, mention of creativity (notably in aesthetics), linked with the concept of imagination, became more frequent.[24] In the writing of Thomas Hobbes, imagination became a key element of human cognition;[1] William Duff was one of the first to identify imagination as a quality of genius, typifying the separation being made between talent (productive, but breaking no new ground) and genius.[20]

As a direct and independent topic of study, creativity effectively received no attention until the 19th century.[20] Runco and Albert argue that creativity as the subject of proper study began seriously to emerge in the late 19th century with the increased interest in individual differences inspired by the arrival of Darwinism. In particular, they refer to the work of Francis Galton, who through his eugenicist outlook took a keen interest in the heritability of intelligence, with creativity taken as an aspect of genius.[1]

In the late 19th and early 20th centuries, leading mathematicians and scientists such as Hermann von Helmholtz (1896) and Henri Poincaré (1908) began to reflect on and publicly discuss their creative processes.

The insights of Poincaré and von Helmholtz were built on in early accounts of the creative process by pioneering theorists such as Graham Wallas[25] and Max Wertheimer. In his work Art of Thought, published in 1926, Wallas presented one of the first models of the creative process. In the Wallas stage model, creative insights and illuminations may be explained by a process consisting of 5 stages:

(i) preparation (preparatory work on a problem that focuses the individual's mind on the problem and explores the problem's dimensions),
(ii) incubation (where the problem is internalized into the unconscious mind and nothing appears externally to be happening),
(iii) intimation (the creative person gets a "feeling" that a solution is on its way),
(iv) illumination or insight (where the creative idea bursts forth from its preconscious processing into conscious awareness);
(v) verification (where the idea is consciously verified, elaborated, and then applied).
Wallas' model is often treated as four stages, with "intimation" seen as a sub-stage.

Wallas considered creativity to be a legacy of the evolutionary process, which allowed humans to quickly adapt to rapidly changing environments. Simonton[26] provides an updated perspective on this view in his book, Origins of genius: Darwinian perspectives on creativity.

In 1927, Alfred North Whitehead gave the Gifford Lectures at the University of Edinburgh, later published as Process and Reality.[27] He is credited with having coined the term "creativity" to serve as the ultimate category of his metaphysical scheme: "Whitehead actually coined the term – our term, still the preferred currency of exchange among literature, science, and the arts. . . a term that quickly became so popular, so omnipresent, that its invention within living memory, and by Alfred North Whitehead of all people, quickly became occluded".[28]

Although psychometric studies of creativity had been conducted by The London School of Psychology as early as 1927 with the work of H. L. Hargreaves into the Faculty of Imagination,[29] the formal psychometric measurement of creativity, from the standpoint of orthodox psychological literature, is usually considered to have begun with J. P. Guilford's address to the American Psychological Association in 1950.[30] The address helped to popularize the study of creativity and to focus attention on scientific approaches to conceptualizing creativity. Statistical analyses led to the recognition of creativity (as measured) as a separate aspect of human cognition to IQ-type intelligence, into which it had previously been subsumed. Guilford's work suggested that above a threshold level of IQ, the relationship between creativity and classically measured intelligence broke down.[31]

James C. Kaufman and Beghetto introduced a "four C" model of creativity; mini-c ("transformative learning" involving "personally meaningful interpretations of experiences, actions, and insights"), little-c (everyday problem solving and creative expression), Pro-C (exhibited by people who are professionally or vocationally creative though not necessarily eminent) and Big-C (creativity considered great in the given field). This model was intended to help accommodate models and theories of creativity that stressed competence as an essential component and the historical transformation of a creative domain as the highest mark of creativity. It also, the authors argued, made a useful framework for analyzing creative processes in individuals.[32]

The contrast of terms "Big C" and "Little c" has been widely used. Kozbelt, Beghetto and Runco use a little-c/Big-C model to review major theories of creativity.[31] Margaret Boden distinguishes between h-creativity (historical) and p-creativity (personal).[33]

Robinson[34] and Anna Craft[35] have focused on creativity in a general population, particularly with respect to education. Craft makes a similar distinction between "high" and "little c" creativity.[35] and cites Ken Robinson as referring to "high" and "democratic" creativity. Mihaly Csikszentmihalyi[36] has defined creativity in terms of those individuals judged to have made significant creative, perhaps domain-changing contributions. Simonton has analysed the career trajectories of eminent creative people in order to map patterns and predictors of creative productivity.[37]

There has been much empirical study in psychology and cognitive science of the processes through which creativity occurs. Interpretation of the results of these studies has led to several possible explanations of the sources and methods of creativity.

Incubation is a temporary break from creative problem solving that can result in insight.[38] There has been some empirical research looking at whether, as the concept of "incubation" in Wallas' model implies, a period of interruption or rest from a problem may aid creative problem-solving. Ward[39] lists various hypotheses that have been advanced to explain why incubation may aid creative problem-solving, and notes how some empirical evidence is consistent with the hypothesis that incubation aids creative problem in that it enables "forgetting" of misleading clues. Absence of incubation may lead the problem solver to become fixated on inappropriate strategies of solving the problem.[40] This work disputes the earlier hypothesis that creative solutions to problems arise mysteriously from the unconscious mind while the conscious mind is occupied on other tasks.[41] This earlier hypothesis is discussed in Csikszentmihalyi's five phase model of the creative process which describes incubation as a time that your unconscious takes over. This allows for unique connections to be made without our consciousness trying to make logical order out of the problem.[42]

J. P. Guilford[43] drew a distinction between convergent and divergent production (commonly renamed convergent and divergent thinking). Convergent thinking involves aiming for a single, correct solution to a problem, whereas divergent thinking involves creative generation of multiple answers to a set problem. Divergent thinking is sometimes used as a synonym for creativity in psychology literature. Other researchers have occasionally used the terms flexible thinking or fluid intelligence, which are roughly similar to (but not synonymous with) creativity.[44]

In 1992, Finke et al. proposed the "Geneplore" model, in which creativity takes place in two phases: a generative phase, where an individual constructs mental representations called preinventive structures, and an exploratory phase where those structures are used to come up with creative ideas. Some evidence shows that when people use their imagination to develop new ideas, those ideas are heavily structured in predictable ways by the properties of existing categories and concepts.[45] Weisberg[46] argued, by contrast, that creativity only involves ordinary cognitive processes yielding extraordinary results.

Helie and Sun[47] more recently proposed a unified framework for understanding creativity in problem solving, namely the Explicit–Implicit Interaction (EII) theory of creativity. This new theory constitutes an attempt at providing a more unified explanation of relevant phenomena (in part by reinterpreting/integrating various fragmentary existing theories of incubation and insight).

The EII theory relies mainly on five basic principles, namely:

The co-existence of and the difference between explicit and implicit knowledge;
The simultaneous involvement of implicit and explicit processes in most tasks;
The redundant representation of explicit and implicit knowledge;
The integration of the results of explicit and implicit processing;
The iterative (and possibly bidirectional) processing.
A computational implementation of the theory was developed based on the CLARION cognitive architecture and used to simulate relevant human data. This work represents an initial step in the development of process-based theories of creativity encompassing incubation, insight, and various other related phenomena.

In The Act of Creation, Arthur Koestler introduced the concept of bisociation — that creativity arises as a result of the intersection of two quite different frames of reference.[48] This idea was later developed into conceptual blending. In the 1990s, various approaches in cognitive science that dealt with metaphor, analogy, and structure mapping have been converging, and a new integrative approach to the study of creativity in science, art and humor has emerged under the label conceptual blending.

Honing theory, developed principally by psychologist Liane Gabora, posits that creativity arises due to the self-organizing, self-mending nature of a worldview. The creative process is a way in which the individual hones (and re-hones) an integrated worldview. Honing theory places emphasis not only on the externally visible creative outcome but also the internal cognitive restructuring and repair of the worldview brought about by the creative process. When faced with a creatively demanding task, there is an interaction between the conception of the task and the worldview. The conception of the task changes through interaction with the worldview, and the worldview changes through interaction with the task. This interaction is reiterated until the task is complete, at which point not only is the task conceived of differently, but the worldview is subtly or drastically transformed as it follows the natural tendency of a worldview to attempt to resolve dissonance and seek internal consistency amongst its components, whether they be ideas, attitudes, or bits of knowledge.

A central feature of honing theory is the notion of a potentiality state.[49] Honing theory posits that creative thought proceeds not by searching through and randomly ‘mutating’ predefined possibilities, but by drawing upon associations that exist due to overlap in the distributed neural cell assemblies that participate in the encoding of experiences in memory. Midway through the creative process one may have made associations between the current task and previous experiences, but not yet disambiguated which aspects of those previous experiences are relevant to the current task. Thus the creative idea may feel ‘half-baked’. It is at that point that it can be said to be in a potentiality state, because how it will actualize depends on the different internally or externally generated contexts it interacts with.

Honing theory is held to explain certain phenomena not dealt with by other theories of creativity, for example, how different works by the same creator are observed in studies to exhibit a recognizable style or 'voice' even though in different creative outlets. This is not predicted by theories of creativity that emphasize chance processes or the accumulation of expertise, but it is predicted by honing theory, according to which personal style reflects the creator's uniquely structured worldview. Another example is in the environmental stimulus for creativity. Creativity is commonly considered to be fostered by a supportive, nurturing, trustworthy environment conducive to self-actualization. However, research shows that creativity is also associated with childhood adversity, which would stimulate honing.

In everyday thought, people often spontaneously imagine alternatives to reality when they think "if only...".[50] Their counterfactual thinking is viewed as an example of everyday creative processes.[51] It has been proposed that the creation of counterfactual alternatives to reality depends on similar cognitive processes to rational thought.[52]

Holm-Hadulla developed with his colleagues a dialectical theory of creativity.[53] It is an interdisciplinary theory.[54] The dialectical theory of creativity starts with the antique concept that creativity takes place in an interplay between order and chaos. Similar ideas can be found in neurosciences and psychology. Neurobiologically it can be shown that the creative process takes place in a dynamic interplay between coherence and incoherence that leads to new and usable neuronal networks. Psychology shows how the dialectics of convergent and focused thinking with divergent and associative thinking leads to new ideas and products.[55] Also creative personality traits like the ‘Big Five’ seem to be dialectically intertwined in the creative process: emotional instability vs. stability, extraversion vs. introversion, openness vs. reserve, agreeableness vs. antagonism and disinhibition vs. constraint.[56] Holm-Hadulla and colleagues exemplified the dialectics of creative processes and personality traits on behalf of extraordinary personalities like Johann W. v. Goethe,[57][58] Robert Schumann,[59] Jim Morrison,[60] Madonna Ciccone and Mick Jagger.[61] The dialectical theory of creativity applies also to counseling and psychotherapy.[62]

There was a creativity quotient developed similar to the intelligence quotient (IQ). It makes use of the results of divergent thinking tests (see below) by processing them further. It gives more weight to ideas that are radically different from other ideas in the response.[63]

J. P. Guilford's group,[43] which pioneered the modern psychometric study of creativity, constructed several tests to measure creativity in 1967:

Plot Titles, where participants are given the plot of a story and asked to write original titles.
Quick Responses is a word-association test scored for uncommonness.
Figure Concepts, where participants were given simple drawings of objects and individuals and asked to find qualities or features that are common by two or more drawings; these were scored for uncommonness.
Unusual Uses is finding unusual uses for common everyday objects such as bricks.
Remote Associations, where participants are asked to find a word between two given words (e.g. Hand _____ Call)
Remote Consequences, where participants are asked to generate a list of consequences of unexpected events (e.g. loss of gravity)
Originally, Guilford was trying to create a model for intellect as a whole, but in doing so also created a model for creativity. Guilford made an important assumption for creative research: creativity isn't one abstract concept.  The idea that creativity is a category rather than one single concept opened up the ability for other researchers to look at creativity with a whole new perspective.[64][65]

Additionally, Guilford hypothesized one of the first models for the components of creativity.  He explained that creativity was a result of having:

Sensitivity to problems, or the ability to recognize problems;
Fluency, which encompasses
a. Ideational fluency, or the ability rapidly to produce a variety of ideas that fulfill stated requirements;
b. Associational fluency, or the ability to generate a list of words, each of which is associated with a given word;                    
c. Expressional fluency, or the ability to organize words into larger units, such as phrases, sentences, and paragraphs;
Flexibility, which encompasses                      
a. Spontaneous flexibility, or the ability to demonstrate flexibility;                          
b. Adaptive flexibility, or the ability to produce responses that are novel and high in quality.
This represents the base model by which several researchers would take and alter to produce their new theories of creativity years later.[64] Building on Guilford's work, Torrance[66] developed the Torrance Tests of Creative Thinking in 1966.[67] They involved simple tests of divergent thinking and other problem-solving skills, which were scored on:

Fluency – The total number of interpretable, meaningful, and relevant ideas generated in response to the stimulus.
Originality – The statistical rarity of the responses among the test subjects.
Elaboration – The amount of detail in the responses.
Such tests, sometimes called Divergent Thinking (DT) tests have been both supported[68] and criticized.[69]

Considerable progress has been made in automated scoring of divergent thinking tests using semantic approach. When compared to human raters, NLP techniques were shown to be reliable and valid in scoring the originality.[70][71] The reported computer programs were able to achieve a correlation of 0.60 and 0.72 respectively to human graders.

Semantic networks were also used to devise originality scores that yielded significant correlations with socio-personal measures.[72] Most recently, an NSF-funded[73] team of researchers led by James C. Kaufman and Mark A. Runco[74] combined expertise in creativity research, natural language processing, computational linguistics, and statistical data analysis to devise a scalable system for computerized automated testing (SparcIt Creativity Index Testing system). This system enabled automated scoring of DT tests that is reliable, objective, and scalable, thus addressing most of the issues of DT tests that had been found and reported.[69] The resultant computer system was able to achieve a correlation of 0.73 to human graders.[75]

Some researchers have taken a social-personality approach to the measurement of creativity. In these studies, personality traits such as independence of judgement, self-confidence, attraction to complexity, aesthetic orientation, and risk-taking are used as measures of the creativity of individuals.[30] A meta-analysis by Gregory Feist showed that creative people tend to be "more open to new experiences, less conventional and less conscientious, more self-confident, self-accepting, driven, ambitious, dominant, hostile, and impulsive." Openness, conscientiousness, self-acceptance, hostility, and impulsivity had the strongest effects of the traits listed.[76] Within the framework of the Big Five model of personality, some consistent traits have emerged.[77] Openness to experience has been shown to be consistently related to a whole host of different assessments of creativity.[78] Among the other Big Five traits, research has demonstrated subtle differences between different domains of creativity. Compared to non-artists, artists tend to have higher levels of openness to experience and lower levels of conscientiousness, while scientists are more open to experience, conscientious, and higher in the confidence-dominance facets of extraversion compared to non-scientists.[76]

An alternative is using biographical methods. These methods use quantitative characteristics such as the number of publications, patents, or performances of a work. While this method was originally developed for highly creative personalities, today it is also available as self-report questionnaires supplemented with frequent, less outstanding creative behaviors such as writing a short story or creating your own recipes. For example, the Creative Achievement Questionnaire, a self-report test that measures creative achievement across 10 domains, was described in 2005 and shown to be reliable and valid when compared to other measures of creativity and to independent evaluation of creative output.[79] Besides the English original, it was also used in a Chinese,[80] French,[81] and German-speaking[82] version. It is the self-report questionnaire most frequently used in research.[80]

The potential relationship between creativity and intelligence has been of interest since the late 1900s, when a multitude of influential studies – from Getzels & Jackson,[83] Barron,[84] Wallach & Kogan,[85] and Guilford[86] – focused not only on creativity, but also on intelligence. This joint focus highlights both the theoretical and practical importance of the relationship: researchers are interested not only if the constructs are related, but also how and why.[87]

There are multiple theories accounting for their relationship, with the 3 main theories as follows:

Threshold Theory – Intelligence is a necessary, but not sufficient condition for creativity. There is a moderate positive relationship between creativity and intelligence until IQ ~120.[84][86]
Certification Theory – Creativity is not intrinsically related to intelligence. Instead, individuals are required to meet the requisite level intelligence in order to gain a certain level of education/work, which then in turn offers the opportunity to be creative. Displays of creativity are moderated by intelligence.[88]
Interference Theory – Extremely high intelligence might interfere with creative ability.[89]
Sternberg and O’Hara[90] proposed a framework of 5 possible relationships between creativity and intelligence:

Creativity is a subset of intelligence
Intelligence is a subset of creativity
Creativity and intelligence are overlapping constructs
Creativity and intelligence are part of the same construct (coincident sets)
Creativity and intelligence are distinct constructs (disjoint sets)
Creativity as a subset of intelligence
A number of researchers include creativity, either explicitly or implicitly, as a key component of intelligence.

Examples of theories that include creativity as a subset of intelligence

Sternberg's Theory of Successful intelligence[89][90][91] (see Triarchic theory of intelligence) includes creativity as a main component, and comprises 3 sub-theories: Componential (Analytic), Contextual (Practical), and Experiential (Creative). Experiential sub-theory – the ability to use pre-existing knowledge and skills to solve new and novel problems – is directly related to creativity.
The Cattell–Horn–Carroll theory includes creativity as a subset of intelligence. Specifically, it is associated with the broad group factor of long-term storage and retrieval (Glr). Glr narrow abilities relating to creativity include:[92] ideational fluency, associational fluency, and originality/creativity. Silvia et al.[93] conducted a study to look at the relationship between divergent thinking and verbal fluency tests, and reported that both fluency and originality in divergent thinking were significantly affected by the broad level Glr factor. Martindale[94] extended the CHC-theory in the sense that it was proposed that those individuals who are creative are also selective in their processing speed Martindale argues that in the creative process, larger amounts of information are processed more slowly in the early stages, and as the individual begins to understand the problem, the processing speed is increased.
The Dual Process Theory of Intelligence[95] posits a two-factor/type model of intelligence. Type 1 is a conscious process, and concerns goal directed thoughts, which are explained by g. Type 2 is an unconscious process, and concerns spontaneous cognition, which encompasses daydreaming and implicit learning ability. Kaufman argues that creativity occurs as a result of Type 1 and Type 2 processes working together in combination. The use of each type in the creative process can be used to varying degrees.
Intelligence as a subset of creativity
In this relationship model, intelligence is a key component in the development of creativity.

Sternberg & Lubart's Investment Theory.[96][97] Using the metaphor of a stock market, they demonstrate that creative thinkers are like good investors – they buy low and sell high (in their ideas). Like under/low-valued stock, creative individuals generate unique ideas that are initially rejected by other people. The creative individual has to persevere, and convince the others of the ideas value. After convincing the others, and thus increasing the ideas value, the creative individual ‘sells high’ by leaving the idea with the other people, and moves onto generating another idea. According to this theory, six distinct, but related elements contribute to successful creativity: intelligence, knowledge, thinking styles, personality, motivation, and environment. Intelligence is just one of the six factors that can either solely, or in conjunction with the other five factors, generate creative thoughts.
Amabile's Componential Model of Creativity.[98][99] In this model, there are 3 within-individual components needed for creativity – domain-relevant skills, creativity-relevant processes, and task motivation – and 1 component external to the individual: their surrounding social environment. Creativity requires a confluence of all components. High creativity will result when an individual is: intrinsically motivated, possesses both a high level of domain-relevant skills and has high skills in creative thinking, and is working in a highly creative environment.
Amusement Park Theoretical Model.[100] In this 4-step theory, both domain-specific and generalist views are integrated into a model of creativity. The researchers make use of the metaphor of the amusement park to demonstrate that within each of these creative levels, intelligence plays a key role:
To get into the amusement park, there are initial requirements (e.g., time/transport to go to the park). Initial requirements (like intelligence) are necessary, but not sufficient for creativity. They are more like prerequisites for creativity, and if an individual does not possess the basic level of the initial requirement (intelligence), then they will not be able to generate creative thoughts/behaviour.
Secondly are the subcomponents – general thematic areas – that increase in specificity. Like choosing which type of amusement park to visit (e.g. a zoo or a water park), these areas relate to the areas in which someone could be creative (e.g. poetry).
Thirdly, there are specific domains. After choosing the type of park to visit e.g. waterpark, you then have to choose which specific park to go to. Within the poetry domain, there are many different types (e.g. free verse, riddles, sonnet, etc.) that have to be selected from.
Lastly, there are micro-domains. These are the specific tasks that reside within each domain e.g. individual lines in a free verse poem / individual rides at the waterpark.
Creativity and intelligence as overlapping yet distinct constructs
This possible relationship concerns creativity and intelligence as distinct, but intersecting constructs.

Renzulli's Three-Ring Conception of Giftedness.[101] In this conceptualisation, giftedness occurs as a result from the overlap of above average intellectual ability, creativity, and task commitment. Under this view, creativity and intelligence are distinct constructs, but they do overlap under the correct conditions.
PASS theory of intelligence. In this theory, the planning component – relating to the ability to solve problems, make decisions and take action – strongly overlaps with the concept of creativity.[102]
Threshold Theory (TT). A number of previous research findings have suggested that a threshold exists in the relationship between creativity and intelligence – both constructs are moderately positively correlated up to an IQ of ~120. Above this threshold of an IQ of 120, if there is a relationship at all, it is small and weak.[83][84][103] TT posits that a moderate level of intelligence is necessary for creativity.
In support of the TT, Barron[84][104] reported finding a non-significant correlation between creativity and intelligence in a gifted sample; and a significant correlation in a non-gifted sample. Yamamoto[105] in a sample of secondary school children, reported a significant correlation between creativity and intelligence of r = .3, and reported no significant correlation when the sample consisted of gifted children. Fuchs-Beauchamp et al.[106] in a sample of preschoolers found that creativity and intelligence correlated from r = .19 to r = .49 in the group of children who had an IQ below the threshold; and in the group above the threshold, the correlations were r = <.12. Cho et al.[107] reported a correlation of .40 between creativity and intelligence in the average IQ group of a sample of adolescents and adults; and a correlation of close to r = .0 for the high IQ group. Jauk et al.[108] found support for the TT, but only for measures of creative potential; not creative performance.

Much modern day research reports findings against TT. Wai et al.[109] in a study using data from the longitudinal Study of Mathematically Precocious Youth – a cohort of elite students from early adolescence into adulthood – found that differences in SAT scores at age 13 were predictive of creative real-life outcomes 20 years later. Kim's[110] meta-analysis of 21 studies did not find any supporting evidence for TT, and instead negligible correlations were reported between intelligence, creativity, and divergent thinking both below and above IQ's of 120. Preckel et al.,[111] investigating fluid intelligence and creativity, reported small correlations of r = .3 to r = .4 across all levels of cognitive ability.

Under this view, researchers posit that there are no differences in the mechanisms underlying creativity in those used in normal problem solving; and in normal problem solving, there is no need for creativity. Thus, creativity and Intelligence (problem solving) are the same thing. Perkins[112] referred to this as the ‘nothing-special’ view.

Weisberg & Alba[113] examined problem solving by having participants complete the 9-dot problem (see Thinking outside the box#Nine dots puzzle) – where the participants are asked to connect all 9 dots in the 3 rows of 3 dots using 4 straight lines or less, without lifting their pen or tracing the same line twice. The problem can only be solved if the lines go outside the boundaries of the square of dots. Results demonstrated that even when participants were given this insight, they still found it difficult to solve the problem, thus showing that to successfully complete the task it is not just insight (or creativity) that is required.

In this view, creativity and intelligence are completely different, unrelated constructs.

Getzels and Jackson[83] administered 5 creativity measures to a group of 449 children from grades 6-12, and compared these test findings to results from previously administered (by the school) IQ tests. They found that the correlation between the creativity measures and IQ was r = .26. The high creativity group scored in the top 20% of the overall creativity measures, but were not included in the top 20% of IQ scorers. The high intelligence group scored the opposite: they scored in the top 20% for IQ, but were outside the top 20% scorers for creativity, thus showing that creativity and intelligence are distinct and unrelated.

However, this work has been heavily criticised. Wallach and Kogan[85] highlighted that the creativity measures were not only weakly related to one another (to the extent that they were no more related to one another than they were with IQ), but they seemed to also draw upon non-creative skills. McNemar[114] noted that there were major measurement issues, in that the IQ scores were a mixture from 3 different IQ tests.

Wallach and Kogan[85] administered 5 measures of creativity, each of which resulted in a score for originality and fluency; and 10 measures of general intelligence to 151 5th grade children. These tests were untimed, and given in a game-like manner (aiming to facilitate creativity). Inter-correlations between creativity tests were on average r = .41. Inter-correlations between intelligence measures were on average r = .51 with each other. Creativity tests and intelligence measures correlated r = .09.


distributed functional brain network associated with divergent thinking
The neuroscience of creativity looks at the operation of the brain during creative behaviour. It has been addressed[115] in the article "Creative Innovation: Possible Brain Mechanisms." The authors write that "creative innovation might require coactivation and communication between regions of the brain that ordinarily are not strongly connected." Highly creative people who excel at creative innovation tend to differ from others in three ways:

they have a high level of specialized knowledge,
they are capable of divergent thinking mediated by the frontal lobe.
and they are able to modulate neurotransmitters such as norepinephrine in their frontal lobe.
Thus, the frontal lobe appears to be the part of the cortex that is most important for creativity.

This article also explored the links between creativity and sleep, mood and addiction disorders, and depression.

In 2005, Alice Flaherty presented a three-factor model of the creative drive. Drawing from evidence in brain imaging, drug studies and lesion analysis, she described the creative drive as resulting from an interaction of the frontal lobes, the temporal lobes, and dopamine from the limbic system. The frontal lobes can be seen as responsible for idea generation, and the temporal lobes for idea editing and evaluation. Abnormalities in the frontal lobe (such as depression or anxiety) generally decrease creativity, while abnormalities in the temporal lobe often increase creativity. High activity in the temporal lobe typically inhibits activity in the frontal lobe, and vice versa. High dopamine levels increase general arousal and goal directed behaviors and reduce latent inhibition, and all three effects increase the drive to generate ideas.[116] A 2015 study on creativity found that it involves the interaction of multiple neural networks, including those that support associative thinking, along with other default mode network functions.[117]

Vandervert[118] described how the brain's frontal lobes and the cognitive functions of the cerebellum collaborate to produce creativity and innovation. Vandervert's explanation rests on considerable evidence that all processes of working memory (responsible for processing all thought[119]) are adaptively modeled for increased efficiency by the cerebellum.[120] The cerebellum (consisting of 100 billion neurons, which is more than the entirety of the rest of the brain[121]) is also widely known to adaptively model all bodily movement for efficiency. The cerebellum's adaptive models of working memory processing are then fed back to especially frontal lobe working memory control processes[122] where creative and innovative thoughts arise.[123] (Apparently, creative insight or the "aha" experience is then triggered in the temporal lobe.[124])

According to Vandervert, the details of creative adaptation begin in "forward" cerebellar models which are anticipatory/exploratory controls for movement and thought. These cerebellar processing and control architectures have been termed Hierarchical Modular Selection and Identification for Control (HMOSAIC).[125] New, hierarchically arranged levels of the cerebellar control architecture (HMOSAIC) develop as mental mulling in working memory is extended over time. These new levels of the control architecture are fed forward to the frontal lobes. Since the cerebellum adaptively models all movement and all levels of thought and emotion,[126] Vandervert's approach helps explain creativity and innovation in sports, art, music, the design of video games, technology, mathematics, the child prodigy, and thought in general.

Essentially, Vandervert has argued that when a person is confronted with a challenging new situation, visual-spatial working memory and speech-related working memory are decomposed and re-composed (fractionated) by the cerebellum and then blended in the cerebral cortex in an attempt to deal with the new situation. With repeated attempts to deal with challenging situations, the cerebro-cerebellar blending process continues to optimize the efficiency of how working memory deals with the situation or problem.[127] Most recently, he has argued that this is the same process (only involving visual-spatial working memory and pre-language vocalization) that led to the evolution of language in humans.[128] Vandervert and Vandervert-Weathers have pointed out that this blending process, because it continuously optimizes efficiencies, constantly improves prototyping attempts toward the invention or innovation of new ideas, music, art, or technology.[129] Prototyping, they argue, not only produces new products, it trains the cerebro-cerebellar pathways involved to become more efficient at prototyping itself. Further, Vandervert and Vandervert-Weathers believe that this repetitive "mental prototyping" or mental rehearsal involving the cerebellum and the cerebral cortex explains the success of the self-driven, individualized patterning of repetitions initiated by the teaching methods of the Khan Academy. The model proposed by Vandervert has, however, received incisive critique from several authors.[130][131]

Creativity involves the forming of associative elements into new combinations that are useful or meet some requirement. Sleep aids this process.[132] REM rather than NREM sleep appears to be responsible.[133][134] This has been suggested to be due to changes in cholinergic and noradrenergic neuromodulation that occurs during REM sleep.[133] During this period of sleep, high levels of acetylcholine in the hippocampus suppress feedback from the hippocampus to the neocortex, and lower levels of acetylcholine and norepinephrine in the neocortex encourage the spread of associational activity within neocortical areas without control from the hippocampus.[135] This is in contrast to waking consciousness, where higher levels of norepinephrine and acetylcholine inhibit recurrent connections in the neocortex. It is proposed that REM sleep adds creativity by allowing "neocortical structures to reorganize associative hierarchies, in which information from the hippocampus would be reinterpreted in relation to previous semantic representations or nodes."[133]

Some theories suggest that creativity may be particularly susceptible to affective influence. As noted in voting behavior, the term "affect" in this context can refer to liking or disliking key aspects of the subject in question. This work largely follows from findings in psychology regarding the ways in which affective states are involved in human judgment and decision-making.[136]

According to Alice Isen, positive affect has three primary effects on cognitive activity:

Positive affect makes additional cognitive material available for processing, increasing the number of cognitive elements available for association;
Positive affect leads to defocused attention and a more complex cognitive context, increasing the breadth of those elements that are treated as relevant to the problem;
Positive affect increases cognitive flexibility, increasing the probability that diverse cognitive elements will in fact become associated. Together, these processes lead positive affect to have a positive influence on creativity.
Barbara Fredrickson in her broaden-and-build model suggests that positive emotions such as joy and love broaden a person's available repertoire of cognitions and actions, thus enhancing creativity.

According to these researchers, positive emotions increase the number of cognitive elements available for association (attention scope) and the number of elements that are relevant to the problem (cognitive scope).

Various meta-analyses, such as Baas et al. (2008) of 66 studies about creativity and affect support the link between creativity and positive affect.[137][138] Research has shown that practicing creative leisure activities is interrelated with the emotional creativity.[139]

Jürgen Schmidhuber's formal theory of creativity[140][141] postulates that creativity, curiosity, and interestingness are by-products of a simple computational principle for measuring and optimizing learning progress. Consider an agent able to manipulate its environment and thus its own sensory inputs. The agent can use a black box optimization method such as reinforcement learning to learn (through informed trial and error) sequences of actions that maximize the expected sum of its future reward signals. There are extrinsic reward signals for achieving externally given goals, such as finding food when hungry. But Schmidhuber's objective function to be maximized also includes an additional, intrinsic term to model "wow-effects." This non-standard term motivates purely creative behavior of the agent even when there are no external goals. A wow-effect is formally defined as follows. As the agent is creating and predicting and encoding the continually growing history of actions and sensory inputs, it keeps improving the predictor or encoder, which can be implemented as an artificial neural network or some other machine learning device that can exploit regularities in the data to improve its performance over time. The improvements can be measured precisely, by computing the difference in computational costs (storage size, number of required synapses, errors, time) needed to encode new observations before and after learning. This difference depends on the encoder's present subjective knowledge, which changes over time, but the theory formally takes this into account. The cost difference measures the strength of the present "wow-effect" due to sudden improvements in data compression or computational speed. It becomes an intrinsic reward signal for the action selector. The objective function thus motivates the action optimizer to create action sequences causing more wow-effects. Irregular, random data (or noise) do not permit any wow-effects or learning progress, and thus are "boring" by nature (providing no reward). Already known and predictable regularities also are boring. Temporarily interesting are only the initially unknown, novel, regular patterns in both actions and observations. This motivates the agent to perform continual, open-ended, active, creative exploration. Schmidhuber's work is highly influential in intrinsic motivation which has emerged as a research topic in its own right as part of the study of artificial intelligence and robotics.

According to Schmidhuber, his objective function explains the activities of scientists, artists, and comedians.[142][143] For example, physicists are motivated to create experiments leading to observations obeying previously unpublished physical laws permitting better data compression. Likewise, composers receive intrinsic reward for creating non-arbitrary melodies with unexpected but regular harmonies that permit wow-effects through data compression improvements. Similarly, a comedian gets intrinsic reward for "inventing a novel joke with an unexpected punch line, related to the beginning of the story in an initially unexpected but quickly learnable way that also allows for better compression of the perceived data."[144] Schmidhuber argues that ongoing computer hardware advances will greatly scale up rudimentary artificial scientists and artists[clarification needed] based on simple implementations of the basic principle since 1990.[145] He used the theory to create low-complexity art[146] and an attractive human face.[147]

A study by psychologist J. Philippe Rushton found creativity to correlate with intelligence and psychoticism.[148] Another study found creativity to be greater in schizotypal than in either normal or schizophrenic individuals. While divergent thinking was associated with bilateral activation of the prefrontal cortex, schizotypal individuals were found to have much greater activation of their right prefrontal cortex.[149] This study hypothesizes that such individuals are better at accessing both hemispheres, allowing them to make novel associations at a faster rate. In agreement with this hypothesis, ambidexterity is also associated with schizotypal and schizophrenic individuals. Three recent studies by Mark Batey and Adrian Furnham have demonstrated the relationships between schizotypal[150][151] and hypomanic personality[152] and several different measures of creativity.

Particularly strong links have been identified between creativity and mood disorders, particularly manic-depressive disorder (a.k.a. bipolar disorder) and depressive disorder (a.k.a. unipolar disorder). In Touched with Fire: Manic-Depressive Illness and the Artistic Temperament, Kay Redfield Jamison summarizes studies of mood-disorder rates in writers, poets, and artists. She also explores research that identifies mood disorders in such famous writers and artists as Ernest Hemingway (who shot himself after electroconvulsive treatment), Virginia Woolf (who drowned herself when she felt a depressive episode coming on), composer Robert Schumann (who died in a mental institution), and even the famed visual artist Michelangelo.

A study looking at 300,000 persons with schizophrenia, bipolar disorder, or unipolar depression, and their relatives, found overrepresentation in creative professions for those with bipolar disorder as well as for undiagnosed siblings of those with schizophrenia or bipolar disorder. There was no overall overrepresentation, but overrepresentation for artistic occupations, among those diagnosed with schizophrenia. There was no association for those with unipolar depression or their relatives.[153]

Another study involving more than one million people, conducted by Swedish researchers at the Karolinska Institute, reported a number of correlations between creative occupations and mental illnesses. Writers had a higher risk of anxiety and bipolar disorders, schizophrenia, unipolar depression, and substance abuse, and were almost twice as likely as the general population to kill themselves. Dancers and photographers were also more likely to have bipolar disorder.[154]

As a group, those in the creative professions were no more likely to have psychiatric disorders than other people, although they were more likely to have a close relative with a disorder, including anorexia and, to some extent, autism, the Journal of Psychiatric Research reports.[154]

According to psychologist Robert Epstein, PhD, creativity can be obstructed through stress.[155]

Conversely, research has shown that creative activities such as art therapy, poetry writing, journaling, and reminiscence can promote mental well-being.[156]

Creativity can be expressed in a number of different forms, depending on unique people and environments. A number of different theorists have suggested models of the creative person. One model suggests that there are four "Creativity Profiles" that can help produce growth, innovation, speed, etc.[157]

(i) Incubate (Long-term Development)
(ii) Imagine (Breakthrough Ideas)
(iii) Improve (Incremental Adjustments)
(iv) Invest (Short-term Goals)
Research by Dr Mark Batey of the Psychometrics at Work Research Group at Manchester Business School has suggested that the creative profile can be explained by four primary creativity traits with narrow facets within each

(i) "Idea Generation" (Fluency, Originality, Incubation and Illumination)
(ii) "Personality" (Curiosity and Tolerance for Ambiguity)
(iii) "Motivation" (Intrinsic, Extrinsic and Achievement)
(iv) "Confidence" (Producing, Sharing and Implementing)
This model was developed in a sample of 1000 working adults using the statistical techniques of Exploratory Factor Analysis followed by Confirmatory Factor Analysis by Structural Equation Modelling.[158]

An important aspect of the creativity profiling approach is to account for the tension between predicting the creative profile of an individual, as characterised by the psychometric approach, and the evidence that team creativity is founded on diversity and difference.[159]

One characteristic of creative people, as measured by some psychologists, is what is called divergent production. Divergent production is the ability of a person to generate a diverse assortment, yet an appropriate amount of responses to a given situation.[160] One way of measuring divergent production is by administering the Torrance Tests of Creative Thinking.[161] The Torrance Tests of Creative Thinking assesses the diversity, quantity, and appropriateness of participants responses to a variety of open-ended questions.

Other researchers of creativity see the difference in creative people as a cognitive process of dedication to problem solving and developing expertise in the field of their creative expression. Hard working people study the work of people before them and within their current area, become experts in their fields, and then have the ability to add to and build upon previous information in innovative and creative ways. In a study of projects by design students, students who had more knowledge on their subject on average had greater creativity within their projects.[162]

The aspect of motivation within a person's personality may predict creativity levels in the person. Motivation stems from two different sources, intrinsic and extrinsic motivation. Intrinsic motivation is an internal drive within a person to participate or invest as a result of personal interest, desires, hopes, goals, etc. Extrinsic motivation is a drive from outside of a person and might take the form of payment, rewards, fame, approval from others, etc. Although extrinsic motivation and intrinsic motivation can both increase creativity in certain cases, strictly extrinsic motivation often impedes creativity in people.[163]

From a personality-traits perspective, there are a number of traits that are associated with creativity in people.[164] Creative people tend to be more open to new experiences, are more self-confident, are more ambitious, self-accepting, impulsive, driven, dominant, and hostile, compared to people with less creativity.

From an evolutionary perspective, creativity may be a result of the outcome of years of generating ideas. As ideas are continuously generated, the need to evolve produces a need for new ideas and developments. As a result, people have been creating and developing new, innovative, and creative ideas to build our progress as a society.[165]

In studying exceptionally creative people in history, some common traits in lifestyle and environment are often found. Creative people in history usually had supportive parents, but rigid and non-nurturing. Most had an interest in their field at an early age, and most had a highly supportive and skilled mentor in their field of interest. Often the field they chose was relatively uncharted, allowing for their creativity to be expressed more in a field with less previous information. Most exceptionally creative people devoted almost all of their time and energy into their craft, and after about a decade had a creative breakthrough of fame. Their lives were marked with extreme dedication and a cycle of hard-work and breakthroughs as a result of their determination.[166]

Another theory of creative people is the investment theory of creativity. This approach suggest that there are many individual and environmental factors that must exist in precise ways for extremely high levels of creativity opposed to average levels of creativity. In the investment sense, a person with their particular characteristics in their particular environment may see an opportunity to devote their time and energy into something that has been overlooked by others. The creative person develops an undervalued or under-recognised idea to the point that it is established as a new and creative idea. Just like in the financial world, some investments are worth the buy in, while others are less productive and do not build to the extent that the investor expected. This investment theory of creativity views creativity in a unique perspective compared to others, by asserting that creativity might rely to some extent on the right investment of effort being added to a field at the right time in the right way.[167]

So called malevolent creativity is associated with the "dark side" of creativity.[168] This type of creativity is not typically accepted within society and is defined by the intention to cause harm to others through original and innovative means. Malevolent creativity should be distinguished from negative creativity in that negative creativity may unintentionally cause harm to others, whereas malevolent creativity is explicitly malevolently motivated. While it is often associated with criminal behaviour, it can also be observed in ordinary day-to-day life as lying, cheating and betrayal.[169]

Malevolent creativity is often a key contributor to crime and in its most destructive form can even manifest as terrorism. As creativity requires deviating from the conventional, there is a permanent tension between being creative and producing products that go too far and in some cases to the point of breaking the law. Aggression is a key predictor of malevolent creativity, and studies have also shown that increased levels of aggression also correlates to a higher likelihood of committing crime.[170]

Although everyone shows some levels of malevolent creativity under certain conditions, those that have a higher propensity towards it have increased tendencies to deceive and manipulate others to their own gain. While malevolent creativity appears to dramatically increase when an individual is placed under unfair conditions, personality, particularly aggressiveness, is also a key predictor in anticipating levels of malevolent thinking. Researchers Harris and Reiter-Palmon investigated the role of aggression in levels of malevolent creativity, in particular levels of implicit aggression and the tendency to employ aggressive actions in response to problem solving. The personality traits of physical aggression, conscientiousness, emotional intelligence and implicit aggression all seem to be related with malevolent creativity.[168] Harris and Reiter-Palmon's research showed that when subjects were presented with a problem that triggered malevolent creativity, participants high in implicit aggression and low in premeditation expressed the largest number of malevolently-themed solutions. When presented with the more benign problem that triggered prosocial motives of helping others and cooperating, those high in implicit aggression, even if they were high in impulsiveness, were far less destructive in their imagined solutions. They concluded premeditation, more than implicit aggression controlled an individual's expression of malevolent creativity.[171]

Creativity is viewed differently in different countries.[172] For example, cross-cultural research centred on Hong Kong found that Westerners view creativity more in terms of the individual attributes of a creative person, such as their aesthetic taste, while Chinese people view creativity more in terms of the social influence of creative people e.g. what they can contribute to society.[173] Mpofu et al. surveyed 28 African languages and found that 27 had no word which directly translated to 'creativity' (the exception being Arabic).[174] The principle of linguistic relativity, i.e. that language can affect thought, suggests that the lack of an equivalent word for 'creativity' may affect the views of creativity among speakers of such languages. However, more research would be needed to establish this, and there is certainly no suggestion that this linguistic difference makes people any less (or more) creative; Africa has a rich heritage of creative pursuits such as music, art, and storytelling. Nevertheless, it is true that there has been very little research on creativity in Africa,[175] and there has also been very little research on creativity in Latin America.[176] Creativity has been more thoroughly researched in the northern hemisphere, but here again there are cultural differences, even between countries or groups of countries in close proximity. For example, in Scandinavian countries, creativity is seen as an individual attitude which helps in coping with life's challenges,[177] while in Germany, creativity is seen more as a process that can be applied to help solve problems.[178]

Training meeting in an eco-design stainless steel company in Brazil. The leaders among other things wish to cheer and encourage the workers in order to achieve a higher level of creativity.
It has been the topic of various research studies to establish that organizational effectiveness depends on the creativity of the workforce to a large extent. For any given organization, measures of effectiveness vary, depending upon its mission, environmental context, nature of work, the product or service it produces, and customer demands. Thus, the first step in evaluating organizational effectiveness is to understand the organization itself — how it functions, how it is structured, and what it emphasizes.

Amabile[179] argued that to enhance creativity in business, three components were needed:

Expertise (technical, procedural and intellectual knowledge),
Creative thinking skills (how flexibly and imaginatively people approach problems),
and Motivation (especially intrinsic motivation).
There are two types of motivation:

extrinsic motivation – external factors, for example threats of being fired or money as a reward,
intrinsic motivation – comes from inside an individual, satisfaction, enjoyment of work, etc.
Six managerial practices to encourage motivation are:

Challenge – matching people with the right assignments;
Freedom – giving people autonomy choosing means to achieve goals;
Resources – such as time, money, space, etc. There must be balance fit among resources and people;
Work group features – diverse, supportive teams, where members share the excitement, willingness to help, and recognize each other's talents;
Supervisory encouragement – recognitions, cheering, praising;
Organizational support – value emphasis, information sharing, collaboration.
Nonaka, who examined several successful Japanese companies, similarly saw creativity and knowledge creation as being important to the success of organizations.[180] In particular, he emphasized the role that tacit knowledge has to play in the creative process.

In business, originality is not enough. The idea must also be appropriate—useful and actionable.[181][182] Creative competitive intelligence is a new solution to solve this problem. According to Reijo Siltala it links creativity to innovation process and competitive intelligence to creative workers.

Creativity can be encouraged in people and professionals and in the workplace. It is essential for innovation, and is a factor affecting economic growth and businesses. In 2013, the sociologist Silvia Leal Martín, using the Innova 3DX method, suggested measuring the various parameters that encourage creativity and innovation: corporate culture, work environment, leadership and management, creativity, self-esteem and optimism, locus of control and learning orientation, motivation, and fear.[183]

Similarly, social psychologists, organizational scientists, and management scientists who conduct extensive research on the factors that influence creativity and innovation in teams and organizations have developed integrative theoretical models that emphasize the roles of team composition, team processes, and organizational culture, as well as the mutually reinforcing relationships between them in promoting innovation.[184][185][186][187]

The investigation by Loo (2017) [188] on creative working in the knowledge economy brings together studies of creativity as delineated in this web page. It offers connections with the sections on the ‘”Four C” model’, ‘Theories of creative processes’, ‘Creativity as a subset of intelligence’, ‘Creativity and personality’, and ‘In organisations’ It is the last section that the investigation addresses.

Research studies of the knowledge economy may be classified into three levels: macro, meso and micro. Macro studies refer to investigations at a societal or transnational dimension. Meso studies focus on organisations. Micro investigations centre on the minutiae workings of workers. There is also an interdisciplinary dimension such as research from businesses (e.g. Burton-Jones, 1999; Drucker, 1999), economics (e.g. Cortada, 1998; Reich, 2001; Florida, 2003), education (e.g. Farrell and Fenwick, 2007; Brown, Lauder and Ashton, 2011), human resource management (e.g. Davenport, 2005), knowledge and organizational management (Alvesson, 2004; Defillippi, Arthur and Lindsay, 2006; Orr, Nutley, Russell, Bain, Hacking and Moran, 2016), sociology, psychology, and knowledge economy-related sectors – especially information technology (IT) software (e.g. O’Riain, 2004; Nerland, 2008) and advertising (e.g. Grabher, 2004; Lury, 2004) (Loo, 2017).

Loo (2017) studies how individual workers in the knowledge economy use their creativity and know-how in the advertising and IT software sectors. It examines this phenomenon across three developed countries of England, Japan and Singapore to observe global perspectives. Specifically, the study uses qualitative data from semi-structured interviews of the related professionals in the roles of creative directing and copywriting (in advertising), and systems software developing and software programme managing.

The study offers a conceptual framework (Loo, 2017, p. 49) of a two-dimensional matrix of individual and collaborative working styles, and single and multi-contexts. The investigation draws on literature sources from the four disciplines of economics (e.g. Reich, 2001; Quah, 2002), management (e.g. ,Drucker, 1994; Nonaka and Takeuchi, 1995; von Hippel, 2006), sociology (e.g. Zuboff, 1988; Bell, 1973; Lash and Urry, 1994; Castells, 2000; Knorr Cetina, 2005), and psychology (e.g. Gardner, 1984; Csikszentmihalyi, 1988; Sternberg, Kaufman and Pretz, 2004). The themes arising from the analysis of knowledge work and creativity literature serve to create a distinct theoretical framework of creative knowledge work. These workers apply their cognitive abilities, creative personalities and skill sets in the areas of science, technology, or culture industries to invent or discover new possibilities – e.g. a medium, product or service. These work activities may be done individually or collectively. Education, training and ‘encultured environments’ are necessary for the performance of these creative activities. Acts of creativity are viewed as asking new questions over and above those questions asked by an intelligent person, seeking novelty when reviewing a situation (Gardner, 1993), and creating something that is different and novel, i.e. a ‘variation’ on the idea of existing ideas in a domain (Csikszentmihalyi, 1988). This framework is evidenced by the empirical chapters on the micro-workings of creative workers in the two knowledge economy sectors from global perspectives.

This investigation identifies a definition of creative work, three types of work and the necessary conditions for it to occur. These workers use a combination of creative applications including anticipatory imagination, problem-solving, problem seeking, and generating ideas and aesthetic sensibilities. Taking aesthetic sensibilities as an example, for a creative director in the advertising industry, it is a visual imagery whether still or moving via a camera lens, and for a software programmer, it is the innovative technical expertise in which the software is written. There are specific creative applications for each of the sectors such as emotional connection in the advertising sector, and the power of expression and sensitivity in the IT software sector. In addition to the creative applications, creative workers require abilities and aptitudes to carry out their roles. Passion for one's job is generic. For copywriters, this passion is identified with fun, enjoyment and happiness alongside attributes such as honesty (regarding the product), confidence, and patience in finding the appropriate copy. Knowledge is also required in the disciplines of the humanities (e.g. literature), the creative arts (e.g. painting and music) and technical-related know-how (e.g. mathematics, computer sciences and physical sciences). In the IT software, technical knowledge of computer languages (e.g. C++) is especially significant for programmers whereas the degree of technical expertise may be less for a programme manager, as only knowledge of the relevant language is necessary to understand the issues for communicating with the team of developers and testers.

There are three types of work. One is intra-sectoral (e.g. ‘general sponge’ and ’in tune with the zeitgeist’ [advertising], and ‘power of expression’ and ‘sensitivity’ [IT software]). The second is inter-sectoral (e.g. ‘integration of advertising activities’ [advertising], and ‘autonomous decentralized systems’ [ADS] [IT software]). The third relates to changes in culture/practices in the sectors (e.g. ‘three-dimensional trust’ and ‘green credentials’ [advertising], and ‘collaboration with HEIs and industry’ and ‘ADS system in the Tokyo train operator’ [IT software]).

The necessary conditions for creative work to exist are a supportive environment such as supportive information, communications and electronic technologies (ICET) infrastructure, training, work environment and education.

This investigation has implications for lifelong learning of these workers informally and formally. Teaching institutions need to offer multi-disciplinary knowledge of humanities, arts and sciences and it has impacts on the programme structure, delivery approaches and assessments. At a macro level, governments need to offer a rich diet of cultural activities, outdoor activities and sports fixtures that inform potential creative workers in the areas of video gaming and advertising. This study has implications for work organisations that support and encourage collaborative working alongside individual working, offer opportunities to engage in continuous professional development (formally and informally), and foster an environment, which promotes experiential functioning and supports experimentation.

Diversity between team members’ backgrounds and knowledge can increase team creativity by expanding the total collection of unique information that is available to the team and introducing different perspectives that can integrate in novel ways. However, under some conditions, diversity can also decrease team creativity by making it more difficult for team members to communicate about ideas and causing interpersonal conflicts between those with different perspectives.[189] Thus, the potential advantages of diversity must be supported by appropriate team processes and organizational cultures in order to enhance creativity.[184][185][186][187][190][191]

Team communication norms, such as respecting others’ expertise, paying attention to others’ ideas, expecting information sharing, tolerating disagreements, negotiating, remaining open to others’ ideas, learning from others, and building on each other's ideas, increase team creativity by facilitating the social processes involved with brainstorming and problem solving. Through these processes, team members are able to access their collective pool of knowledge, reach shared understandings, identify new ways of understanding problems or tasks, and make new connections between ideas. Engaging in these social processes also promotes positive team affect, which facilitates collective creativity.[184][186][187][190]

Supportive and motivational environments that create psychological safety by encouraging risk taking and tolerating mistakes increase team creativity as well.[184][185][186][187] Organizations in which help-seeking, help giving, and collaboration are rewarded promote innovation by providing opportunities and contexts in which team processes that lead to collective creativity can occur.[192] Additionally, leadership styles that downplay status hierarchies or power differences within an organization and empower people to speak up about their ideas or opinions also help to create cultures that are conducive to creativity.[184][185][186][187]

There is a long-standing debate on how material constraints (e.g., lack of money, materials, or equipment) affect creativity. In psychological and managerial research, two competing views in this regard prevail. In one view, many scholars propose a negative effect of material constraints on innovation and claim that material constraints starve creativity.[193] The proponents of this view argue that adequate material resources are needed to engage in creative activities like experimenting with new solutions and idea exploration.[193] In an opposing view, scholars assert that people tend to stick to established routines or solutions as long as they are not forced to deviate from them by constraints.[194][195][196] In this sense, Neren posits that scarcity is an important driver of creativity.[197] Consistently, Gibbert and Scranton demonstrated how material constraints facilitated the development of jet engines in World War II.[198]

To reconcile these competing views, contingency models were proposed.[199][200][201] The rationale behind these models is that certain contingency factors (e.g., creativity climate or creativity relevant skills) influence the relationship between constraints and creativity.[199] These contingency factors reflect the need for higher levels of motivation and skills when working on creative tasks under constraints.[199] Depending on these contingency factors, there is either a positive or negative relationship between constraints and creativity.[199][200]

Economic approaches to creativity have focussed on three aspects — the impact of creativity on economic growth, methods of modelling markets for creativity, and the maximisation of economic creativity (innovation).

In the early 20th century, Joseph Schumpeter introduced the economic theory of creative destruction, to describe the way in which old ways of doing things are endogenously destroyed and replaced by the new. Some economists (such as Paul Romer) view creativity as an important element in the recombination of elements to produce new technologies and products and, consequently, economic growth. Creativity leads to capital, and creative products are protected by intellectual property laws.

Mark A. Runco and Daniel Rubenson have tried to describe a "psychoeconomic" model of creativity.[202] In such a model, creativity is the product of endowments and active investments in creativity; the costs and benefits of bringing creative activity to market determine the supply of creativity. Such an approach has been criticised for its view of creativity consumption as always having positive utility, and for the way it analyses the value of future innovations.[203]

The creative class is seen by some to be an important driver of modern economies. In his 2002 book, The Rise of the Creative Class, economist Richard Florida popularized the notion that regions with "3 T's of economic development: Technology, Talent and Tolerance" also have high concentrations of creative professionals and tend to have a higher level of economic development.

Several different researchers have proposed methods of increasing the creativity of an individual. Such ideas range from the psychological-cognitive, such as Osborn-Parnes Creative Problem Solving Process, Synectics, science-based creative thinking, Purdue Creative Thinking Program, and Edward de Bono's lateral thinking; to the highly structured, such as TRIZ (the Theory of Inventive Problem-Solving) and its variant Algorithm of Inventive Problem Solving (developed by the Russian scientist Genrich Altshuller), and Computer-Aided morphological analysis.

Daniel Pink, in his 2005 book A Whole New Mind, repeating arguments posed throughout the 20th century, argues that we are entering a new age where creativity is becoming increasingly important. In this conceptual age, we will need to foster and encourage right-directed thinking (representing creativity and emotion) over left-directed thinking (representing logical, analytical thought). However, this simplification of 'right' versus 'left' brain thinking is not supported by the research data.[204]

Nickerson[205] provides a summary of the various creativity techniques that have been proposed. These include approaches that have been developed by both academia and industry:

Establishing purpose and intention
Building basic skills
Encouraging acquisitions of domain-specific knowledge
Stimulating and rewarding curiosity and exploration
Building motivation, especially internal motivation
Encouraging confidence and a willingness to take risks
Focusing on mastery and self-competition
Promoting supportable beliefs about creativity
Providing opportunities for choice and discovery
Developing self-management (metacognitive skills)
Teaching techniques and strategies for facilitating creative performance
Providing balance
Managing the need for closure
Experiments suggest the need for closure of task participants, whether as a reflection of personality or induced (through time pressure), negatively impacts creativity.[206] Accordingly, it has been suggested that reading fiction, which can reduce the cognitive need for closure, may help to encourage creativity.[207]

Some see the conventional system of schooling as stifling of creativity and attempt (particularly in the preschool/kindergarten and early school years) to provide a creativity-friendly, rich, imagination-fostering environment for young children.[205][208][209] Researchers have seen this as important because technology is advancing our society at an unprecedented rate and creative problem solving will be needed to cope with these challenges as they arise.[209] In addition to helping with problem solving, creativity also helps students identify problems where others have failed to do so.[205][208][210] See the Waldorf School as an example of an education program that promotes creative thought.

Promoting intrinsic motivation and problem solving are two areas where educators can foster creativity in students. Students are more creative when they see a task as intrinsically motivating, valued for its own sake.[208][209][211][212] To promote creative thinking, educators need to identify what motivates their students and structure teaching around it. Providing students with a choice of activities to complete allows them to become more intrinsically motivated and therefore creative in completing the tasks.[205][213]

Teaching students to solve problems that do not have well defined answers is another way to foster their creativity. This is accomplished by allowing students to explore problems and redefine them, possibly drawing on knowledge that at first may seem unrelated to the problem in order to solve it.[205][208][209][211] In adults, mentoring individuals is another way to foster their creativiy.[214] However, the benefits of mentoring creativity apply only to creative contributions considered great in a given field, not to everyday creative expression.[82]

In the Scottish education system, creativity is identified as a core skillset for learning, life and work and is defined as “a process which generates ideas that have value to the individual. It involves looking at familiar things with a fresh eye, examining problems with an open mind, making connections, learning from mistakes and using imagination to explore new possibilities.” [1] The need to develop a shared language and understanding of creativity and its role across every aspect of learning, teaching and continuous improvement was identified as a necessary aim [2] and a set of four skills is used to allow educators to discuss and develop creativity skills across all subjects and sectors of education – curiosity, open—mindedness, imagination and problem solving. [3] Distinctions are made between creative learning (when learners are using their creativity skills), creative teaching (when educators are using their creativity skills) and creative change (when creativity skills are applied to planning and improvement). [4] Scotland's national Creative Learning Plan [5] supports the development of creativity skills in all learners and of educators’ expertise in developing creativity skills. A range of resources have been created to support and assess this [6] including a national review of creativity across learning by Her Majesty's Inspectorate for Education. [7]

Danah Zohar coined the term "spiritual intelligence" and introduced the idea in 1997 in her book ReWiring the Corporate Brain.[1]

In the same year, 1997, Ken O'Donnell, an Australian author and consultant living in Brazil, also introduced the term "spiritual intelligence" in his book Endoquality - the emotional and spiritual dimensions of the human being in organizations.[2]

In 2000, in the book Spiritual Intelligence, author Steven Benedict outlined the concept as a perspective offering a way to bring together the spiritual and the material, that is ultimately concerned with the well-being of the universe and all who live there.[3]

Howard Gardner, the originator of the theory of multiple intelligences, chose not to include spiritual intelligence in his "intelligences" due to the challenge of codifying quantifiable scientific criteria.[4] Instead, Gardner suggested an "existential intelligence" as viable.[5] However, contemporary researchers continue to explore the viability of Spiritual Intelligence (often abbreviated as "SQ") and to create tools for measuring and developing it. So far, measurement of spiritual intelligence has tended to rely on self-assessment instruments, which can be susceptible to false or unreliable reporting.

Variations of spiritual intelligence are sometimes used in corporate settings, as a means of motivating employees.[6] and providing a non-religious, diversity-sensitive framework for addressing issues of values in the workplace.[7] According to Stephen Covey, "Spiritual intelligence is the central and most fundamental of all the intelligences, because it becomes the source of guidance for the others."[8]

Definitions of spiritual intelligence rely on the concept of spirituality as being distinct from religiosity - existential intelligence.[9]

Danah Zohar defined 12 principles underlying spiritual intelligence:[10]

Self-awareness: Knowing what I believe in and value, and what deeply motivates me.
Spontaneity: Living in and being responsive to the moment.
Being vision- and value-led: Acting from principles and deep beliefs, and living accordingly.
Holism: Seeing larger patterns, relationships, and connections; having a sense of belonging.
Compassion: Having the quality of "feeling-with" and deep empathy.
Celebration of diversity: Valuing other people for their differences, not despite them.
Field independence: Standing against the crowd and having one's own convictions.
Humility: Having the sense of being a player in a larger drama, of one's true place in the world.
Tendency to ask fundamental "Why?" questions: Needing to understand things and get to the bottom of them.
Ability to reframe: Standing back from a situation or problem and seeing the bigger picture or wider context.
Positive use of adversity: Learning and growing from mistakes, setbacks, and suffering.
Sense of vocation: Feeling called upon to serve, to give something back.
Ken O'Donnell, advocates[11] the integration of spiritual intelligence (SQ) with both rational intelligence (IQ) and emotional intelligence (EQ). IQ helps us to interact with numbers, formulas and things, EQ helps us to interact with people and SQ helps us to maintain inner balance. To calculate one's level of SQ he suggests the following criteria:

How much time, money and energy and thoughts do we need to obtain a desired result.
How much bilateral respect there exists in our relationships.
How "clean" a game we play with others.
How much dignity we retain in respecting the dignity of others.
How tranquil we remain in spite of the workload.
How sensible our decisions are.
How stable we remain in upsetting situations.
How easily we see virtues in others instead of defects.
Robert Emmons defines spiritual intelligence as "the adaptive use of spiritual information to facilitate everyday problem solving and goal attainment."[12] He originally proposed 5 components of spiritual intelligence:

The capacity to transcend the physical and material.
The ability to experience heightened states of consciousness.
The ability to sanctify everyday experience.
The ability to utilize spiritual resources to solve problems.
The capacity to be virtuous.
Higher Level of Intelligence and self awareness.
Early Maturing
Control over emotions.
The fifth capacity was later removed due to its focus on human behavior rather than ability, thereby not meeting previously established scientific criteria for intelligence.

Frances Vaughan offers the following description: "Spiritual intelligence is concerned with the inner life of mind and spirit and its relationship to being in the world."[13]

Cindy Wigglesworth defines spiritual intelligence as "the ability to act with wisdom and compassion, while maintaining inner and outer peace, regardless of the circumstances."[14] She breaks down the competencies that comprise SQ into 21 skills, arranged into a four quadrant model similar to Daniel Goleman's widely used model of emotional intelligence or EQ. The four quadrants of spiritual intelligence are defined as:


David B. King has undertaken research on spiritual intelligence at Trent University in Peterborough, Ontario, Canada. King defines spiritual intelligence as a set of adaptive mental capacities based on non-material and transcendent aspects of reality, specifically those that:

"...contribute to the awareness, integration, and adaptive application of the nonmaterial and transcendent aspects of one's existence, leading to such outcomes as deep existential reflection, enhancement of meaning, recognition of a transcendent self, and mastery of spiritual states."[15]

King further proposes four core abilities or capacities of spiritual intelligence:

Critical Existential Thinking: The capacity to critically contemplate the nature of existence, reality, the universe, space, time, and other existential/metaphysical issues; also the capacity to contemplate non-existential issues in relation to one's existence (i.e., from an existential perspective).
Personal Meaning Production: The ability to derive personal meaning and purpose from all physical and mental experiences, including the capacity to create and master a life purpose.
Transcendental Awareness: The capacity to identify transcendent dimensions/patterns of the self (i.e., a transpersonal or transcendent self), of others, and of the physical world (e.g., nonmaterialism) during normal states of consciousness, accompanied by the capacity to identify their relationship to one's self and to the physical.
Conscious State Expansion: The ability to enter and exit higher states of consciousness (e.g. pure consciousness, cosmic consciousness, unity, oneness) and other states of trance at one's own discretion (as in deep contemplation, meditation, prayer, etc.).[16]
Also, Vineeth V. Kumar and Manju Mehta have also researched the concept, extensively. Operationalizing the construct, they defined spiritual intelligence as "the capacity of an individual to possess a socially relevant purpose in life by understanding 'self' and having a high degree of conscience, compassion and commitment to human values."[17]

Measurement of spiritual intelligence relies on self-reporting. David King and Teresa L. DeCicco have developed a self-report measure, the Spiritual Intelligence Self-Report Inventory (SISRI-24) with psychometric and statistical support across two large university samples.[16] Cindy Wigglesworth has developed the SQ21, a self-assessment inventory that has tested positively for criterion validity and construct validity in statistically significant samples.[18] Wigglesworth's SQ model and assessment instrument have been successfully used in corporate settings.[19]

The Scale for Spiritual Intelligence (SSI; Kumar & Mehta, 2011) is a 20-item, self-report measure of spiritual intelligence in adolescents. The idea behind the development of this scale was to generate and assess the concept of spiritual intelligence in the collectivist culture bounded with eastern philosophy. The SSI is rated on a Likert scale and can be completed in 10 minutes.[20] The 29-item Spiritual Intelligence Questionnaire:This test was normalized by Abdollahzadeh(2008) with the collaboration of Mahdieh Kashmiri and Fatemeh Arabameri on students. The normal group was 280 people, 200 of whom were the students of Gorgan University of Natural Resources and 80 students of Payame Noor University of Behshahr. Of these, 184 were female and 96 were male. First, a 30-item questionnaire was prepared by the test developers and implemented on 30 students.The reliability of the test in the initial phase was 0.87 by the alpha method. In the analysis of the question by Loop method, 12 questions were removed and the final questionnaire was adjusted with 29 phrases. At the final stage, the questionnaire was implemented on 280 subjects and the reliability was 0.89 at this stage. Factor analysis was used to evaluate validity in addition to formal content validity that the questions were confirmed by the experts (colleagues) and the correlation of all questions was higher than 0.3. In Varimax rotation, two major factors were found to reduce variables. The first factor with 12 question was called "understanding and communicating with the source of universe” and the second factor with 17 items was called “spiritual life or reliance on the inner core." The first factor included the questions 1, 4, 5, 7, 8, 9, 11, 15, 16, 24, 27, 29 and the second factor included the questions 2, 3, 6, 10, 12, 13, 14, 17, 18, 19, 20, 21, 22, 23, 25, 26, 28. [21]

It has been argued that Spiritual Intelligence cannot be recognized as a form of intelligence. Howard Gardner, originator of multiple intelligence theory, chose not to include spiritual intelligence amongst his intelligences due to the challenge of codifying quantifiable scientific criteria.[4] Later, Gardner suggested an "existential intelligence" as viable, but argued that it was better to

put aside the term spiritual, with its manifest and problematic connotations, and to speak instead of an intelligence that explores the nature of existence in its multifarious guises. Thus, an explicit concern with spiritual or religious matters would be one variety — often the most important variety — of an existential intelligence.[5],